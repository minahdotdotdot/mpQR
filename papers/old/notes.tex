\documentclass{article}
%Packages
\usepackage[utf8]{inputenc}
\usepackage{geometry, graphicx}
\usepackage{enumerate}
\usepackage{amsmath,amssymb,amsfonts,amsthm, bm}
\usepackage{xcolor, ulem} %just for visible comments and edits.
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[sort&compress, numbers]{natbib}
%\usepackage[toc,page]{appendix}

% New theorems and commands
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newcommand\mycommfont[1]{\ttfamily\textcolor{blue}{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\dd}{\delta}
\newcommand{\tth}{\theta}
\newcommand{\bb}[1]{\mathbf{#1}}
\newcommand{\fl}{\mathrm{fl}}
\SetCommentSty{mycommfont}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

% Document
\title{Notes and random things}
\author{L. Minah Yang, Alyson Fox, and Geoffrey Sanders}
\date{\today}
\begin{document}
%\titlepage
{\color{blue}Higham says QR factorizations are unique if full rank.}
\section{Floating Point Numbers and Error Analysis Tools}
We will be using floating-point operation error analysis tools developed in \citet{Higham2002}.
Let $\F \subset \R$ denote the space of some floating point number system with base $\beta$, precision $t$, significand/mantissa $m$, and exponent range $e_{\text{ran}}:=\{e_{\text{min}}, e_{\text{min}}+1, \cdots, e_{\text{max}} \}$.
Then every element $y$ in$\F$ can be written as 
\begin{equation}
y = \pm m\times \beta^{e-t},
\label{eqn:FPbasic}
\end{equation} 
where $m$ is any integer in $[0,\beta^{t}-1]$, and $e\in e_{\text{ran}}$.
Table \ref{table:ieee} shows IEEE precision types described by the same paramters as in Equation \ref{eqn:FPbasic}.
While operations we use on $\R$ cannot be replicated exactly due to the finite cardinality of $\F$, we can still approximate the accuracy of analogous floating point operations using these error analysis tools. \par

\begin{table}[h]
	\centering
	\begin{tabular}{||c|c|c|c|c|c||} 
		\hline 
		Name & $\beta$ & $t$ & \# of exponent bits & $e_{\text{min}}$ & $e_{\text{max}}$ \\ \hline 
		IEEE754 half & 2 & 11 & 5 & -15 & 16  \\ \hline 
		IEEE754 single & 2 & 24 & 8 & -127 & 128 \\ \hline 
		IEEE754 double& 2 & 53 & 11 & -1023 & 1024 \\ \hline 
	\end{tabular}
	\caption{IEEE754 formats with $j$ exponent bits range from $1-2^{j-1}$ to $2^{j-1}$.}
	\label{table:ieee}
\end{table}
 
Suppose that a single basic floating-point operation yields a relative error, $\dd$.
\begin{equation}
\fl(x\text{ op }y) = (1 + \dd)(x\text{ op }y),\quad |\dd|\leq u, \quad \text{op}\in\{+, -, \times, \div\} \label{eqn: singlefpe}
\end{equation}
The true value $(x\text{ op }y)$ lies in $\R$ and it is rounded to the nearest floating point number, $\fl(x\text{ op }y)$, admitting a rounding error. 
A short analysis (cf. Theorem 2.2 \citep{Higham2002}) shows that the relative error $|\dd|$ is bounded by the unit round-off, $u:=\frac{1}{2}\beta^{1-t}$. \par

We use Equation \ref{eqn: singlefpe} as a building block in accumulating errors from successive floating point operations in product form.
Lemma \ref{lem:gamma} introduces new notations that replace the relative error $\dd$ with $\theta_n$ and the upper bound $u$ with $\gamma^{(n)}$, and simplifies many error analyses. 
\begin{lemma}[Lemma 3.1 \cite{Higham2002}]
\label{lem:gamma}
    Let $|\dd_i|<u$ and $\rho_i \in\{-1, +1\}$ for $i = 1 , \cdots, n$, and $nu < 1$. Then, 
    \begin{equation}
        \prod_{i=1}^n (1+\dd_i)^{\rho_i} = 1 + \tth^{(n)}
    \end{equation}
    where
    \begin{equation}
        |\tth^{(n)}|\leq \frac{nu}{1-nu}=:\gamma^{(n)}.
    \end{equation}
\end{lemma}
While the assumption $nu<\frac{1}{2}$ (which implies $\gamma^{(n)} < 1$) is easily satisfied by single and double precision types, it becomes a problem for lower precision types.
Table \ref{table:ieeen} shows the maximum value of $n$ that still guarantees a relative error below $100\%$ ($\gamma^{(n)} < 1$). 
\begin{table}[h]
	\centering
	\begin{tabular}{||c|c||} 
		\hline
		precision & $\mathrm{argmax}_n(\gamma_{nu} \leq 1)$ \\ \hline
		half & $512$\\
		single & $4194304\approx 4.19\times 10^6$ \\
		double & $2251799813685248 \approx 2.25\times 10^{15}$ \\ \hline
	\end{tabular}
	\caption{Upper limits of validity in the $\gamma$ notation.}
	\label{table:ieeen}
\end{table}
This reflects on two sources of difficulty: 1) Numerous operations in lower precision types grow less stable quickly, and 2) the upper bound given by $\gamma^{(n)}$ becomes suboptimal faster in low precision.
However, error analysis within the framework given by Lemma \ref{lem:gamma} is still the best method and we will be using it to study variable-precision block QR factorization methods. \par 

In addition to Lemma \ref{lem:gamma}, Lemma 3.3 in \citep{Higham2002} summarizes other useful relations for error analysis.
We present modified versions of these relations that supports multiple precision types.
We distinguish between the different precision types using subscripts--- these types include products ($p$), sums ($s$), and storage formats ($st$).
%For any precision $q$ in $\{p, s, st\}$, note that $\delta_q \equiv \theta_q^{(1)}$ is strictly less than $\gamma_q^{(1)}$.

%\begin{lemma}[Modified version of Lemma 3.3 from \cite{Higham2002}]
%    For any positive integer $k$, let $\tth_k$ denote a quantity bounded according to $|\tth_k|\leq \frac{k u }{1-ku} =:\gamma_k$. The following relations hold for positive integers $i$, $j$, and $k$. {\color{red}(Actually, $k$ can be zero.)}:
%    \begin{align*}
%        (1+\tth_k)(1+\tth_j)&=(1+\tth_{k+j}) \\
%        \frac{1+\tth_k}{1+\tth_j} &=\begin{cases}
%1+\tth_{k+j},& j \leq k\\
%1+\tth_{k+2j},& j > k\\
%\end{cases} \\
%\gamma_k\gamma_j &\leq \gamma_{\rm{min}(k,j)}, \quad\text{for } \rm{max}_{(j,k)} u \leq \frac{1}{2} \\
%i\gamma_k &\leq \gamma_{ik} \\
%\gamma_k + u &\leq \gamma_{k+1} \\ 
%\gamma_k+\gamma_j+\gamma_k\gamma_j & \leq \gamma_{k+j}
%    \end{align*}
%\end{lemma}
%
%The third rule can actually be extended for $c\in(0, N]$ where $Nu\leq 1$.
%\begin{lemma}
%\label{lem:gamma1}
%The following inequalities hold.
%    \begin{align*}
%    \gamma_{ck} \leq c \gamma_k < \gamma_k&, \quad\text{for } c\in(0,1){\color{blue} \text{ Not sure if useful.}}\\
%    c \gamma_k \leq \gamma_{ck}&, \quad\text{for }c\in[1, \frac{1}{ku}]
%    \end{align*}
%\end{lemma}

\begin{lemma}[Modified version of Lemma 3.3 from \citep{Higham2002}]
	For any nonnegative integer $k$, and some precision $q$, let $\tth_{k}^{(q)}$ denote a quantity bounded according to $|\tth_k^{(q)}|\leq \frac{k u^{(q)} }{1-ku^{(q)}} =:\gamma_{k}^{(q)}$.
	The following relations hold for 2 precisions $s$ and $p$, positive integers, $j_s$,$j_p$, non-negative integers $k_s$ and $k_p$, and $c>0$.
	Most of these result from commutativity. 
\begin{align*}
		(1+\tth_{p}^{(k_p)})(1+\tth_{p}^{(j_p)})(1+\tth_{s}^{(k_s)})(1+\tth_{s}^{(j_s)})&=(1+\tth_{p}^{(k_p+j_p)})(1+\tth_{s}^{(k_s+j_s)}) \\
		\frac{(1+\tth_{p}^{(k_p)})(1+\tth_{s}^{(k_s)})}{(1+\tth_{p}^{(j_p)})(1+\tth_{s}^{(j_s)})} &=\begin{cases}
(1+\tth_{s}^{(k_s+j_s)})(1+\tth_{p}^{(k_p+j_p)}),& j_s \leq k_s \text{ and } j_p \leq k_p\\
(1+\tth_{s}^{(k_s+2j_s)})(1+\tth_{p}^{(k_p+j_p)}),& j_s \leq k_s \text{ and } j_p > k_p\\
(1+\tth_{s}^{(k_s+j_s)})(1+\tth_{p}^{(k_p+2j_p)}),& j_s > k_s \text{ and } j_p \leq k_p \\
(1+\tth_{s}^{(k_s+2j_s)})(1+\tth_{p}^{(k_p+2j_p)}),& j_s > k_s \text{ and } j_p > k_p
\end{cases} \\
\end{align*}
Without loss of generality, let $u_s \ll u_p$, and define $N_{s,p}:=\lfloor\frac{u_p}{u_s}\rfloor$. 
For all integers $k_s < N_{s,p}${\color{blue} (Check: $<$ or $\leq$)}, we result in the following set of inequalities.
\begin{align*}
	\gamma_{s}^{(k_s)}\gamma_{p}^{(k_p)} &\leq \gamma_{p}^{(k_p)}, \quad\text{for } k_p u_p \leq \frac{1}{2}  \\
	\gamma_{s}^{(k_s)}+u_p &\leq \gamma_{p}^{(2)} \\
	\gamma_{p}^{(k_p)} + u_{s} &\leq \gamma_{p}^{(k_p+1)} \\ 
	\gamma_{p}^{(k_p)}+\gamma_{s}^{(k_s)}+\gamma_{p}^{(k_p)}\gamma_{s}^{(k_s)} & \leq \gamma_{p}^{(k_p+1)}
\end{align*}
\end{lemma}
\section{Householder QR Backward Error Analysis}
We present up the error analysis for the Householder QR factorization where all inner products are done with precision $p$ for products, and precision $s$ for the summation, and stored in $st$ precision.
\subsection{Revised (hopefully tighter) 2-Norm error}
Consider performing an inner product with different floating point precision assigned to multiplication and addition. 
That is, for some $\bb{x},\bb{y}\in \mathbb{F}_{st}^{n}\subset\R^n$, the element-wise product is done in some precision, $p$ with eps $u_p$, and summation is done in precision $s$ with eps $u_s$.
Let $u_p$ be the unit round-off for products, and $u_s$ be the unit round-off for summation. 
We are ignoring errors that could(?) arise from conversion between floating types. 
Following the notation in \cite{Higham2002}, we let $\dd_p$ and $\dd_s$ be defined as quantities that are bounded by: $|\dd_p| < u_p$ and $|\dd_s| < u_s$.
{\color{blue} 
Probably too much detail:
\begin{align*}
    \hat{s_1} &= \fl (x_1y_1) = x_1y_1(1 + \dd_p^{(1)})\\
    \hat{s_2} &= \fl(\hat{s_1} + x_2y_2) \\
    &= \left[x_1y_1(1 + \dd_p^{(1)}) + x_2y_2(1 + \dd_p^{(2)})\right](1+\dd_s^{(1)})\\
    \hat{s_3} &= \fl(\hat{s_2}+x_3y_3) \\
    &= \left(\left[x_1y_1(1 + \dd_p^{(1)}) + x_2y_2(1 + \dd_p^{(2)})\right](1+\dd_s^{(1)})  + x_3y_3(1+\dd_p^{(3)})\right)(1+\dd_s^{(2)})
\end{align*}
}

We can see a pattern emerging. We drop the superscripts as we only need to distinguish between $\dd_p$'s and $\dd_s$'s.
The error for a general length $n$ vector dot product is then:
 \begin{equation}
 \label{eqn:dperr_1}
    \hat{s_n} = (x_1y_1+x_2y_2)(1\pm \dd_p)(1\pm\dd_s)^{n-1} + (1\pm\dd_p)\sum_{i=3}^n x_iy_i(1\pm\dd_s)^{n-(j-1)} 
\end{equation}

{\color{blue} Why $\pm$'s?\\}
Using Lemma \ref{lem:gamma} and that $\gamma_{u,n}$ is a monotonically increasing function with respect to $n$  (for $nu < 1$), we further simplify. 
\begin{align*}
    \fl(\bb{x}^{\top}\bb{y}) &= \hat{s_n}\\
    &= (1+\tth_{1,p})\left[|x_1y_1|(1+\tth_{n-1,s})+|x_2y_2|(1+\tilde{\tth}_{n-1,s}) + \sum_{i=3}^n|x_iy_i|(1+\tth_{n-(i-1),s}) \right] \\
    &= (\bb{x}+\Delta \bb{x})^{\top} \bb{y} = \bb{x}^{\top}(\bb{y}+\Delta \bb{y}), \text{where } \{\Delta \bb{x}, \Delta \bb{y}\} \leq (1+\tth_{1,p})(1+\gamma_{m-1,s})\{|\bb{x}|,|\bb{y}|\}\text{componetwise.}\\
    &\leq(1+\tth_{1,p})\left[|x_1y_1|+|x_2y_2|(1+\gamma_{n-1,s})+ \sum_{i=3}^n|x_iy_i|(1+\gamma_{n-(i-1),s}) \right]\\
    &\leq (1+\tth_{1,p})(1+\gamma_{n-1,s})\sum_{i=1}^n|x_iy_i|=(1+\tth_{1,p})(1+\gamma_{n-1,s})|\bb{x}|^{\top}|\bb{y}|\\
    %&\leq \left(1 + \frac{(n-1)u_s+u_p}{1-(n-1)u_s}\right)|x|^{\top}|y|\\
    %|\fl(x^{\top}y)-x^{\top}y|&= \frac{(n-1)u_s+u_p}{1-(n-1)u_s} |x|^{\top}|y|,\quad \hat{\gamma}_{n,s,p}:= \frac{(n-1)u_s+u_p}{1-(n-1)u_s}
\end{align*}
While this result does not give us a relative backward error, it can be used as a rough estimate while keeping in mind that it loses high relative accuracy as  $|\bb{x}^{\top}\bb{y}| \ll |\bb{x}|^{\top}|\bb{y}|$.
If $\bb{y}=\bb{x}$, we have exactly $|\bb{x}^{\top}\bb{x}| = |\bb{x}|^{\top}|\bb{x}|=\|\bb{x}\|_2^2$.
So we get {\color{blue}\sout{(CHECK: first inequality)}}
$$
(1-u_{1,p})(1-\gamma_{n-1,s})\|\bb{x}\|_2^2 \leq \fl(\|\bb{x}\|_2^2) \leq (1+u_{1,p})(1+\gamma_{n-1,s})\|\bb{x}\|_2^2,
$$
which gives us:
$$
\fl(\|\bb{x}\|_2^2) = (1+\tth_{1,p})(1+\tth_{n-1,s})\|\bb{x}\|_2^2
$$
Then, 
\begin{align*}
\fl(\|\bb{x}\|_2) &= \fl(\sqrt{\|\bb{x}\|_2^2})\\
&= (1+\tth_{st})\sqrt{(1+\tth_{1,p})(1+\tth_{n-1,s})}\|\bb{x}\|_2,
\end{align*}
where $\tth_{st}$ represents the rounding error that results from the square root operation. 
Taylor expansion analysis shows that $\sqrt{1+\tth^{(n)}}=1+\frac{5}{8}\tth_{n}$.

For small $|\epsilon|\ll 1$, 
\begin{align*}
    \sqrt{1+\epsilon} = 1 + \frac{1}{2}\epsilon + \sum_{i=2}^\infty \left[\frac{1}{i!}\epsilon^i\prod_{j=1}^i\left(\frac{1}{2}-(j-1)\right)\right]
\end{align*}
The series can also be written in recursive form: $a_1 = (1+\frac{1}{2}\epsilon)$ and $a_n=a_{n-1}\frac{\epsilon}{n}\left(\frac{1}{2}-(n-1)\right)$ for $n\geq 2$.
As this is an alternating series for any $\epsilon < 1$, we can bound the partial sum. 
\begin{align*}
    \left|\sqrt{1+\epsilon}-(1+\frac{1}{2}\epsilon)\right|&\leq \frac{1}{8}\epsilon^2 \\
    \frac{1}{2}\epsilon - \frac{1}{8}\epsilon^2 \leq \sqrt{1+\epsilon} -1 & \leq \frac{1}{2}\epsilon + \frac{1}{8}\epsilon^2 \\
    -\frac{1}{2}|\epsilon| - \frac{1}{8}\epsilon^2 \leq \sqrt{1+\epsilon} -1 & \leq \frac{1}{2}|\epsilon| + \frac{1}{8}\epsilon^2 \\
     \left|\sqrt{1+\epsilon}-1\right| &\leq \frac{1}{2}|\epsilon| + \frac{1}{8}\epsilon^2
\end{align*}
Applying this result to $\tth^{(n)}$, we get: 
\begin{align*}
    |\sqrt{1+\tth^{(n)}}-1| &\leq \frac{1}{2}\gamma^{(n)} + \frac{1}{8}\gamma^{(n)}\gamma^{(n)} \leq \frac{5}{8}\gamma^{(n)}\\
    \sqrt{1+\tth^{(n)}} &= 1 + \frac{5}{8}\tth^{(n)}
\end{align*}
This is a slight improvement compared to the analysis in \cite{Higham2002}, which used a looser bound of: $\sqrt{1+\tth^{(n)}} = 1 + \tth^{(n)}$.
Now, the largest possible $n$ such that the relative error $\frac{5}{8}\gamma^{(n)} < 1$ for an arbitrary precision with unit round-off $u$ is $\lfloor\frac{8}{13}\frac{1}{u}\rfloor$. 

Thus, the 2-norm error derived from using multiple precision dot products is:
\begin{equation}
    \fl(\|\bb{x}\|_2) = (1+\tth_{st})(1+\frac{5}{8}\tth_p)(1+\frac{5}{8}\tth_{n-1,s})\|\bb{x}\|_2.
\end{equation}
\subsection{Calculation and normalization of Householder Vector}
The Householder QR factorization utilizes Householder transformations to zero out elements below the diagonal of a matrix. 
We consider the simpler task of zeroing out all but the first element of a vector, $\bb{x}\in\R^n$. Then, the appropriate Householder vector is defined by: $\bb{v}:=\bb{x}+\rm{sign}(x_1)\|\bb{x}\|_2\hat{e_1}$.
An efficient algorithm for calculating $\bb{v}$ is shown in Algorithm \ref{algo:hh_v1}.
\begin{algorithm}
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	\KwIn{$\bb{x}\in\R^n$}
	\KwOut{$\bb{v}\in\R^n$, and $\sigma, \beta\in\R$ such that $(I-\beta \bb{v} \bb{v}^{\top})\bb{x} = \pm \|\bb{x}\|_2 \hat{e_1} = \sigma\hat{e_1}$ }
	\tcc{We choose the sign of sigma to avoid cancellation of $x_1$ (As is the standard in LAPACK, LINPACK packages \cite{Higham2002}). This makes $\beta>0$.}
	$\bb{v}\gets \bb{x}$\\
	$\sigma \gets -\rm{sign}(x_1)\|\bb{x}\|_2$\\
	$v_1 \gets v_1-\sigma$\\
	$\beta \gets -\frac{1}{\sigma v_1}$\\
	\Return $\beta$, $\bb{v}$
	\caption{Given a vector $\bb{x}\in\R^n$, return a Householder vector $v$ and a Householder constant $\beta$ such that $(I-\beta \bb{v}\bb{v}^{\top})\bb{x} \in \mathrm{span}(\hat{e_1})$.}
	\label{algo:hh_v1}
\end{algorithm}

This algorithm leaves $\bb{v}$ unnormalized, but it is often normalized via the various methods and reasons listed below:
\begin{enumerate}
    \item Set $v_1$ to $1$ for efficient storage of many Householder vectors.
    \item Set the 2-norm of $\bb{v}$ to $\sqrt{2}$ to always have $\beta=1$.
    \item Set the 2-norm of $\bb{v}$ to $1$ to prevent extremely large values, and to always have $\beta=2$.
\end{enumerate}
The first normalizing method adds an extra rounding error to $\beta$ and $\bb{v}$ each, whereas the remaining methods only add an extra rounding error to $\bb{v}$ since we set $\beta$ to what it should be exactly. 
The LINPACK implementation of the Householder QR factorization uses {\color{blue}CHECK!} the first method of normalizing via setting $v_1$ to $1$. 
Algorithm \ref{algo:hh_v2} shows how this convention could be carried out. 
The error analysis in the subsequent section assumes that there may exist errors in both $\beta$ and $\bb{v}$ to get the worse-case scenario and to be consistent with the LINPACK implementation. 
\begin{algorithm}
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	\KwIn{$\bb{x}\in\R^n$}
	\KwOut{$\bb{v}\in\R^n$, and $\sigma, \beta\in\R$ such that $(I-\beta \bb{v}\bb{v}^{\top})\bb{x} = \pm \|\bb{x}\|_2 \hat{e_1} = \sigma\hat{e_1}$ }
	\tcc{We choose the sign of sigma to avoid cancellation of $x_1$ (As is the standard in LAPACK, LINPACK packages \cite{Higham2002}). This makes $\beta>0$.}
	$\bb{v}\gets \bb{x}$[2:end]\\
	$\sigma \gets -\rm{sign}(x_1)\|\bb{x}\|_2$\\
	$v_1 \gets x_1-\sigma$ \tcp*{This $v_1$ is still not normalized.} 
	$\beta \gets -\frac{1}{\sigma v_1^2}$\\
	$\bb{v} \gets \frac{1}{v_1}\bb{v}$\\
	\Return $\beta$, $\bb{v}$
	\caption{Given a vector $\bb{x}\in\R^n$, return the portion of the Householder vector $\bb{v}$ without the first component, and a Householder constant $\beta$ such that $(I-\beta \bb{v}\bb{v}^{\top})\bb{x} \in \mathrm{span}(\hat{e_1})$, and $v_1=1$.}
	\label{algo:hh_v2}
\end{algorithm}

\subsubsection{Error analysis for $v_1$}
This algorithm only introduces error in $v_1$. 
\begin{align*}
    \fl(\bb{x}^{\top}\bb{x}) &\leq (1+u_p)(1+\gamma_{n-1,s})\bb{x}^{\top}\bb{x} \\
    \fl(\sigma) &= \fl(-\rm{sign}(x_1)\|\bb{x}\|_2)\\
    &= -\rm{sign}(x_1)\sqrt{\fl(\bb{x}^{\top}\bb{x})}\\
    &= (1+\tth_{st})\sqrt{(1+\tth_{1,p})(1+\tth_{n-1,s})} \sigma\\
    &= (1+\tth_{st})(1+\frac{5}{8}\tth_p)(1+\frac{5}{8}\tth_{n-1,s})\sigma\\
    \fl(v_1) &= \fl(x_1-\sigma) = (1+\dd_{st})(x_1-\hat{\sigma})\\
    &=(1+\tth_{2,st})\sqrt{(1+\tth_{1,p})(1+\tth_{n-1,s})}v_1\\
    &=(1+\tth_{2,st})(1+\frac{5}{8}\tth_p)(1+\frac{5}{8}\tth_{n-1,s})v_1\\
\end{align*}
The above equalities are permitted since we are using arbitrary $\tth$ values.

\subsubsection{Error analysis for $\beta$}
\begin{align*}
    \hat{\beta} = \fl(\frac{1}{\fl(\hat{\sigma}\hat{v_1})}) &= -\frac{1+\dd_{st}}{(1+\dd_{st})\left[(1+\tth_{3,st})\sqrt{(1+\tth_{1,p})(1+\tth_{n-1,s})}^2\sigma v_1\right]}\\
    &= \frac{1+\tth_{9,st}}{(1+\tth_{1,p})(1+\tth_{n-1,s})}\beta\\
    &= (1+\tth_{9, st})(1+\tth_{2,p})(1+\tth_{2n-2,s})\beta
\end{align*}

\subsection{Error analysis for a single Householder transformation to a vector}
We never form the Householder matrix. 
Instead we apply a series of inner and outer products, since the matrix is a rank-1 update of the identity. 
For some $P_{\bb{v}}=I-\beta \bb{v}\bb{v}^{\top}$, we result in the following computation.
\begin{equation}
    P_{\bb{v}} \bb{x} = (I-\beta \bb{v}\bb{v}^{\top})\bb{x} = \bb{x} - (\beta \bb{v}^{\top}\bb{x})\bb{v}
\end{equation}
\textbf{Case 1: Applying $P_{\bb{v}}$ to zero out vector}\\ 
Recall that we chose a specific $\bb{v}$ such that $P_{\bb{v}}\bb{x} = \sigma \hat{e}_1$. Therefore, the only error lies in the first element, $\sigma$. 

\textbf{Case 2: Applying $P_{\bb{v}}$ to the rest of the matrix ($x$ has no relationship to $v$.)}\\  
LOOK AT LEMMA 3.9
\begin{align*}
    \fl(P_{\bb{v}} \bb{x}) &= \fl(\bb{x}-\fl(\fl(\bb{v}^{\top}\bb{x}) \bb{v}))\\
    &\text{For each component:} (i = 1 : n)\\
    \fl(P_{\bb{v}} \bb{x})_i &= (1+\dd_{st})(1+\dd_{st})(x_i-\fl{(\bb{v}{\top}\bb{x})}v_i)
\end{align*}
Recall 

\subsection{Fused Multiply-Add Operation}
Refer to section 2.6 of \cite{Higham2002} and https://developer.download.nvidia.com/assets/cuda/files/NVIDIA-CUDA-Floating-Point.pdf



\clearpage
\bibliographystyle{plainnat}
\bibliography{report}


\end{document}