\documentclass[10pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{geometry, graphicx,wrapfig}
\usepackage{enumerate}
\usepackage{amsmath,amssymb,amsfonts,amsthm, bm}
%\usepackage{amsmath,amssymb,amsfonts,bm}
\usepackage{xcolor} %just for visible comments.
\usepackage[linesnumbered,ruled,vlined,algo2e]{algorithm2e}
\usepackage[toc,page]{appendix}
\usepackage{makecell}
\usepackage{cleveref}
\usepackage{sectsty}
\usepackage{url}
\usepackage{titling}

% New theorems and commands
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{assump}[theorem]{MP Setting}
\newcommand\mycommfont[1]{\ttfamily\textcolor{orange}{#1}}
%\SetCommentSty{mycommfont}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\dd}{\delta}
\newcommand{\tth}{\theta}
\newcommand{\bb}[1]{\mathbf{#1}}
\newcommand{\fl}{\mathrm{fl}}
\newcommand{\cO}{\mathcal{O}}
\sectionfont{\large\normalfont\centering}
\posttitle{\par\end{center}}
\setlength{\droptitle}{-1in}

%opening
\title{Comments to Reviewers}
\author{\small L. Minah Yang, Alyson Fox, Geoffrey Sanders \vspace{-5cm}}
\date{\small \today}
\begin{document}

\maketitle
We would like to thank both reviewers for your time and effort. 
First, we would like to make a few comments on the major aspects of the revisions:
\begin{itemize}\bfseries
	\item Column partitioned Householder QR factorization algorithm (HQR) has been added to reflect on mixed precision settings of GPU tensor cores units.\\
	{\normalfont We added the analysis of the level-3 BLAS variant of HQR since it is the standard HQR implementation in many libraries and can be effortlessly adapted to utilize the block Fused Multiply-Add operations (bFMAs) of NVIDIA TensorCore units.
	The WY representation of \cite{Bischof1987} is used instead of the compact, storage-efficient version of \cite{Schreiber1989} since the former is discussed in both \cite{golub2013matrix,Higham2002}, which we often refer to in the text.
	}
	\item We have added the mixed precision setting of NVIDIA's TensorCore bFMAs.
	{\normalfont
	This setting is more relevant and practical since these hardware units are already in use, and result in fewer low precision errors than the inner product mixed precision setting we had introduced. 
	While we do not discuss the speed-up advantages in depth, we do refer to the speed benchmarks for these operations that already exist and may be of interest to the readers.
	}
	\item Section 5 (Applications) has been removed from this manuscript.\\
	{\normalfont 
	Given the length of the first submission and the additional materials introduced as explained above, we have removed the applications section from this text.
	However, we plan on presenting our work on using mixed precision arithmetic in graph clustering in a future work, where we will include other graph problems.
	} 
	
\end{itemize}
Next, we would like to address concerns voiced by both referees:
\begin{itemize}\bfseries
	\item Missing references to relevant and prior works.\\
	Referee \#1: The paper also suffers a complete lack of acknowledgment of prior work in this area and of the current state-of-the-art.
	\begin{itemize}\bfseries
		\item lines 157-159: citation missing at the end of this sentence. \\
		{\normalfont
		Citations for probabilistic rounding error analyses have been added (\cite{Higham2019a,Ipsen2019}) to line A. 
		}
		\item lines 163-165: Be more specific about what rounding error analysis framework was established in the textbook [13]. As far as I know, the textbook does not establish any new error analysis framework, nor is it limited to analyses using a single precision.\\
		{\normalfont
		We did not intend to imply that \cite{Higham2002} establishes new error analysis framework. 
		In section 3, we have included the standard rounding error analyses for HQR and its level-3 BLAS variant and have appropriately attributed and redirected readers to a main source, \cite{Higham2002}. 
		The textbook (\cite{Higham2002}) does refer to mixed precision iterative refinement, but the rounding error analysis for HQR(Section 19.3) and aggregated Householder transformations (Section 19.5) both assume a uniform precision setting. 
	    } 
		\item Section 2.2: Prior work should be discussed and cited here.\\
		{\normalfont
		In section 2.2, we define MP Setting 2.3 (line C), which is the mixed precision inner product that mimics TensorCore bFMAs but in a level-2 BLAS operation.
		Later in section 4.1 (line D), we discuss the specifics of NVIDIA TensorCore bFMAs, and reference work from \cite{Blanchard2019}. 
		}
	\end{itemize}
    Referee \#2: {\normalfont We have added the suggested references for the following topics.}
    \begin{itemize}\normalfont
    	\item We have referenced \cite{Blanchard2019} for their work on mixed precision matrix products and LU decomposition in section 4.1 (line D).
    	\item References to (\cite{Higham2019a,Ipsen2019}) on probabilistic rounding error analyses have been added to to line A. 
    	\item Work on proving that the faithfulness of Algorithm 1 on simulating half precision arithmetic have been referenced (\cite{HighamPranesh2019b}) in line E. 
    \end{itemize}
	\item Inconsistencies in notation and adhering to standard notation:
	\begin{itemize}\normalfont
		\item \textbf{Referee \#1: }Hyphens have been removed from ``low-'', ``mixed-'', and ``high-precision''.
		\item \textbf{Referee \#2: }We changed the notation for $\gamma$ for $k$ accumulated rounding errors in precision type $q$ from $\gamma_{q}^{(k)}$ to $\gamma_{k}^{(q)}$ to match the standard notation.
	\end{itemize}
	\item P2L44-45: ``QR factorization is known to provide a backward stable solution to the linear least squares problem ...' \\
	{\normalfont
	We meant to motivate the need for mixed precision QR factorization algorithms since mixed precision is an active area of research.
	However, we have removed this sentence as it was confusing and did not serve the purpose of the paper.
	}
\end{itemize}

We now address unique concerns from each referee separately.
\section*{Requests from Referee \#1}
\begin{enumerate}\bfseries
	\item The primary problem that I see is that the authors have not convinced me that there is any novelty in what they call a ``new framework'' for doing mixed-precision floating point error analysis.\\
	{\normalfont I need help addressing this.}
%	\item The paper also suffers a complete lack of acknowledgement of prior work in this area and of the current state-of-the-art.
	\item line 39: What is meant by ``exact products''? \\
	{\normalfont 
	The documentation for TensorCore bFMAs says ``full precision products'', which in our context is equivalent to a product in exact arithmetic.
	We have added a full explanation to clarify this in section 2 (line F). 
	}
	%\item line 44-45: ``QR factorization is known to provide a backward stable solution to the linear least squares problem ...' Using what algorithm/under what conditions? 
%	\item lines 157-159: citation missing at the end of this sentence. 
%	\item lines 163-165: Be more specific about what rounding error analysis framework was established in the textbook [13]. As far as I know, the textbook does not establish any new error analysis framework, nor is it limited to analyses using a single precision. 
%	\item Section 2.2: Prior work should be discussed and cited here. 
	\item Pages 2-6 contain standard introductory textbook material and can be significantly shortened. \\
	{\normalfont
	While this material is standard, intermediate results from these analyses are necessary for the rounding error analyses for the mixed precision variant of the algorithms in section 4.
	We are aware that this adds significant length to this paper and have tried to shorten it while keeping it accessible for readers who may not be familiar with rounding error analyses.
	Modifying the existing analyses for various mixed precision settings is precisely our major contribution, which is presented in Section 4.
	}
	%\item Consistency in notation, typos...
\end{enumerate}
\section*{Requests from Referee \#2}
\begin{enumerate}\bfseries
	\item Choice of mixed precision assumptions
	\begin{itemize}
		\item Relation with GPU tensor core units\\
		{\normalfont We have added a mixed precision setting that addresses this specific hardware.
		This is first mentioned in line G of section 2.2, and further explored in section 4.2 (line D).	
	}
		\item Assumptions on storage precision types\\
		{\normalfont In line H of section 4.2, we explain that  intermediate matrix products should be stored in the higher precision and the low precision output is used only for the final result of the block matrix product in order to gain the highest accuracy.
		In the analysis for a mixed precision variant of the level-3 HQR (BQR) in section 4.2, this translates to introducing only $\cO(n/r)$ low precision rounding errors for forming the QR factorization for an $m$-by-$n$ sized matrix whose columns are partitioned in groups of $r$.
		Also, we reference the readers to \cite{Blanchard2019} for a full analysis of matrix products. 
		}
		\item Distinction between Lemma 2.4 and Corollary 2.5\\
		{\normalfont 
		We only define the exact product variant of Corollary 2.5 in MP Setting 2.3. 
		With adding the TensorCore bFMAs, we felt that we didn't need two different types of mixed precision inner products. 
		}
	\end{itemize}
	\item Rounding error analysis framework
	\begin{itemize}\bfseries
		\item Add interpretation after every major result.\\ 
		{\normalfont
			As suggested, we moved the material from Appendix A into the main body. 
			Since we have added the bFMAs as a separate mixed precision setting, we first present the standard error analysis in section 3, present mixed precision variants of the 3 main algorithms (HQR, BQR, TSQR) and their rounding error analysis in section 4. 
			The error analysis in section 4 heavily relies on intermediate results from section 3, and references specific equations and lemmas established in section 3. 
		}
	\item Keep different precisions in the bound.\\
	{\normalfont
		In the first submission, we had converted bounds in the form of $\gamma_{k_1}^{(l)}+\gamma_{k_2}^{(h)}$ to $\gamma_{\lceil k_1+k_2\frac{u^h}{u^l}\rceil}^{(l)}$ by defining some constant $d\approx \lceil k_1+k_2\frac{u^h}{u^l}\rceil$. 
		Since it was difficult to determine how to relate $d$ back to the original problem size, we have eliminated this conversion and kept track of the errors in the low and high precision separately, as suggested. 
	}
	\item Use the $\tilde{\gamma}$ notation to avoid keeping track of all constants.\\
	{\normalfont
	We changed to using the $\tilde{\gamma}$ notation whenever it was possible to do so, and specifically mentioned whenever we applied the assumptions under this notation to get rid of a non-leading order terms (e.g. lines I's).
	}
	\end{itemize}
	
	\item Conclusions from the mixed precision HQR analysis (section 3). 
	The main results need to be highlighted, discussed in more depth
	\begin{itemize}\bfseries
		\item How do we feel about the low precision error term still having a dependence on $n$? Is there a way to drop this dependence?\\
		{\normalfont }
		\item Do the numerical experiments give any insight to the questions posed above?\\
		{\normalfont }
	\end{itemize}
	\item Conclusion from HQR vs TSQR comparison (section 4).
	\begin{itemize}\bfseries
		\item It may be beneficial to summarize the error bounds for HQR, TSQR, and their mixed precision variants in one place. 
		{\normalfont
			
		}
		\item Uniform precision comparison of HQR and TSQR needs to be simplified and clarified \\
		{\normalfont
		}
		\item Mixed precision comparison of HQR and TSQR needs to be emphasized, and not overshadowed by Figure 3. 
		\begin{itemize}\bfseries
			\item P18L531: The $(L+1)/2^L$ factor is reversed \\
			{\normalfont This is fixed and is now on line O of section 3.3.3.}
		\end{itemize}
	\end{itemize}
	
	\item Mislabel of forward and backward errors.\\
	{\normalfont
		We fixed these inaccuracies in the various locations noted by the referee.
	}
	\item P1L15: ``standard algorithms may no longer be numerically stable when using half precision''.\\
	{\normalfont
		We reworded this sentence to ``standard algorithms yield insufficient accuracy when using half precision'' as suggested since the definition of numerical stability is relative with respect to machine precision (line K, section 1).
	}
	\item P2L49: fp16 should be removed, bfloat should be bfloat16.\\
	{\normalfont
		We made this change in line J of section 1.
	}
	\item P5L125: Rewording is needed to clarify ``k represents the number of FLOPs'' \\
	{\normalfont
		A rewording of this sentence eliminated that phrase in line L of section 2.1. 
	}
	\item Title suggestion:\\
	{\normalfont
		We altered the title to: Rounding Error Analysis of Mixed Precision Block Householder QR Algorithms.
	}
	\item P1L20: what does ``weight'' refer to in this context?\\
	{\normalfont
		Weight refers to the physical weight of the hardware, which is a relevant feature in sensor formation literature (line M of section 1).
		We did not add an explanation in the text as it is just an example, and does not play a crucial role in the main contributions of this paper.
	}
%	\item P2L57 ``can successfully'' $\rightarrow$ ``can be succcessfully''.\\
%	{\normalfont
%		This portion of the introduction was removed since we omitted the graph application section.
%	}
%	\item P5L134: $\gamma_p^{(d+2)}$ has not been defined yet.\\
%	{\normalfont
%		
%	}
	\item P12L329: the middle term should be $(1+\dd_w)(x_1-\sigma-\Delta\sigma)$, rather than $(1+\dd_w)(\sigma+\Delta\sigma)$. Moreover, the last equality is only true because no cancellation can happen, since $x_1$ and $\sigma$ have the same sign: this should be commented on.
	{\normalfont
	The changes were made and the last equality being contingent on the special case that $x_1$ and $\sigma$ have the same sign was mentioned in line N of section 3.1.2.
	}
	\item Equations (4.6) and (4.7): isn't the $\sqrt{n}$ factor on the wrong equation?\\
	{\normalfont
		The $\sqrt{n}$ factor should be on the bounds for $\|\Delta\bb{Q}\|_F$ and $\|\hat{\bb{Q}}\hat{\bb{R}}-\bb{A}\|_F$.
		We checked for and fixed this inaccuracy in various locations throughout sections 3 and 4. 
	}
%	\item P18L527: ``for the a meaningful''.\\
%	{\normalfont
%		
%	}
%	\item P18L531: as mentioned above, the $(L+1)/2^L$ factor is reversed\\
%	{\normalfont
%		
%	}
	\item P20L599: I find it very strange that the backward error depends on the condition number of the matrix! Is it rather the forward error that is being plotted?\\
	{\normalfont
	
	}
	\item Section 5: given the relatively theoretical nature of this article, section 5 felt slightly out of place to me. Given that the article is quite long, perhaps the authors could consider including section 5 in another piece of work? \\
	{\normalfont We plan to include this section in another piece of work.}
	
\end{enumerate}

\bibliography{../../../../../library.bib,../../../../../sans_library.bib,./report.bib}
\bibliographystyle{siamplain}%ieeetr
\end{document}
