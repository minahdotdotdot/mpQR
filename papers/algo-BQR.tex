\subsection{Block HQR with partitioned columns (BQR)}\label{sec:BQR}
We refer to the blocked variant of HQR where the columns are partitioned as BQR. 
Note that this section relies on the WY representation described in \cite{Bischof1987} instead of the storage-efficient version of \cite{Schreiber1989}, even though both are known to be just as numerically stable as HQR.
%, which is widely implemented.
\subsubsection{The WY Representation}
A convenient matrix representation that accumulates $r$ Householder reflectors is known as the WY representation (see \cite{Bischof1987,golub2013matrix}).
\Cref{lem:WY} shows how to update a rank-$j$ update of the identity, $\bb{Q}^{(j)}$, with a Householder transformation, $\bb{P}$, to produce a rank-$(j+1)$ update of the identity, $\bb{Q}^{(j+1)}$. 
With the correct initialization of $\bb{W}$ and $\bb{Y}$, we can build the WY representation of successive Householder transformations as shown in \Cref{algo:buildWY}. 
This algorithm assumes that the Householder vectors, $\bb{V}$, and constants,$\bm{\beta}$, have already been computed.
Since the $\bb{Y}$ factor is exactly $\bb{V}$, we only need to compute the $\bb{W}$ factor.
\begin{lemma}\label{lem:WY}
	Suppose $\bb{X}^{(j)}=\bb{I}-\bb{W}^{(j)}\bb{Y}^{(j)\top}\in\R^{m\times m}$ is an orthogonal matrix with $\bb{W}^{(j)},\bb{Y}^{(j)}\in\R^{m\times j}$.
	Let us define $\bb{P}=\bb{I}-\beta\bb{vv}^{\top}$ for some $\bb{v}\in\R^m$ and let $\bb{z}^{(j+1)}=\beta\bb{X}^{(j)}\bb{v}$.
	Then, \[\bb{X}^{(j+1)} = \bb{X}^{(j)}\bb{P} = \bb{I} - \bb{W}^{(j+1)}\bb{Y}^{(j+1)\top}, \]where $ \bb{W}^{(j+1)} =[\bb{W}^{(j)}|\bb{z}]$ and $ \bb{Y}^{(j+1)}=[\bb{Y}^{(j)}|\bb{v}]$ are each $m$-by-$(j+1)$. 
\end{lemma}
%
%Let us now show the proof for \cref{lem:WY}.
%\begin{proof}
%	A direct right multiplication of $\bb{P}:=\bb{I}_m - \beta\bb{v}\bb{v}^{\top}$ onto $\bb{Q}$ can be written as
%	\begin{equation*}
%	\bb{QP}=\bb{Q}-\beta\bb{Q}\bb{v}\bb{v}^{\top}.
%	\end{equation*}
%	Let us use the WY representation of $\bb{Q}$.
%	\begin{equation*}
%	\bb{QP}= \bb{I}_m - \bb{WY}^{\top} -\beta\bb{Q}\bb{v}\bb{v}^{\top} = \bb{I}_m - \bb{WY}^{\top} - \bb{z}\bb{v}^{\top}
%	\end{equation*}
%	Now note that the two subtracted terms are exactly the updated WY factors:
%	\[ \bb{W}_+\bb{Y}_+^{\top} = [\bb{W} \quad \bb{z}]\begin{bmatrix}
%	\bb{Y}^{\top}\\ 
%	\bb{v}^{\top}
%	\end{bmatrix} = \bb{WY}^{\top} + \bb{z}\bb{v}^{\top}.\]
%\end{proof}

\begin{algorithm2e}
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	\KwIn{$\bb{V}\in\R^{m \times r}$, $\bm{\beta}\in\R^{r}$ where $m > r$.}	
	\KwOut{$\bb{W}$} 
	Initialize: $\bb{W}:=\bm{\beta}_1\bb{V}[:,1]$.\tcc*{$\bb{Y}$ is $\bb{V}$.}
	\For{$j=2:r$}{
		$\bb{z}\gets \bm{\beta}_j \left[\bb{V}[:,j] - \bb{W}\left(\bb{V}[:,1:j-1]^{\top}\bb{V}[:,j]\right)\right]$\\
		$\bb{W} \gets [\bb{W}\quad \bb{z}]$ \tcc*{Update $\bb{W}$ to an $m$-by-$j$ matrix.}
		%$\bb{Y} \gets [\bb{Y}\quad \bb{V}[:,j]]$ \tcc*{Update $\bb{Y}$.}
	}
	\Return $\bb{W}$
	\caption{$\bb{W},\bb{Y}\gets {\tt buildWY}(V, \bm{\beta})$: Given a set of householder vectors $\{\bb{V}[:,i]\}_{i=1}^r$ and their corresponding constants $\{\bm{\beta}_i\}_{i=1}^r$, form the final $\bb{W}$ and $\bb{Y}$ factors of the WY representation of $\bb{P}_1\cdots \bb{P}_r$, where $\bb{P}_i := \bb{I}_m - \bm{\beta}_i\bb{v}_i\bb{v}_i^{\top}$}
	\label{algo:buildWY}
\end{algorithm2e}

In HQR, $\bb{A}$ is transformed into an upper triangular matrix $\bb{R}$ by identifying a Householder transformation that zeros out a column below the diagonal, then applying that Householder transformation to the bottom right partition. 
For example, the $k^{th}$ Householder transformation finds an $m-k+1$ sized Householder transformation that zeros out column $k$ below the diagonal and then applies it to the $(m-k+1)$-by-$(n-k)$ partition of the matrix, $\bb{A}[k:m,k+1:n]$.
Since the $k+1^{st}$ column is transformed by the $k^{th}$ Householder transformation, this algorithm must be executed serially as shown in \cref{algo:hhQR}.
The highest computational burden at each iteration falls on \cref{algo:hhQR} line 6, which requires Level-2 BLAS operations when computed efficiently. \par

In contrast, BQR replaces this step with Level-3 BLAS operations by partitioning $\bb{A}$ into blocks of columns.
Let $\bb{A} = [\bb{C}_1 \cdots  \bb{C}_N]$ where $\bb{C}_1,\cdots,\bb{C}_{N-1}$ are each $m$-by-$r$, and $\bb{C}_N$ holds the remaining columns.
The $k^{th}$ block, $\bb{C}_k$, is transformed with HQR (\cref{algo:hhQR}), and the WY representation of these $r$ successive Householder transformations is constructed as in \cref{algo:buildWY}.
We write the WY update as
\begin{equation}
	\bb{X}_k = \bb{I}_m -\bb{W}_{k}\bb{Y}_{k}^{\top} = \bb{P}_k^{(1)}\cdots\bb{P}_{k}^{(r)}.
\end{equation}
Thus far, \cref{algo:hhQR,algo:buildWY} are rich in Level-2 BLAS operations.
Next, $\bb{I} -\bb{Y}_{k}\bb{W}_{k}^{\top}$ is applied to $[\bb{C}_2 \cdots  \bb{C}_N]$ with two Level-3 BLAS operations as shown in line 5 of \cref{algo:blockHQR}.
BQR performs approximately $1-\cO(1/N)$ fraction of its FLOPs in Level-3 BLAS operations (see section 5.2.3 of \cite{golub2013matrix}), and can reap the benefits from the accelerated block FMA feature of TensorCore. 
Note that BQR does require strictly more FLOPs when compared to HQR, but these additional FLOPs are negligble in standard precision and does not impact the numerical stability.
%TODO: some statement about experiment about this in low/mixed precision
A pseudoalgorithm for BQR is shown in \cref{algo:blockHQR} where we assume that $n=Nr$ to make our error analysis in \cref{sec:BQRerr} simple.
In practice, an efficient implementation might require $r$ to be a power of two or a product of small prime factors and result a thinner $N^{th}$ block compared to the rest. 
This discrepancy is easily fixed by padding the matrix with zeros, a standard procedure for standard algorithms like the Fast Fourier Transform (FFT).
%TODO: cite?
For any variable $x\in\{\bb{X},\bb{W}, \bb{Y}, \bb{z}, \beta, \bb{v}, \bb{P}\}$,  $x_k^{(j)}$ corresponds to the $j^{th}$ update for the $k^{th}$ block.
%Note that the subscripts on $\bb{W}_{k},\bb{Y}_k$ indicate the WY representation for the Householder transformations on the $k^{th}$ block of $\bb{A}$, $\bb{C}_{k}$, whereas the superscripts on $\bb{W}_k^{(j)}$ in \cref{lem:WY} refers to the $j^{th}$ update within building a WY representation. 
\begin{algorithm2e}
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	\KwIn{$\bb{A}\in\R^{m \times n}$, $r\in\R$ where $r < n$.}
	\KwOut{$\bb{Q},\bb{R}$}
	$N=\frac{n}{r}$\\
	\tcp{Let $\bb{A} = [\bb{C}_{1} \cdots  \bb{C}_{N}]$ where all blocks except $\bb{C}_{N}$ are $m$-by-$r$ sized.}
	%\tcp{Let $n_i=ri$ for $i=1:N-1$ and $n_N=n$.} 
	\For{$i=1:N$}{
		$\bb{V}_{i},\bm{\beta}_i,\bb{C}_{i}\gets$ {\tt hhQR}($\bb{C}_{i}$)\tcc*{\Cref{algo:hhQR}}
		%$\bb{V}_i,\bm{\beta}_i,\bb{A}_{n_{i-1}+1:m,n_{i-1}+1:n_i}\gets$ {\tt hhQR}$(\bb{A}_{n_{i-1}:m,n_{i-1}+1:n_i})$\tcc*{\Cref{algo:hhQR}}
		$\bb{W}_{i}\gets $ {\tt buildWY}$(\bb{V}_{i},\bm{\beta}_i)$ \tcc*{\Cref{algo:buildWY}}
		$[\bb{C}_{i+1}\cdots\bb{C}_{N}]$ -= $\bb{V}_{i} \left(\bb{W}_{i}^{\top}[\bb{C}_{i+1}\cdots\bb{C}_{N}]\right) $ \tcc*{update the rest: BLAS-3}
	}
	%	\tcp{Upper-triangularize the last set of columns and get the last WY factors.}
	%	$\bb{V}_N,\bm{\beta}_N,\bb{A}_{n_{}:m,n_{N-1}+1:n}\gets$ {\tt hhQR}$(\bb{A}_{m_N+1:m,n_{N-1}+1:n})$\tcc*{\Cref{algo:hhQR}}
	%	$\bb{W}_N,\bb{Y}_N \gets $ {\tt buildWY}$(\bb{V}_N,\bm{\beta}_N)$ \tcc*{\Cref{algo:buildWY}}
	\tcp{$\bb{A}$ has been transformed into $\bb{R}=\bb{Q}^{\top}\bb{A}$.}
	\tcp{Now build $\bb{Q}$ using level-3 BLAS operations.} 
	$\bb{Q}\gets \bb{I}$\tcc*{$\bb{I}_m$ if full QR, and $\bb{I}_{m\times n}$ if thin QR.}
	\For{$i=N:-1:1$}{
		$\bb{Q}[(i-1)r+1:m,(i-1)r+1:n]$-= $\bb{W}_i \left(\bb{V}_i^{\top}\bb{Q}[(i-1)r+1:m,(i-1)r+1:n]\right)$
	}
	\Return $\bb{Q},\bb{A}$
	\caption{\label{algo:blockHQR} $\bb{Q},\bb{R}\gets {\tt blockHQR}(\bb{A}, r)$: Perform Householder QR factorization of matrix $\bb{A}$ with column partitions of size $r$.}
\end{algorithm2e}
\subsubsection{BQR: Rounding Error Analysis}\label{sec:BQRerr}
We now present the basic structure for the rounding error analysis for \cref{algo:blockHQR}, which consist of: 1)HQR, 2)building the W factor, and 3) updating the remaining blocks with the WY representation.
We have adapted the analysis from \cite{Higham2002} to fit this exact variant, and denote $\hat{\bb{Q}}_{BQR},\hat{\bb{R}}_{BQR}$ to be the outputs from \cref{algo:blockHQR}.
First, we analyze the error accumulated from updating $\bb{X}_k^{(j-1)}$ to $\bb{X}_k^{(j)}$, which applies a rank-1 update via the subtraction of the outer product $\hat{\bb{z}}_{k}^{(j)}\hat{\bb{v}}_{k}^{(j)\top}$.
Since $\bb{z}_{k}^{(j)} = \beta_k^{(j)}\bb{X}_{k}^{(j-1)}\bb{v}_{k}^{(j)}$, this update requires a single Householder transformation on the right side in the same efficient implementation that is discussed in \cref{eqn:effH},
\begin{equation}
\hat{\bb{X}_k^{(j)}} = 
%\bb{I}-\begin{bmatrix}
%\hat{\bb{W}}_k^{(j-1)}& \fl(\bb{z}_k^{(j)})
%\end{bmatrix}\begin{bmatrix}
%\hat{\bb{Y}}_k^{(j-1)\top} \\
%\hat{\bb{v}}_k^{(j)\top}
%\end{bmatrix} = 
\hat{\bb{X}}_k^{(j-1)} - \fl(\hat{\beta}_k^{(j-1)}\hat{\bb{X}}_k^{(j-1)}\hat{\bb{v}}_k^{(j-1)})\hat{\bb{v}}_k^{(j)\top} = \hat{\bb{X}}_k^{(j-1)}(\bb{P}_k^{(j)}+\Delta \bb{P}_k^{(j)}), \label{eqn:Xupdate}
\end{equation}
where $\|\Delta \bb{P}_k^{(j)}\|_F \leq \tilde{\gamma}_{m-(k-1)r}$.
Since $\hat{\bb{X}}_k^{(1)} = \bb{I} - \hat{\beta}_k^{(1)}\hat{\bb{v}}_k^{(1)}\hat{\bb{v}}_k^{(1)\top} = \bb{P}_k^{(1)} + \Delta \bb{P}_k^{(1)}$, we can travel up the recursion relation in \cref{eqn:Xupdate} and use \cref{lem:3.7} to find 
\begin{equation}
	\|\Delta \bb{X}_k^{(j)} \|_F \leq j\tilde{\gamma}_{m-(k-1)r}. \label{eqn:deltX}
\end{equation}

\paragraph{HQR within each block: line 3 of \cref{algo:blockHQR}}
We apply \Cref{algo:hhQR} to the $k^{th}$ block, $\hat{\bb{X}}_{k-1}\cdots\hat{\bb{X}}_1\bb{C}_k$, which applies $r$ more Householder transformations to columns that had been transformed by $(k-1)$ WY transformations in prior iterations.
The upper trapezoidal factor that results from applying HQR to $\bb{C}_{k}^{((k-1)r)}$ corresponds to the $(k-1)r+1^{st}$ to $kr^{th}$ columns of $\hat{\bb{R}}_{BQR}$, and applying \cref{lem:3.7,lem:19.3} yields
\begin{equation*}
	\|\hat{\bb{R}}_{BQR}[:,j]-\bb{R}[:,j]\|_2 \leq r\tilde{\gamma}_{m}\|\hat{\bb{X}}_{k-1}\cdots\hat{\bb{X}}_1^{\top}\bb{C}_k[:,j]\|_2,\;\; j=(k-1)r+1:kr.%\leq kr\tilde{\gamma}_{m}\|\bb{A}[:,j]\|_2,\;\; j=(k-1)r+1:kr.
\end{equation*}
%where $\bb{A}^{((k-1)r)}[:,j]$ has been transformed with $k-1$ WY updates already.
%Let $\bb{R}_{k}$ be the $\bb{R}$ factor acquired from applying HQR on $\bb{C}_k$ : $ \hat{\bb{R}}_{k} = \bb{Q}^{\top} (\bb{C}_{k} + \Delta \bb{C}_{k}), $ where $\Delta \bb{C}_{k}$ represent the backward error incurred in this step. 
%Then, applying \cref{lem:19.3} to the $j^{th}$ column of $\bb{C}_{k}$ yields 
%\begin{equation*}
%	\|\Delta\bb{C}_k[:,j]\|_2 = \leq j\tilde{\gamma}_{m-(k-1)r} \|\bb{ C}_k[:,j]\|_2.
%\end{equation*}
%We can similarly apply \cref{thm:feHQR} for block-wise norm errors. 
%Adjusting \cref{eqn:deltQ} for $k<N$ yields
%\begin{equation}
%\|\Delta \bb{Q'}\|_2\leq \|\Delta \bb{Q'}\|_F = \|\Delta \bb{Q'}^{\top}\|_F\leq r \tilde{\gamma}_{m-(i-1)k} \label{eqn:deltQBQR}
%\end{equation}
%\begin{align}
%\|\Delta \bb{Q'}_k^{\top}\|_F &= \left|\left| \prod_{j=r}^{1}\left(\bb{P}_{(k-1)r+j} +\Delta\bb{P}_{(k-1)r+j}\right) - \prod_{j=r}^{1}\bb{P}_{(k-1)r+j} \right|\right|_F,\label{eqn:rPs-BQR}\\
%&\leq \left(\prod_{i=1}^r(1+\tilde{\gamma}_{m-(r-1)k})-1\right)\prod_{i=r}^1\|\bb{P}_i\|_2 = (1+\tilde{\gamma}_{m-(r-1)k})^r-1. \label{eqn:rPs-BQR-uniform}
%\end{align}
%Since the $j^{th}$ Householder vector for $\bb{C}_{k}$ zeros out the $r(k-1)+j^{th}$ column of $\bb{A}$ below the diagonal, it has length  $m -(k-1)r-j+1$. 
%This corresponds to setting $i=r(k-1)+j$ in \cref{eqn:vbound,eqn:beta3}
\paragraph{Build WY at each block: line 4 of \cref{algo:blockHQR}}
%The WY representation of $r$ Householder transformations in full precision is exactly that, just another representation. 
We now calculate the rounding errors incurred from building the WY representation when given a set of Householder vectors and constants as shown in \cref{algo:buildWY}.
%Consider the $j^{th}$ update of the WY representation for block $\bb{C}_{k}$.
%Our goal is to analyze the error accumulated from updating the WY representation from the $j-1^{st}$ step to the $j^{th}$ for block $\bb{C}_{k}$.
%Let us represent the $j^{th}$ Householder constant and vector of the $k^{th}$ block computed with FLOPs as with $\hat{\beta}_k^{(j)}$ and $\hat{\bb{v}}_{k}^{(j)}$ and the $j^{th}$ update to the WY representation as $$\bb{X}_{k}^{(j)} = \bb{I} - \hat{\bb{W}}_{k}^{(j)}\hat{\bb{Y}}_{k}^{(j)\top}.$$ %we write $\bb{V}_k[:,1:j]=:\bb{Y}_k^{(j)}$ to make a clear connection to \cref{lem:WY}, and also denote
%The update in \cref{lem:WY} applies a rank-1 update via the subtraction of the outer product $\hat{\bb{z}}_{k}^{(j)}\hat{\bb{v}}_{k}^{(j)\top}$ to apply $\hat{\bb{P}}_{\bb{(k-1)r+j}}\equiv \hat{\bb{P}}_k^{(j)}$ on the right. 
%Since $\bb{z}_{k}^{(j)} = \beta_k^{(j)}\bb{X}_{k}^{(j-1)}\bb{v}_{k}^{(j)}$, this update requires a single Householder transformation in the same efficient implementation that is discussed in \cref{eqn:effH}, but on the right side:
%\begin{align*}
%	\bb{X}_{k}^{(j)} &=\bb{X}_{k}^{(j-1)} - \bb{z}_k^{(j)}\bb{v}_k^{(j)\top}\\ %&=\bb{X}_{k}^{(j-1)}-\beta_k^{j}\bb{X}_{k}^{(j-1)}\bb{v}_{k}^{(j)}\bb{v}_{k}^{(j-1)\top} \\
%	&= \bb{X}_{k}^{(j-1)} (\bb{I}-\beta_k^{(j)}\bb{v}_{k}^{(j)}\bb{v}_{k}^{(j)\top})=  \bb{X}_{k}^{(j-1)}\bb{P}_{k}^{(j)}.
%\end{align*}
%The analysis for this update is slightly different as it is applied to a matrix, but the componentwise error remains the same
%Traveling up this recursion relation to $j=1$, we find that $\hat{\bb{X}}_{k}^{(j)}$ is built from applying a sequence of Householder transformations $\{\hat{\bb{P}}_{k}^{(2)},\cdots \hat{\bb{P}}_k^{(j)}\}$ to $\bb{P}_k^{(1)}$.
%Therefore, we can apply \cref{eqn:19.3} (set $\bb{Q}=\bb{X}_{k}^{(j-1)}$, $\bb{x}=\fl(\hat{\beta}_k^{(j)}\hat{\bb{v}}_{k}^{(j)})$, and $\bb{y}=\bb{z}_{k}^{(j)}$) to form \cref{eqn:BQR-z}.
% directly for the construction of $\bb{z}_{k}^{(j)}$
Since the columns of $\hat{\bb{Y}}_k$ are simply $\{\hat{\bb{v}}_k^(j)\}$ built in \cref{algo:hhQR} the errors for forming these are shown in \cref{eqn:vbound} where $m$ should be replaced by $m-(k-1)r$.
The Householder constants, $\hat{\beta}_k^{(j)}$ are bounded by \cref{eqn:beta3} modified similarly. 
Thus, $\bb{z}_k^{(j)}$ is the only newly computed quantity. 
Using \cref{eqn:deltX,eqn:vbound,eqn:beta3}, we find
\begin{align*}
\|\Delta \bb{z}_k^{(j)}\|_2 &= \|\Delta\bb{X}_k^{(j-1)}\hat{\beta}_k^{(j)}\hat{\bb{v}}_k^{(j)} \|_2 \leq \|\Delta\bb{X}_k^{(j-1)}\|_2 \|\hat{\beta}_k^{(j)}\hat{\bb{v}}_k^{(j)}\|_2\\
& \leq \|\Delta\bb{X}_k^{(j)-1}\|_F\|\hat{\beta}_k^{(j)}\hat{\bb{v}}_k^{(j)}\|_2 \\
& \leq \left((1+(j-1)\tilde{\gamma}_{m-(k-1)r})(1 + \tilde{\gamma}_{m-(k-1)r})-1\right) \| \beta_k^{(j)}\bb{v}_k^{(j)}\|_2 \\
& \leq j\tilde{\gamma}_{m-(k-1)r}\|\bb{z}_k^{(j)}\|_2 
%	\|\Delta \hat{\bb{z}}_k^{(j)}\|_2 &= \|\bb{X}_k^{(j)-1}(\hat{\beta}_k^{(j)}\hat{\bb{v}}_k^{(j)}-\beta_k^{(j)}\bb{v}_k^{(j)}) + \Delta \bb{X}_k^{(j-1)}\hat{\beta}_k^{(j)}\hat{\bb{v}}_k^{(j)} \|_2 \\
%	&\leq \|\bb{X}_k^{(j)-1}(\hat{\beta}_k^{(j)}\hat{\bb{v}}_k^{(j)}-\beta_k^{(j)}\bb{v}_k^{(j)})\|_2 + \|\Delta \bb{X}_k^{(j-1)}\hat{\beta}_k^{(j)}\hat{\bb{v}}_k^{(j)}\|_2 \\
%&\leq \tilde{\gamma}_{m-(k-1)r}\|\bb{X}_k^{(j-1)}\|_2\|\beta_k^{(j)}\bb{v}_k^{(j)}\|_2 + \|\Delta \bb{X}_k^{(j-1)}\|_F\|\hat{\beta}_k^{(j)}\hat{\bb{v}}_k^{(j)}\|_2 \\
\end{align*}
Componentwise bounds follow immediately, and are summarized in \cref{lem:BQR-build}.
\begin{lemma}\label{lem:BQR-build}
	Consider the construction of the WY representation for the $k^{th}$ partition of matrix $\bb{A}\in\R^{m\times n}$ given a set of Householder constants and vectors, $\{\beta_k^{(j)}\}_{j=1}^r$ and $\{\bb{v}_{k}^{(j)}\}$ via \cref{algo:buildWY}.
	Then, 
	\begin{align}
		\hat{\bb{z}}_{k}^{(j)}& = \bb{z}_{k}^{(j)} + \Delta \bb{z}_{k}^{(j)},\;\; |\Delta \bb{z}_{k}^{(j)}| \leq j\tilde{\gamma}_{m-(k-1)r} |\bb{z}_{k}^{(j)}|\label{eqn:BQR-z}\\
		\hat{\bb{v}}_{k}^{(j)}& = \bb{v}_{k}^{(j)} + \Delta \bb{v}_{k}^{(j)},\;\; |\Delta \bb{v}_{k}^{(j)}| \leq \tilde{\gamma}_{m-(k-1)r} |\bb{v}_{k}^{(j)}|\label{eqn:BQR-v},
	\end{align}
	where the second bound is derived from \cref{eqn:vbound}.
\end{lemma}
Most importantly, this shows that constructing the WY update is just as numerically stable as applying successive Householder transformations (see Section 19.5 of \cite{Higham2002}).

\paragraph{Update blocks to the right: line 5 of \cref{algo:blockHQR}}
We now consider applying $\bb{X}_{k}:=\bb{I}-\bb{W}_k\bb{Y}_k^{\top}$ to some matrix, $\bb{B}$.
In practice, $\bb{B}$ is the bottom right submatrix, $[\bb{C}_{k+1}\cdots \bb{C}_{N}][(k-1)r+1:m,:]$.
%In practice, this step is performed with a level-3 BLAS operation. 
%Regardless, 
We can apply \cref{eqn:deltX} directly to the columns of $\bb{B}$, 
\begin{align}
	\|\fl(\hat{\bb{X}}_k \bb{B}[:,j])\|_2 = \|\fl(\hat{\bb{X}}_k^{(r)} \bb{B}[:,j])\|_2 \leq r\tilde{\gamma}_{m-(k-1)r} \|\bb{B}[:,j]\|_2
\end{align}
A normwise bound for employing a general matrix-matrix multiplication operation is stated in section 19.5 of \cite{Higham2002}.
%\begin{align*}
%%\fl(\hat{\bb{Y}}_k^{\top}\bb{b}) &= (\hat{\bb{Y}}_k + \tilde{\Delta} \bb{Y}_k)^{\top}\bb{b},\\
%\hat{\bb{y}}_k&=\fl(\hat{\bb{X}}_{k}\bb{b})=\fl(\bb{b} - \fl(\hat{\bb{W}}_{k}\fl(\hat{\bb{Y}}_{k}^{\top}\bb{b}))) \\
%&=(1+\dd)(\bb{b} - (\hat{\bb{W}}_{k} + \tilde{\Delta} \bb{W}_{k}) (\hat{\bb{Y}}_{k} + \tilde{\Delta} \bb{Y}_{k})^{\top}\bb{b}),\\
%&=\bb{X}_k\bb{b}+\Delta\bb{y}_k= (\bb{X}_{k} + \Delta \bb{X}_{k})\bb{b},
%\end{align*}
%where $\tilde{\Delta} \bb{W}_{k}$ and $\tilde{\Delta} \bb{Y}_{k}$ each represent the backward error for a matrix-vector multiply with inner products of lengths $m-(k-1)r$ and $r$ and $\hat{\bb{W}}_{k},\hat{\bb{Y}}_k$ include the errors from forming the WY representation (see \cref{lem:BQR-build}).
%%= \bb{W}_{k}+\Delta \bb{W}_{k}$, $\hat{\bb{Y}}_{k} = \bb{Y}_{k}+\Delta \bb{Y}_{k}$ where the columns of $\Delta \bb{W}_{k}$,$\Delta \bb{Y}_{k}$ are bounded by \cref{eqn:BQR-z,eqn:BQR-v}.
%Since $|\tilde{\Delta} \bb{W}_{k}| \leq \gamma_{m-(k-1)r}|\hat{\bb{W}}_{k}|$ and $|\tilde{\Delta} \bb{Y}_{k}| \leq \gamma_{r}|\hat{\bb{Y}}_{k}|$, they are small compared to the errors from forming $\hat{\bb{W}}_k$,$\hat{\bb{Y}}_k$, and we result in
%\begin{equation*}
%\|\Delta \bb{y}_k\|_2= \|\fl(\hat{\bb{X}}_{k}\bb{b}) - \bb{X}_k\bb{b}\|_2 \leq  r\tilde{\gamma}_{m-(k-1)r}\|\bb{b}\|_2.
%%\left(|\bb{b}_j|+|\hat{\bb{W}}_k||\hat{\bb{Y}}_k|^{\top}|\bb{b}_j|\right),
%\end{equation*}
%where $\tilde{\gamma}_{\bb{X}_{k}}$ accounts for the errors from perturbations $\tilde{\Delta}\bb{W}_k+\Delta \bb{W}_k$, $\tilde{\Delta}\bb{Y}_k + \Delta \bb{Y}_k$, and $\dd$.
%In uniform precision, this is largely derived from 
%\begin{align*}
%	\fl(\hat{\bb{W}}_k\fl(\hat{\bb{Y}}_k^{\top}\bb{b}_j)) &= (1+\tth_{m-(k-1)r})\hat{\bb{W}}_k\left[(1+\tth_{r}) \hat{\bb{Y}}_k^{\top}\bb{b}_j)\right] \\
%	&= (1+\tth_{m-(k-2)r})\hat{\bb{W}}_k\hat{\bb{Y}}_k^{\top}\bb{b}_j,\\
%	\|\hat{\bb{W}}_k\hat{\bb{Y}}_k^{\top}\bb{b}_j - \bb{W}_k\bb{Y}_k\bb{b}_j\|_2 &= \left(\sum_{j=1}^r \left(\bb{v}_k^{(j)}+\Delta \bb{v}_k^{(j)}\right)^{\top}\bb{b}_j\left(\bb{z}_k^{(j)}+\Delta \bb{z}_k^{(j)}\right)\right)-\left(\sum_{j=1}^r\bb{v}_k^{(j)\top}\bb{b}_j\bb{z}_k^{(j)}\right)
%\end{align*}
%\begin{align*}
%|(\bb{W}_{k} +\Delta \bb{W}_{k} &+ \tilde{\Delta} \bb{W}_{k}) (\bb{Y}_{k} + \Delta \bb{Y}_{k} + \tilde{\Delta} \bb{Y}_{k})^{\top}\bb{b}_j - \bb{W}_{k} \bb{Y}_{k}^{\top}\bb{b}_j| \\
%&\leq \left[(1+r\tilde{\gamma}_{m-(k-1)r}+ \gamma_{r})(1 + \tilde{\gamma}_{m-(k-1)r}+ \gamma_{m-(k-1)r})-1\right]|\bb{W}_k||\bb{Y}_k|^{\top}|\bb{b}_j| \\ 
%&\leq r\tilde{\gamma}_{m-(k-1)r}|\bb{W}_k||\bb{Y}_k|^{\top}|\bb{b}_j|,\;\; \gamma_{\bb{X}_{k}}:= r\tilde{\gamma}_{m-(k-1)r},
%\end{align*}
%since the subtraction step only adds a single rounding error. 
% I have used that $\bb{W}_{k}$ accrues additional rounding errors compared to $\bb{Y}_{k}$.
%Note that we implicitly covered the same step of applying an WY update in the construction of $\bb{z}_{k}^{(j)}$, but used \cref{lem:19.3} instead since we were concerned with the error occurred at a single update. 
%Now, we leave this error in a general form to allow for different mixed precision settings in \cref{sec:mpanalysis}.
%As a result of introducing generality, 
%Since $\Delta \bb{X}_k = \Delta\bb{y}_k\bb{b}^{\top}/\|\bb{b}\|_2^2$, we conclude with a backward matrix norm bound, 
%\begin{equation}
%\fl(\hat{\bb{X}}_{k}\bb{b}) = (\bb{X}_{k} + \Delta \bb{X}_{k})\bb{b}, \;\; \|\Delta \bb{X}_{k}\|_F \leq r\tilde{\gamma}_{m-(k-1)r}.\label{eqn:Xkbound}
%\end{equation}
%We showed earlier in this section that HQR performed on $\bb{C}_{k}$ accrues error of order $\tilde{\gamma}_{m-(k-1)r}$ by applying \cref{thm:feHQR}, and the building of the W factor, $r\tilde{\gamma}_{m-(k-1)r}$ order error.
%These combined with the error in \cref{eqn:BQR} still amount to $cO(r\tilde{\gamma}_{m-(k-1)r})$, so we conclude that.
% is stated in \cref{lem:WYnorm}.
%\begin{lemma}\label{lem:WYnorm}
%	Consider applying the WY transformation of \cref{lem:WY} to a matrix $\bb{B}\in\R^{m\times n-r}$ as would be done at the first iteration of line  in \cref{algo:blockHQR}.	%The WY factors $\hat{\bb{W}},\hat{\bb{Y}}\in\R^{m\times r}$ formed via \cref{algo:buildWY} are slight perturbations of their exact arithmetic variants.	%	Suppose that 
%	\begin{equation*}	
%	\hat{\bb{Q}}:=\fl(\bb{I}-\hat{\bb{W}}\hat{\bb{Y}}^{\top}) = \bb{U} + \Delta\bb{U},\quad
%		\hat{\bb{W}} = \bb{W} + \Delta\bb{W},\quad
%		\hat{\bb{W}} = \bb{Y} + \Delta\bb{Y},
%	\end{equation*}
%	where $\bb{U}$ is orthogonal and 
%	\begin{equation*}
%		\|\Delta\bb{U}\|_2 \leq ud_1, \quad
%		\|\Delta\bb{W}\|_2  \leq d_2, \quad
%		\|\Delta\bb{Y}\|_2 \leq d_3.
%	\end{equation*}
%	Finally, we denote the rounding errors incurred from a level-3 BLAS matrix-matrix multiplication of matrices of sizes $p$-by-$t$ and $t$-by-$q$ to be represented by a function, $c(p,t,q)$.
%	Then, 
%	\begin{align}
%		\|\fl(\hat{\bb{Q}}\bb{B}) - \bb{QB}\|_2 \leq u\left[1+d_1+d_2d_3\left(1+c(m,r,n-r)+c(r,m,n-r)\right)\right] \|\bb{B}\|_2 + \cO(u^2).
%	\end{align}
%\end{lemma}
\paragraph{Multiple WY updates: line 8-9 of \cref{algo:blockHQR}}
All that remains is to consider the application of successive WY updates to form the QR factorization computed with BQR denoted as $\bb{Q}_{BQR}$ and $\bb{R}_{BQR}$. 
We can apply \cref{lem:3.7} directly by setting $\bb{X}_{k}:= \bb{I}-\bb{W}_{k}\bb{Y}_{k}^{\top}$ and consider the backward errors for applying the sequence to a vector, $\bb{x}\in\R^{m}$, as we did for \cref{lem:19.3}. 
Since $\bb{X}_{k}=\bb{P}_{(k-1)r+1}\cdots\bb{P}_{kr}$, is simply a sequence of Householder transformations, it is orthogonal, i.e. $\|\bb{X}_{k}\|_2=1$.
We only need to replace with $\bb{x}$ with $\bb{A}[:,i]$'s to form the columnwise bounds for $\bb{R}_{BQR}$, and apply the transpose to $\hat{e}_i$'s to form the bounds for $\bb{Q}_{BQR}$. 
%We define the backward error as $\Delta \bb{Q'}_{BQR}) \bb{b}$.
%Then, this backward error is quantified via
Then, 
\begin{align}
\left|\left|\prod_{k=1}^N (\bb{X}_{k} + \Delta \bb{X}_{k})- \prod_{k=1}^N\bb{X}_{k} \right|\right|_F &\leq\left(-1+\sum_{k=1}^N (1+r\tilde{\gamma}_{m-(k-1)r})\right) \leq rN\tilde{\gamma}_m \equiv n\tilde{\gamma}_m ,\label{eqn:BQR-mp}\\%&\leq \sum_{k=1}^N (1+ r\tilde{\gamma}_{m-(k-1)r})-1 \leq rN\tilde{\gamma}_m \equiv n\tilde{\gamma}_m ,\label{eqn:BQR-mp}\\
\|\hat{\bb{Q}}_{BQR}-\bb{Q}\|_F&\leq n^{3/2}\tilde{\gamma}_m. \label{eqn:BQR}
\end{align}
%\begin{align}
%\Delta \bb{Q'}_{BQR} &:= \prod_{k=N}^{1}\left(\bb{Q}_{k} +\Delta \bb{Q}_{k}\right) - \prod_{k=N}^{1}\bb{Q}_{k} ,\\
%%\|\Delta\bb{Q}'^{\top}\|_F &= \left|\left| \prod_{k=1}^{N}\left(\bb{Q}_k +\Delta\bb{P}_i\right) - \prod_{i=r}^{1}\bb{P}_i \right|\right|_F,\\
%\|\Delta \bb{Q'}_{BQR}\| & \leq \sum_{k=N}^1 (1+\tilde{\gamma}_{\bb{Q}_{k})  -1 \label{eqn:BQR-mp}\\
%&\leq \sum_{k=N}^1 (1+ r\tilde{\gamma}_{m-(k-1)r})-1 \leq rN\tilde{\gamma}_m \equiv n\tilde{\gamma}_m \label{eqn:BQR}.
%\end{align}
We can also form the normwise bound for the $j'^{\ th}$ column of $\hat{\bb{Q}}_{BQR},\hat{\bb{R}}_{BQR}$. 
If we let $k' = \lceil j'/r\rceil^{th}$, then the $j'^{\ th}$ column is the result of applying $k'-1$ WY updates and an additional HQR. 
Applying \cref{lem:3.7} yields 
\begin{align}
\|\Delta \bb{R}_{BQR}[:,j']\|_2 \leq rk'\tilde{\gamma}_{m} \|\bb{A}[:,j]\|_2 \\
\|\Delta \bb{Q}_{BQR}[:,j']\|_2 \leq rk'\tilde{\gamma}_{m}\label{eqn:BQRcol}
\end{align}
and near orthogonality of the $\bb{Q}$ factor is still achieved,
\begin{equation}
	\|\Delta \bb{Q}_{BQR}\|_F = r\tilde{\gamma}_{m}\sum_{j=1}^n \lceil j/r\rceil = n^{3/2}\tilde{\gamma}_{m}.\label{eqn:BQRmat}
\end{equation}
The primary goal of the analysis presented in this section is to provide the basic skeleton for the standard BQR rounding error analysis to make the generalization to mixed precision settings in \cref{sec:mpanalysis} easier.
Readers should refer to \cite{golub2013matrix,Higham2002} for full details.