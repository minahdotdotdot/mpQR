\subsection{Householder QR (HQR)}\label{sec:HQR}
The HQR algorithm uses Householder transformations to zero out elements below the diagonal of a matrix (see \cite{Householder1958}). 
We present this as zeroing out all but the first element of some vector, $\bb{x}\in\R^m$.
\begin{lemma}
	Given vector $\bb{x}\in\R^{m}$, there exist Householder vector, $\bb{v}$, and |Householder transformation matrix, $\bb{P}_{\bb{v}}$, such that $\bb{P}_{\bb{v}}$ zeros out $\bb{x}$ below the first element. 
	\begin{equation}
	\begin{alignedat}{3} 
	\sigma =& -\rm{sign}(\bb{x}_1)\|\bb{x}\|_2, &&\quad  \bb{v} = \bb{x} -\sigma \hat{e_1},\\
	\beta = & \frac{2}{\bb{v}^{\top}\bb{v}}=-\frac{1}{\sigma\bb{v}_1}, && \quad \bb{P}_{\bb{v}}=  \bb{I}_{m} - \beta \bb{v}\bb{v}^{\top}.
	\end{alignedat}
	\label{eqn:HH} 
	\end{equation}
	The transformed vector, $\bb{P_vx}$, has the same 2-norm as $\bb{x}$ since Householder transformations are orthogonal: $\bb{P}_{\bb{v}}\bb{x} = \sigma\hat{\bb{e}_1}$.
	In addition, $\bb{P}_{\bb{v}}$ is symmetric and orthogonal, $\bb{P}_{\bb{v}}=\bb{P}_{\bb{v}}^{\top}=\bb{P}_{\bb{v}}^{-1}$.
	\label{lem:hhvec}
\end{lemma}
\subsubsection{HQR: Algorithm}
Given $\bb{A}\in\R^{m\times n}$ and Lemma \ref{lem:hhvec}, HQR is done by repeating the following processes until only an upper triangle matrix remains.
For $i = 1, 2, \cdots, n,$
\begin{enumerate}[Step 1)]
	\item Compute $\bb{v}$ and $\beta$ that zeros out the $i^{th}$ column of $\bb{A}$ beneath $a_{ii}$ (see \cref{algo:hh_v2}), and
	\item Apply $\bb{P}_{\bb{v}}$ to the bottom right partition, $\bb{A}[i:m, i:n]$ (lines 4-6 of \cref{algo:hhQR}).
\end{enumerate}

Consider the following $4$-by-$3$ matrix example adapted from \cite{Higham2002}. 
Let $\bb{P_i}$ represent the $i^{th}$ Householder transformation of this algorithm. 
\[\bb{A} = \left[ \begin{array}{ccc}
\times & \times & \times \\
\times & \times & \times \\
\times & \times & \times \\
\times & \times & \times
\end{array}
\right]\xrightarrow{\text{apply $\bb{P_1}$ to $\bb{A}$}}\left[ \begin{array}{c|cc}
\times & \times & \times \\ \hline
0 & \times & \times \\
0 & \times & \times \\
0 & \times & \times
\end{array}
\right]
\xrightarrow{\text{apply $\bb{P_2}$ to $\bb{P_1}\bb{A}$}}\]
\[ \left[
\begin{array}{cc|c}
\times & \times & \times \\
0 & \times & \times \\ \hline
0 & 0 & \times \\
0 & 0 & \times 
\end{array} \right]
\xrightarrow{\text{apply $\bb{P_3}$ to $\bb{P_2}\bb{P_1}\bb{A}$}} \left[ \begin{array}{ccc}
\times & \times & \times \\
0 & \times & \times \\
0 & 0 & \times \\
0 & 0 & 0 
\end{array}\right] = \bb{P_3}\bb{P_2}\bb{P_1}\bb{A}=:\bb{R} \] 
Then, the $\bb{Q}$ factor for a full QR factorization is $\bb{Q}:=\bb{P_1}\bb{P_2}\bb{P_3}$ since $\bb{P_i}$'s are symmetric, and the thin factors for a general matrix $\bb{A}\in\R^{m\times n}$ are
\begin{equation}
\bb{Q}_{\text{thin}} = \bb{P_1} \cdots \bb{P_n}\bb{I}_{m\times n}\quad \text{and} \quad \bb{R}_{\text{thin}} = \bb{I}_{m\times n}^{\top}\bb{P_n}\cdots \bb{P_1}\bb{A}.
\end{equation}

\begin{algorithm2e}[H]
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	\KwIn{$\bb{x}\in\R^m$}
	\KwOut{$\bb{v}\in\R^m$, and $\sigma, \beta\in\R$ such that $(I-\beta \bb{v}\bb{v}^{\top})\bb{x} = \pm \|\bb{x}\|_2 \hat{e_1} = \sigma\hat{e_1}$ }
	%\tcc{We choose the sign of sigma to avoid cancellation of $\bb{x}_1$ (As is the standard in LAPACK, LINPACK packages \cite{Higham2002}). This makes $\beta>0$.}
	$\bb{v}\gets$ {\tt copy}($\bb{x}$)\\
	$\sigma \gets -\rm{sign}(\bb{x}_1)\|\bb{x}\|_2$\\
	$\bb{v}_1 \gets \bb{x}_1-\sigma$ \\
	%\tcp*{This is referred to as $\bb{\tilde{v}}_1$ later on.} 
	$\beta \gets -\frac{\bb{v}_1}{\sigma}$\\
	%$\bb{v} \gets \frac{1}{\bb{v}_1}\bb{v}$\\
	\Return $\beta$, $\bb{v}/\bb{v}_1$, $\sigma$
	\caption{$\beta$, $\bb{v}$, $\sigma = {\tt hh\_vec}(\bb{x})$. Given a vector $\bb{x}\in\R^n$, return $\bb{v}$, $\beta$, $\sigma$ that satisfy $(I-\beta \bb{v}\bb{v}^{\top})\bb{x} =\sigma\hat{e_1}$ and $\bb{v}_1=1$ (see \cite{LAPACK, Higham2002}).}
	\label{algo:hh_v2}
\end{algorithm2e}

\begin{algorithm2e}
	\DontPrintSemicolon
	\KwIn{$A\in\R^{m \times n}$ where $m \geq n$.}
	
	\KwOut{$\bb{V}$,$\bm{\beta}$, $\bb{R}$}
	%	\tcc{$\bb{v}_i = V[i:m, i] \in \R^{m-(i-1)}$ and $\bb{B}_i = \bb{B}[i:m, i:d] \in \R^{(m-(i-1))\times(d-(i-1))}$.}
	$\bb{V}, \bm{\beta} \gets \bb{0}_{m\times n}, \bb{0}_m$ \\
	
	\For{$i=1 : n$}{
		$\bb{v}, \beta, \sigma \gets \mathrm{hh\_vec}(\bb{A}[i:\mathrm{end}, i])$ \tcc*{\Cref{algo:hh_v2}}
		$\bb{V}[i:\mathrm{end},i]$, $\bm{\beta}_i$,  $\bb{A}[i,i] \gets \bb{v}, \beta, \sigma$\\
		%\tcp*{Stores the Householder vectors and constants.}
		%\tcc{The next two steps update $\bb{A}$.}
		$\bb{A}[i+1:\mathrm{end}, i]\gets \mathrm{zeros}(m-i)$\\
		$\bb{A}[i:\mathrm{end}, i+1:\mathrm{end}]\gets \bb{A}[i:\mathrm{end}, i+1:\mathrm{end}] - \beta \bb{v} \bb{v}^{\top}\bb{A}[i:\mathrm{end}, i+1:\mathrm{end}]$
		
	}
	\Return $\bb{V}$, $\bm{\beta}$, $\bb{A}[1:n, 1:n]$
	\caption{$\bb{V}$, $\bm{\beta}$, $\bb{R}$ = ${\tt HQR2}(A)$. A Level-2 BLAS implementation of the Householder QR algorithm. Given a matrix $\bb{A}\in\R^{m\times n}$ where $m\geq n$, return matrix $\bb{V}\in\R^{m\times n}$, vector $\bm{\beta}\in\R^{n}$, and upper triangular matrix $\bb{R}$. An orthogonal matrix $\bb{Q}$ can be generated from $\bb{V}$ and $\bm{\beta}$, and $\bb{QR}=\bb{A}$.}
	\label{algo:hhQR}
\end{algorithm2e}
\subsubsection{HQR: Rounding Error Analysis}
Now we present an error analysis for \cref{algo:hhQR} by keeping track of the different operations of \cref{algo:hh_v2} and \cref{algo:hhQR}.
\paragraph{Calculating the $i^{th}$ Householder vector and constant} 
In \cref{algo:hhQR}, the $i^{th}$ Householder vector shares all but the first component with the target column, $\bb{A}[i:m,i]$. 
We first calculate $\sigma$ as is implemented in line 2 of \cref{algo:hh_v2}.
\begin{equation}
\label{eqn:sigma}
\fl(\sigma) = \hat{\sigma} = \rm{fl}(-\rm{sign}(\bb{A}_{i,i})\|\bb{A}[i:m,i]\|_2) = \sigma + \Delta \sigma,\quad |\Delta\sigma| \leq \gamma_{m-i+1}|\sigma|.
\end{equation}
Note that the backward error incurred here is simply that an inner product of a vector in $\R^{m-i+1}$ with itself. 
Let $\bb{\tilde{v}}_1\equiv \bb{A}_{i,i}-\sigma$, the penultimate value $\bb{v}_1$. 
The subtraction adds a single additional rounding error via
\begin{equation*}
	\fl(\bb{\tilde{v}}_1) =\bb{\tilde{v}}_1 + \Delta \bb{\tilde{v}}_1 = (1+\dd) (\bb{A}_{i,i}-\sigma-\Delta\sigma)= (1+\tilde{\tth}_{m-i+2})(\bb{A}_{i,i}-\sigma)
\end{equation*}
where the last equality is granted because the sign of $\sigma$ is chosen to prevent cancellation.  
%TODO: comment reviewer
For the sake of simplicity, we write $|\Delta \bb{\tilde{v}}_1 |\leq \tilde{\gamma}_{m-i+1}|\bb{\tilde{v}}_1|$ even though a tighter relative upper bound is $\tth_{m-i+2}$
We sweep that minor difference (in comparison to $\cO(m-i)$) under the our use of the $\tilde{\gamma}$ notation defined in \cref{lem:gamma}.
Since \cref{algo:hh_v2} normalizes the Householder vector so that its first component is $1$, the remaining components of $\bb{v}$ are divided by $\fl(\tilde{\bb{v}}_1)$ incurring another single rounding error.
As a result, the rounding errors in $\bb{v}$ are
\begin{equation}
	\fl(\bb{v}_j)	= \bb{v}_j + \Delta \bb{v}_j\text{ where }|\Delta \bb{v}_j|\leq 
	\begin{cases}
	0,& j=1\\
	\tilde{\gamma}_{m-i+1}|\bb{v}_j|,&j=2:m-i+1.
	\end{cases}  \label{eqn:vbound}
\end{equation}

Next, we consider the Householder constant, $\beta$, as is computed in line 4 of \cref{algo:hh_v2}.
\begin{align}
\hat{\beta} = \fl\left(-\frac{\tilde{\bb{v}_1}}{\hat{\sigma}}\right) &=-(1+\dd)\frac{\bb{\tilde{v}}_1+\bb{\Delta \tilde{v}}_1}{\sigma + \Delta\sigma} \label{eqn:beta}\\
&= \frac{(1+\dd)(1+\tth_{m-i+1})}{(1+\tth_{m-i+2})}\beta = (1+\tth_{3(m-i+2)})\beta \label{eqn:beta2}\\
& = \beta + \Delta \beta,\text{ where } |\Delta\beta| \leq \tilde{\gamma}_{m-i+1} \beta\label{eqn:beta3}.
\end{align}
We have shown \cref{eqn:beta} to keep our analysis simple in \cref{sec:mpanalysis} and \cref{eqn:beta2,eqn:beta3} show that the error incurred from calculating of $\|\bb{A}[i:m,i]\|_2$ accounts for the vast majority of the rounding error so far.
\paragraph{Applying a Single Householder Transformation}
Now we consider lines 4-6 of \cref{algo:hhQR}. 
Since the entries in $\bb{A}[i+1:m,i]$ are simply zeroed out and $\bb{A}_{i,i}$ is replaced by $\sigma$, we only need to calculate the errors for applying a Householder transformation with the computed Householder vector and constant. 
This is the most crucial building block of the rounding error analysis for any variant of HQR because the $\bb{Q}$ factor is formed by applying the Householder transformations to the identity and both of the blocked versions in \cref{sec:BQR} and \cref{sec:TSQR} require efficient implementations of this step. 
In this section, we only consider a level-2 BLAS implementation of applying the Householder transformation, but in \cref{sec:BQR} we introduce a level-3 BLAS implementation.\par

A Householder transformation is applied through a series of inner and outer products, since Householder matrices are rank-1 updates of the identity. 
That is, computing  $\bb{P}_{\bb{v}}\bb{x}$ for any $\bb{x}\in\R^{m}$ is as simple as computing 
\begin{equation}
	\bb{y}:=\bb{x} - (\beta \bb{v}^{\top}\bb{x})\bb{v}.\label{eqn:effH}
\end{equation}
Let us assume that $\bb{x}$ is an exact vector and there were errors incurred in forming $\bb{v}$ and $\beta$. 
The errors incurred from computing $\bb{v}$ and $\beta$ need to be included in addition to the new rounding errors accumulating from the action of applying $\bb{P}_{\bb{v}}$ to a column.
In practice, $\bb{x}$ would be a column in $\bb{A}^{(i-1)}[i+1:m, i+1:n]$, where the superscript $(i-1)$ indicates that this submatrix of $\bb{A}$ has already been transformed by $i-1$ Householder transformations that zeroed out components below $\bb{A}_{j,j}$ for $j = 1:i-1$.
We show the error for forming $\fl\left(\bb{\hat{v}}^{\top}\bb{x}\right)$ where we continue to let $\bb{v},\bb{x}\in\R^{m-i+1}$ as would be in the $i^{th}$ iteration of the for-loop in \cref{algo:hhQR}:
\begin{equation*}
\fl\left(\bb{\hat{v}}^{\top}\bb{x}\right) = (1+\tth_{m-i+1})(\bb{v}+\bb{\Delta v})^{\top}\bb{x}.
\end{equation*}
Set $\bb{w}:=\beta\bb{v}^{\top}\bb{x}\bb{v}$.
Then,
\begin{equation*}
\bb{\hat{w}} =(1+\tth_{m-i+1})(1+\dd)(1+\tilde{\dd})(\beta+\Delta\beta)(\bb{v}+\bb{\Delta v})^{\top}\bb{x}(\bb{v}+\bb{\Delta v}),
\end{equation*}
where $\tth_{m-i+1}$ is from computing the inner product $\hat{\bb{v}}^{\top}\bb{x}$, and $\dd$ and $\tilde{\dd}$ are from multiplying $\beta$, $\fl(\hat{\bb{v}}^{\top}\bb{x})$, and $\bb{\hat{v}}$ together. 
Finally, we can add in the vector subtraction operation and complete the rounding error analysis of applying a Householder transformation to any vector:
\begin{equation}
\fl(\hat{\bb{P_v}}\bb{x}) = \fl(\bb{x}-\bb{\hat{w}}) = (1+\dd)(\bb{x}-\bb{w}-\bb{\Delta w}) = \bb{y} + \bb{\Delta y},\label{eqn:applyP}
\end{equation}
where $|\bb{\Delta y}| \leq u|\bb{x}| + \tilde{\gamma}_{m-i+1} |\beta||\bb{v}||\bb{v}|^{\top}|\bb{x}|$.
Using $\sqrt{2/\beta} = \|\bb{v}\|_2$, we can conclude 
\begin{equation}
\|\bb{\Delta y}\|_2 \leq \tilde{\gamma}_{m-i+1}\|\bb{x}\|_2. \label{eqn:19.2c}
\end{equation} 
Next, we convert this to a backward error for $\bb{P_v}$.
Since $\bb{\Delta P_v}$ is exactly $\frac{1}{\bb{x}^{\top}\bb{x}}\bb{\Delta y}\bb{x}^{\top}$, we can compute its Frobenius norm by using $\bb{\Delta P}_{ij} = \frac{1}{\|\bb{x}\|_2^2}\bb{\Delta y}_i\bb{x}_j$,
	\begin{equation}
	\|\bb{\Delta P}\|_F
	= \left(\sum_{i=1}^m\sum_{j=1}^m\left(\frac{1}{\|\bb{x}\|_2^2}\bb{\Delta y}_i\bb{x}_j\right)^2\right)^{1/2}
	=  \frac{\|\bb{\Delta y}\|_2}{\|\bb{x}\|_2} \leq \tilde{\gamma}_{m-i+1}\label{eqn:outer},
	\end{equation}
where the last inequality is a direct application of \cref{eqn:19.2c}.
We summarize these results in \cref{lem:19.2}.
\begin{lemma}
	\label{lem:19.2}
	Let $\bb{x}\in\R^m$ and consider the computation of $\hat{\bb{y}}=\fl(\bb{P_vx})$ via 
	\begin{equation*}
		\bb{y}+\bb{\Delta y} = (\bb{P_v} + \bb{\Delta P_v})\bb{x} =\fl(\bb{P_v}\bb{x}) = \fl(\bb{x}-\hat{\beta}\hat{\bb{v}}\hat{\bb{v}}^{\top}\bb{x})
	\end{equation*}
	and rounding errors incurred in forming $\hat{\bb{v}}$ and $\hat{\beta}$ are expressed componentwise via $\hat{\bb{v}} = \bb{v}+\Delta \bb{v}$ and $\hat{\beta} = \beta + \Delta \beta$ where the relative componentwise errors for both are bounded by quantity $\gamma_y$:  
	\begin{equation}
		|\Delta \bb{v}| \leq \gamma_y|\bb{v}|,\qquad |\Delta \beta| \leq\gamma_y |\beta|.
	\end{equation}
	Then, the normwise forward and backward errors are $\|\Delta \bb{y}\|_2 \leq \gamma_y\|\bb{y}\|_2$ and $ \|\bb{\Delta P_v}\|_F \leq \gamma_y.$
\end{lemma}
Note that in a uniform precision setting this bound is represented as $\gamma_y = \tilde{\gamma}_{m}$. 
\paragraph{Applying many successive Householder transformations}
Consider applying a sequence of transformations in the set $\{\bb{P_i}\}_{i=1}^r\subset\R^{m\times m}$ to $\bb{x}\in\R^m$, where $\bb{P_i}$'s are all Householder transformations computed with $\tilde{\bb{v_i}}$'s and $\hat{\beta_i}$'s.
This is directly applicable to HQR as $\bb{Q}=\bb{P_1}\cdots\bb{P_n}\bb{I}$ and $\bb{R} = \bb{Q}^{\top}\bb{A} = \bb{P_n}\cdots\bb{P_1}\bb{A}$.
\Cref{lem:3.7} is very useful for any sequence of transformations, where each transformation has a known bound.
We will invoke this lemma to prove \cref{lem:19.3}, and use it in future sections for other sequential transformations.
\begin{lemma}\label{lem:3.7}
	If $\bb{X_j} + \bb{\Delta X_j} \in\R^{m\times m}$ satisfies $\|\bb{\Delta X_j}\|_F\leq \dd_j \|\bb{X_j}\|_2$ for all $j$, then $$\left|\left|\prod_{j=1}^n (\bb{X_j} + \bb{\Delta X_j})- \prod_{j=1}^n\bb{X_j} \right|\right|_F\leq\left(-1+\prod_{j=1}^n(1+\dd_j)\right)\prod_{j=1}^n\|\bb{X_j}\|_2.$$
\end{lemma}

%Let us define $$\bb{Q}+\bb{\Delta Q'}\equiv \prod_{i=1}^{r}\left(\bb{P}_i +\bb{\Delta P}_i\right)$$ in the context of applying this matrix to a vector, $\bb{x}\in\R^m$ , where $\bb{\Delta Q'}^{\top}$ represents the backward error of forming $\bb{R}$, instead of the forward error of the $\bb{Q}$ factor. 
%The forward error for $\bb{Q}$ is denoted as $\bb{\Delta Q} \equiv \fl(\bb{Q})-\bb{Q}$ where $\fl(\bb{Q})$ is formed via HQR.
%That is, if $\bb{y} = \bb{Q}^{\top}\bb{x}$, then $\fl(\bb{y}) = \bb{y}+ \bb{\Delta y} = (\bb{Q}+\bb{\Delta Q'})^{\top}\bb{x}$.
%Even though an efficient implementation would use that $\bb{P_i}$'s are applied to successively shorter vectors ($\bb{P_i}$ is left multiplied to $\bb{A}[i:m,i+1:n]$, which is equivalent to $n-i$ vectors of length $m-i+1$), we assume $\{\bb{P_i}\}_{i=1}^r\subset\R^{m\times m}$ to allow for a simpler analysis while forming a looser bound. 
%We will now use Lemma 3.7 from \cite{Higham2002} to bound $\bb{\Delta Q}'$ with the Frobenius norm.
%\begin{align}
%\|\bb{\Delta Q}'^{\top}\|_F &= \left|\left| \prod_{r=1}^{1}\left(\bb{P}_i +\bb{\Delta P}_i\right) - \prod_{i=r}^{1}\bb{P}_i \right|\right|_F,\label{eqn:rPs}\\
%&\leq \left(\prod_{i=1}^r(1+\tilde{\gamma}_m)-1\right)\prod_{i=r}^1\|\bb{P}_i\|_2 = (1+\tilde{\gamma}_m)^r-1. \label{eqn:rPs-uniform}
%\end{align}
%While we omit the details here, we can show that $(1+\tilde{\gamma}_m)^r-1 \leq r\tilde{\gamma}_m$ using the argument for \cref{lem:gamma} if $r\tilde{\gamma}_m \leq 1/2$ .
%\Cref{lem:19.3}
\begin{lemma}\label{lem:19.3}
	Consider applying a sequence of transformations $\bb{Q}=\bb{P_r}\cdots\bb{P_2P_1}$ onto vector $\bb{x}\in\R^m$ to form $\hat{\bb{y}} =\fl(\hat{\bb{P_r}}\cdots\hat{\bb{P_2}}\hat{\bb{P_1}}\bb{x}),$
	%$\bb{x}_{k+1} = \bb{P}_k\bb{x}_k$ for $k=1,\cdots,r$, where $\bb{x}_1 = \bb{x}\in\R^{m}$
	where $\hat{\bb{P_k}}$'s are Householder transformations constructed from $\hat{\beta}_k$ and $\hat{\bb{v_k}}$.
	These Householder vectors and constants are computed via \cref{algo:hh_v2} and the rounding errors are bounded by \cref{eqn:beta3,eqn:vbound}.
	If each transformation is computed via \cref{eqn:effH}, then
	\begin{align}
	\hat{\bb{y}} &= \bb{Q} (\bb{x} +\bb{\Delta x}) = (\bb{Q} + \bb{\Delta Q})\bb{x},\\
	\|\bb{\Delta y}\|_2 &\leq r\tilde{\gamma}_m\|\bb{x}\|_2,\;\; \|\bb{\Delta Q}\|_F\leq r\tilde{\gamma}_m .\label{eqn:19.3}
	\end{align}
\end{lemma}
\begin{proof}
	Applying \cref{lem:3.7} directly to $\bb{Q}$ yields
	\begin{align*}
		\|\hat{\bb{Q}}-\bb{Q}\|_F &= \left|\left|\prod_{j=1}^r (\bb{P_j} + \bb{\Delta P_j})- \prod_{j=1}^r\bb{P_j} \right|\right|_F\leq(1+\tilde{\gamma}_m)^r-1\prod_{j=1}^n\|\bb{P_j}\|_2 \leq (1+\tilde{\gamma}_m)^r-1
	\end{align*}
	since $\bb{P_j}$'s are orthogonal and have 2-norm, 1.
	While we omit the details here, we can show that $(1+\tilde{\gamma}_m)^r-1 \leq r\tilde{\gamma}_m$ using the argument for \cref{lem:gamma} if $r\tilde{\gamma}_m \leq 1/2$.
\end{proof}
In this current uniform precision error analysis, the important quantity $\tilde{\gamma}_{m}$ is derived from the backward error of applying one Householder transformation.
To easily generalize this section for mixed precision analysis, we benefit from alternatively denoting this quantity as $\tilde{\gamma}_{\bb{P}}$ with the understanding that $\tilde{\gamma}_{\bb{P}}$ will be some combination of $\tilde{\gamma}$'s of differing precisions. 
The bound in \Cref{eqn:19.3} would then be  replaced by $r\tilde{\gamma}_{\bb{P}}$.
%\begin{equation}
%	(1+\tilde{\gamma}_{\bb{P}})^r-1 \leq r\tilde{\gamma}_{\bb{P}}. \label{eqn:algebra3}
%\end{equation}
Applying \cref{lem:19.2} directly to columns of $\bb{A}$ and $\bb{I}$ allows us to formulate 2-norm forward bounds for columns of $\bb{R}$ and $\bb{Q}$.
We show how to convert these columnwise bounds into matrix norms for the $\bb{R}$ factor.
%Next, we apply \cref{eqn:19.3} to the $i^{th}$ columns of $\bb{Q},\bb{R}$ and set $r=n$ for a full rank matrix, $\bb{A}$.
%Then,
%\begin{align*}
%	\|\bb{\Delta R}[:,i]\|_2 &= \|\Delta \bb{Q'}^{\top}\bb{A}[:,i]\|_2 \leq \|\Delta \bb{Q'}\|_2 \|\bb{A}[:,i]\|_2  \leq n\tilde{\gamma}_{m} \|\bb{A}[:,i]\|_2, \\ 
%	\|\bb{\Delta Q}[:,i]\|_2 &= \|\Delta \bb{Q'}\bb{I}[:,i]\|_2 \leq \|\Delta \bb{Q'}\|_2\leq n\tilde{\gamma}_{m}.
%\end{align*}
%These columnwise bounds can now be transformed into matrix norms as follows: 
\begin{align*}
	\|\bb{\Delta R} \|_F &= \left(\sum_{i=1}^n \|\bb{\Delta R}[:,i]\|_2^2\right)^{1/2} \leq \left(\sum_{i=1}^n n^2\tilde{\gamma}_{m}^2 \|\bb{A}[:,i]\|_2^2\right)^{1/2} = n\tilde{\gamma}_{m} \|\bb{A}\|_F, \\
	%\|\bb{\Delta Q} \|_F &= \left(\sum_{i=1}^n \|\bb{\Delta Q}[:,i]\|_2^2\right)^{1/2} \leq \left(\sum_{i=1}^n n^2\tilde{\gamma}_{m}^2 \right)^{1/2} = n^{3/2}\tilde{\gamma}_{m}.
\end{align*}
We gather these results into \cref{thm:feHQR}.
\begin{theorem}
	\label{thm:feHQR}
	Let $\bb{A}\in\R^{m\times n}$ with $m\geq n$ have full rank, $n$. 
	Let $\hat{\bb{Q}}\in\R^{m\times n}$ and $\hat{\bb{R}}\in\R^{n\times n}$ be the thin QR factors of $\bb{A}$ obtained via \cref{algo:hhQR}.
	Then,
	\begin{align*}
	\hat{\bb{R}} &= \bb{R} + \bb{\Delta R} = \fl(\hat{\bb{P}}_n\cdots\hat{\bb{P}}_1 \bb{A}),\;\; \|\bb{\Delta R}[:,j]\|_2\leq n\tilde{\gamma}_{m} \|\bb{A}[:,j]\|_2,\;\; \|\bb{\Delta R}\|_F\leq n\tilde{\gamma}_{m} \|\bb{A}\|_F\\
	\hat{\bb{Q}} &= \bb{Q} + \bb{\Delta Q} = \fl(\hat{\bb{P}}_1\cdots\hat{\bb{P}}_n \bb{I}),\;\; \|\bb{\Delta Q}[:,j]\|_2\leq n\tilde{\gamma}_{m},\;\; \|\bb{\Delta Q}\|_F \leq n^{3/2} \tilde{\gamma}_{m}.
	\end{align*}
	Let $\bb{A}+\bb{\Delta A} = \hat{\bb{Q}}\hat{\bb{R}}$, where $\hat{\bb{Q}}$ and $\hat{\bb{R}}$ are obtained via Algorithm~\ref{algo:hhQR}.
	Then the backward e|rror is
	\begin{equation}
	\|\bb{\Delta A}\|_F \leq n^{3/2}\tilde{\gamma}_{m}\|\bb{A}\|_F.
	\end{equation}
\end{theorem}
For the remainder of this paper, we compare various QR factorizations with respect to $\|\hat{\bb{Q}}-\bb{Q}\|_F$, a comprehensive measure of orthogonality of the $\bb{Q}$ factor.
The content of this section generalizes the standard rounding error analysis in \cite{Higham2002} by employing quantities denoted via $\Delta \beta$, $\bb{\Delta v}$, $\tilde{\gamma}_y$, and $\tilde{\gamma}_{\bb{P}}$. 
These quantities account for various forward and backward errors formed in computing essential components of HQR, namely the Householder constant and vector, as well as normwise errors of the action of applying Householder transformations.
In the next sections, we present blocked variants of HQR that use \cref{algo:hhQR}.