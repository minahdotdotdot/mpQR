We introduce the Householder QR factorization algorithm (HQR) in \cref{sec:HQR} and two block variants that use HQR within the block in \cref{sec:BQR,sec:TSQR}. 
The blocked HQR (BQR) in \cref{sec:BQR} partitions the columns of the target matrix and utilizes mainly level-3 BLAS operations and is a well-known algorithm that uses the WY representation of \cite{Bischof1987}.
%\cite{Schreiber1989}.
In contrast, the Tall-and-Skinny QR (TSQR) in \cref{sec:TSQR} partitions rows of the matrix and takes a communication-avoiding divide-and-conquer approach that can be easily parallelized (see \cite{Demmel2007}).
We also present the crucial results in standard rounding error analysis of these algorithms that excludes any mixed-precision assumptions.
These building steps of round-off error analysis will be easily tweaked for various mixed-precision assumptions in \cref{sec:mpanalysis}.
\subsection{Householder QR (HQR)}\label{sec:HQR}
The HQR algorithm uses Householder transformations to zero out elements below the diagonal of a matrix (see \cite{Householder1958}). 
We present this as zeroing out all but the first element of some vector, $\bb{x}\in\R^m$.
\begin{lemma}
	Given vector $\bb{x}\in\R^{m}$, there exist Householder vector, $\bb{v}$, and Householder transformation matrix, $\bb{P}_{\bb{v}}$, such that $\bb{P}_{\bb{v}}$ zeros out $\bb{x}$ below the first element. 
	\begin{equation}
	\begin{alignedat}{3} 
	\sigma =& -\rm{sign}(\bb{x}_1)\|\bb{x}\|_2, &&\quad  \bb{v} = \bb{x} -\sigma \hat{e_1},\\
	\beta = & \frac{2}{\bb{v}^{\top}\bb{v}}=-\frac{1}{\sigma\bb{v}_1}, && \quad \bb{P}_{\bb{v}}=  \bb{I}_{m} - \beta \bb{v}\bb{v}^{\top}.
	\end{alignedat}
	\label{eqn:HH} 
	\end{equation}
	The transformed vector, $\bb{P_vx}$, has the same 2-norm as $\bb{x}$ since Householder transformations are orthogonal: $\bb{P}_{\bb{v}}\bb{x} = \sigma\hat{\bb{e}_1}$.
	In addition, $\bb{P}_{\bb{v}}$ is symmetric and orthogonal, $\bb{P}_{\bb{v}}=\bb{P}_{\bb{v}}^{\top}=\bb{P}_{\bb{v}}^{-1}$.
	\label{lem:hhvec}
\end{lemma}
\subsubsection{HQR: Algorithm}
Given $\bb{A}\in\R^{m\times n}$ and Lemma \ref{lem:hhvec}, HQR is done by repeating the following processes until only an upper triangle matrix remains.
For $i = 1, 2, \cdots, n,$
\begin{enumerate}[Step 1)]
	\item Compute $\bb{v}$ and $\beta$ that zeros out the $i^{th}$ column of $\bb{A}$ beneath $a_{ii}$ (see \cref{algo:hh_v2}), and
	\item Apply $\bb{P}_{\bb{v}}$ to the bottom right partition, $\bb{A}[i:m, i:n]$ (lines 4-6 of \cref{algo:hhQR}).
\end{enumerate}

Consider the following $4$-by-$3$ matrix example adapted from \cite{Higham2002}. 
Let $\bb{P_i}$ represent the $i^{th}$ Householder transformation of this algorithm. 
\[\bb{A} = \left[ \begin{array}{ccc}
\times & \times & \times \\
\times & \times & \times \\
\times & \times & \times \\
\times & \times & \times
\end{array}
\right]\xrightarrow{\text{apply $\bb{P_1}$ to $\bb{A}$}}\left[ \begin{array}{c|cc}
\times & \times & \times \\ \hline
0 & \times & \times \\
0 & \times & \times \\
0 & \times & \times
\end{array}
\right]
\xrightarrow{\text{apply $\bb{P_2}$ to $\bb{P_1}\bb{A}$}}\]
\[ \left[
\begin{array}{cc|c}
\times & \times & \times \\
0 & \times & \times \\ \hline
0 & 0 & \times \\
0 & 0 & \times 
\end{array} \right]
\xrightarrow{\text{apply $\bb{P_3}$ to $\bb{P_2}\bb{P_1}\bb{A}$}} \left[ \begin{array}{ccc}
\times & \times & \times \\
0 & \times & \times \\
0 & 0 & \times \\
0 & 0 & 0 
\end{array}\right] = \bb{P_3}\bb{P_2}\bb{P_1}\bb{A}=:\bb{R} \] 
Then, the $\bb{Q}$ factor for a full QR factorization is $\bb{Q}:=\bb{P_1}\bb{P_2}\bb{P_3}$ since $\bb{P_i}$'s are symmetric, and the thin factors for a general matrix $\bb{A}\in\R^{m\times n}$ are
\begin{equation}
\bb{Q}_{\text{thin}} = \bb{P_1} \cdots \bb{P_n}\bb{I}_{m\times n}\quad \text{and} \quad \bb{R}_{\text{thin}} = \bb{I}_{m\times n}^{\top}\bb{P_n}\cdots \bb{P_1}\bb{A}.
\end{equation}

\begin{algorithm2e}[H]
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	\KwIn{$\bb{x}\in\R^m$}
	\KwOut{$\bb{v}\in\R^m$, and $\sigma, \beta\in\R$ such that $(I-\beta \bb{v}\bb{v}^{\top})\bb{x} = \pm \|\bb{x}\|_2 \hat{e_1} = \sigma\hat{e_1}$ }
	%\tcc{We choose the sign of sigma to avoid cancellation of $\bb{x}_1$ (As is the standard in LAPACK, LINPACK packages \cite{Higham2002}). This makes $\beta>0$.}
	$\bb{v}\gets$ {\tt copy}($\bb{x}$)\\
	$\sigma \gets -\rm{sign}(\bb{x}_1)\|\bb{x}\|_2$\\
	$\bb{v}_1 \gets \bb{x}_1-\sigma$ \\
	%\tcp*{This is referred to as $\bb{\tilde{v}}_1$ later on.} 
	$\beta \gets -\frac{\bb{v}_1}{\sigma}$\\
	%$\bb{v} \gets \frac{1}{\bb{v}_1}\bb{v}$\\
	\Return $\beta$, $\bb{v}/\bb{v}_1$, $\sigma$
	\caption{$\beta$, $\bb{v}$, $\sigma = {\tt hh\_vec}(\bb{x})$. Given a vector $\bb{x}\in\R^n$, return $\bb{v}$, $\beta$, $\sigma$ that satisfy $(I-\beta \bb{v}\bb{v}^{\top})\bb{x} =\sigma\hat{e_1}$ and $\bb{v}_1=1$ (see \cite{LAPACK, Higham2002}).}
	\label{algo:hh_v2}
\end{algorithm2e}

\begin{algorithm2e}
	\DontPrintSemicolon
	\KwIn{$A\in\R^{m \times n}$ where $m \geq n$.}
	
	\KwOut{$\bb{V}$,$\bm{\beta}$, $\bb{R}$}
	%	\tcc{$\bb{v}_i = V[i:m, i] \in \R^{m-(i-1)}$ and $\bb{B}_i = \bb{B}[i:m, i:d] \in \R^{(m-(i-1))\times(d-(i-1))}$.}
	$\bb{V}, \bm{\beta} \gets \bb{0}_{m\times n}, \bb{0}_m$ \\
	
	\For{$i=1 : n$}{
		$\bb{v}, \beta, \sigma \gets \mathrm{hh\_vec}(\bb{A}[i:\mathrm{end}, i])$ \tcc*{\Cref{algo:hh_v2}}
		$\bb{V}[i:\mathrm{end},i]$, $\bm{\beta}_i$,  $\bb{A}[i,i] \gets \bb{v}, \beta, \sigma$\\
		%\tcp*{Stores the Householder vectors and constants.}
		%\tcc{The next two steps update $\bb{A}$.}
		$\bb{A}[i+1:\mathrm{end}, i]\gets \mathrm{zeros}(m-i)$\\
		$\bb{A}[i:\mathrm{end}, i+1:\mathrm{end}]\gets \bb{A}[i:\mathrm{end}, i+1:\mathrm{end}] - \beta \bb{v} \bb{v}^{\top}\bb{A}[i:\mathrm{end}, i+1:\mathrm{end}]$
		
	}
	\Return $\bb{V}$, $\bm{\beta}$, $\bb{A}[1:n, 1:n]$
	\caption{$\bb{V}$, $\bm{\beta}$, $\bb{R}$ = ${\tt HQR2}(A)$. A Level-2 BLAS implementation of the Householder QR algorithm. Given a matrix $\bb{A}\in\R^{m\times n}$ where $m\geq n$, return matrix $\bb{V}\in\R^{m\times n}$, vector $\bm{\beta}\in\R^{n}$, and upper triangular matrix $\bb{R}$. An orthogonal matrix $\bb{Q}$ can be generated from $\bb{V}$ and $\bm{\beta}$, and $\bb{QR}=\bb{A}$.}
	\label{algo:hhQR}
\end{algorithm2e}
\subsubsection{HQR: Rounding Error Analysis}
Now we present an error analysis for \cref{algo:hhQR} by keeping track of the different operations of \cref{algo:hh_v2} and \cref{algo:hhQR}.
\paragraph{Calculating the $i^{th}$ Householder vector and constant} 
In \cref{algo:hhQR}, the $i^{th}$ Householder vector shares all but the first component with the target column, $\bb{A}[i:m,i]$. 
We first calculate $\sigma$ as is implemented in line 2 of \cref{algo:hh_v2}.
\begin{equation}
\label{eqn:sigma}
\fl(\sigma) = \hat{\sigma} = \rm{fl}(-\rm{sign}(\bb{A}_{i,i})\|\bb{A}[i:m,i]\|_2) = \sigma + \Delta \sigma,\quad |\Delta\sigma| \leq \gamma_{m-i+1}|\sigma|.
\end{equation}
Note that the backward error incurred here is simply that an inner product of a vector in $\R^{m-i+1}$ with itself. 
Let $\bb{\tilde{v}}_1\equiv \bb{A}_{i,i}-\sigma$, the penultimate value $\bb{v}_1$. 
The subtraction adds a single additional rounding error via
\begin{equation*}
	\fl(\bb{\tilde{v}}_1) =\bb{\tilde{v}}_1 + \Delta \bb{\tilde{v}}_1 = (1+\dd) (\bb{A}_{i,i}-\sigma-\Delta\sigma)= (1+\tilde{\tth}_{m-i+2})(\bb{A}_{i,i}-\sigma)
\end{equation*}
where the last equality is granted because the sign of $\sigma$ is chosen to prevent cancellation.  
%TODO: comment reviewer
For the sake of simplicity, we write $|\Delta \bb{\tilde{v}}_1 |\leq \tilde{\gamma}_{m-i+1}|\bb{\tilde{v}}_1|$ even though a tighter relative upper bound is $\tth_{m-i+2}$
We sweep that minor difference (in comparison to $\cO(m-i)$) under the our use of the $\tilde{\gamma}$ notation defined in \cref{lem:gamma}.
Since \cref{algo:hh_v2} normalizes the Householder vector so that its first component is $1$, the remaining components of $\bb{v}$ are divided by $\fl(\tilde{\bb{v}}_1)$ incurring another single rounding error.
As a result, the rounding errors in $\bb{v}$ are
\begin{equation}
	\fl(\bb{v}_j)	= \bb{v}_j + \Delta \bb{v}_j\text{ where }|\Delta \bb{v}_j|\leq 
	\begin{cases}
	0,& j=1\\
	\tilde{\gamma}_{m-i+1}|\bb{v}_j|,&j=2:m-i+1.
	\end{cases}  \label{eqn:vbound}
\end{equation}

Next, we consider the Householder constant, $\beta$, as is computed in line 4 of \cref{algo:hh_v2}.
\begin{align}
\hat{\beta} = \fl\left(-\frac{\tilde{\bb{v}_1}}{\hat{\sigma}}\right) &=-(1+\dd)\frac{\bb{\tilde{v}}_1+\bb{\Delta \tilde{v}}_1}{\sigma + \Delta\sigma} \label{eqn:beta}\\
&= \frac{(1+\dd)(1+\tth_{m-i+1})}{(1+\tth_{m-i+2})}\beta = (1+\tth_{3(m-i+2)})\beta \label{eqn:beta2}\\
& = \beta + \Delta \beta,\text{ where } |\Delta\beta| \leq \tilde{\gamma}_{m-i+1} \beta\label{eqn:beta3}.
\end{align}
We have shown \cref{eqn:beta} to keep our analysis simple in \cref{sec:mpanalysis} and \cref{eqn:beta2,eqn:beta3} show that the error incurred from calculating of $\|\bb{A}[i:m,i]\|_2$ accounts for the vast majority of the rounding error so far.
\paragraph{Applying a Single Householder Transformation}
Now we consider lines 4-6 of \cref{algo:hhQR}. 
Since the entries in $\bb{A}[i+1:m,i]$ are simply zeroed out and $\bb{A}_{i,i}$ is replaced by $\sigma$, we only need to calculate the errors for applying a Householder transformation with the computed Householder vector and constant. 
This is the most crucial building block of the rounding error analysis for any variant of HQR because the $\bb{Q}$ factor is formed by applying the Householder transformations to the identity and both of the blocked versions in \cref{sec:BQR} and \cref{sec:TSQR} require efficient implementations of this step. 
In this section, we only consider a level-2 BLAS implementation of applying the Householder transformation, but in \cref{sec:BQR} we introduce a level-3 BLAS implementation.\par

A Householder transformation is applied through a series of inner and outer products, since Householder matrices are rank-1 updates of the identity. 
That is, computing  $\bb{P}_{\bb{v}}\bb{x}$ for any $\bb{x}\in\R^{m}$ is as simple as computing $\bb{y}:=\bb{x} - (\beta \bb{v}^{\top}\bb{x})\bb{v}$.
Let us assume that $\bb{x}$ is an exact vector and there were errors incurred in forming $\bb{v}$ and $\beta$. 
The errors incurred from computing $\bb{v}$ and $\beta$ need to be included in addition to the new rounding errors accumulating from the action of applying $\bb{P}_{\bb{v}}$ to a column.
In practice, $\bb{x}$ would be a column in $\bb{A}^{(i-1)}[i+1:m, i+1:n]$, where the superscript $(i-1)$ indicates that this submatrix of $\bb{A}$ has already been transformed by $i-1$ Householder transformations that zeroed out components below $\bb{A}_{j,j}$ for $j = 1:i-1$.
We show the error for forming $\fl\left(\bb{\hat{v}}^{\top}\bb{x}\right)$ where we continue to let $\bb{v},\bb{x}\in\R^{m-i+1}$ as would be in the $i^{th}$ iteration of the for-loop in \cref{algo:hhQR}:
\begin{equation*}
\fl\left(\bb{\hat{v}}^{\top}\bb{x}\right) = (1+\tth_{m-i+1})(\bb{v}+\bb{\Delta v})^{\top}\bb{x}.
\end{equation*}
Set $\bb{w}:=\beta\bb{v}^{\top}\bb{x}\bb{v}$.
Then,
\begin{equation*}
\bb{\hat{w}} =(1+\tth_{m-i+1})(1+\dd)(1+\tilde{\dd})(\beta+\Delta\beta)(\bb{v}+\bb{\Delta v})^{\top}\bb{x}(\bb{v}+\bb{\Delta v}),
\end{equation*}
where $\tth_{m-i+1}$ is from computing the inner product $\hat{\bb{v}}^{\top}\bb{x}$, and $\dd$ and $\tilde{\dd}$ are from multiplying $\beta$, $\fl(\hat{\bb{v}}^{\top}\bb{x})$, and $\bb{\hat{v}}$ together. 
Finally, we can add in the vector subtraction operation and complete the rounding error analysis of applying a Householder transformation to any vector:
\begin{equation}
\fl(\bb{x}-\bb{\hat{w}}) = (1+\dd)(\bb{x}-\bb{w}-\bb{\Delta w}) = (1+\tilde{\tth}_{m-i+1})\bb{y}.\label{eqn:applyP}
\end{equation}
We can easily switch between forward and errors from \cref{eqn:applyP} via
\begin{equation*}
	\bb{y}+\bb{\Delta y} = (1+\tilde{\tth}_{m-i+1})\bb{y} = (1+\tilde{\tth}_{m-i+1})\bb{P_vx} =  (\bb{P_v} +\bb{\Delta P_v})\bb{x},
\end{equation*}
where $|\bb{\Delta y}|\leq \tilde{\gamma}_{m-i+1}|\bb{y}|$ and  $|\bb{\Delta P_v}|\leq \tilde{\gamma}_{m-i+1}|\bb{P_v}|$.\par

Even though we never explicitly form $\bb{P_v}$, forming the normwise error bound for this matrix makes the analysis for HQR simpler.
Therefore, we now transition from componentwise error to matrix norm errors: the 2-norm and the Frobenius norm. 

First, we transition from componentwise forward error to the 2-norm forward error via 
\begin{equation}
\label{eqn:19.2b}
\|\bb{\Delta y}\|_2 = \left(\sum_{i=1}^m \bb{\Delta y}_i^2\right)^{1/2} \leq \left((\tilde{\gamma}_{m-i+1})^2\sum_{i=1}^m |\bb{y}_i|^2\right)^{1/2} =  \tilde{\gamma}_{m-i+1}\|\bb{y}\|_2. 
\end{equation}
In exact arithmetic, we are guaranteed $\|\bb{y}\|_2 = \|\bb{P_v x}\|_2 \leq \|\bb{P}\|_2 \|\bb{x}\|_2 = \|\bb{x}\|_2$
since $\bb{P_v}$ is orthogonal and preserves norms.
Combining this with \cref{eqn:19.2b} we find 
\begin{equation}
\frac{\|\bb{\Delta y}\|_2}{\|\bb{x}\|_2} \leq \tilde{\gamma}_{m-i+1}. \label{eqn:19.2c}
\end{equation}
Now we convert this to a normwise backward error.
Since $\bb{\Delta P}$ is exactly $\frac{1}{\bb{x}^{\top}\bb{x}}\bb{\Delta y}\bb{x}^{\top}$, we can compute its Frobenius norm by using $\bb{\Delta P}_{ij} = \frac{1}{\|\bb{x}\|_2^2}\bb{\Delta y}_i\bb{x}_j$,
	\begin{equation*}
	\|\bb{\Delta P}\|_F
	= \left(\sum_{i=1}^m\sum_{j=1}^m\left(\frac{1}{\|\bb{x}\|_2^2}\bb{\Delta y}_i\bb{x}_j\right)^2\right)^{1/2}
	=  \frac{\|\bb{\Delta y}\|_2}{\|\bb{x}\|_2} \leq \tilde{\gamma}_{m-i+1},
	\end{equation*}
where the last inequality is a direct application of \cref{eqn:19.2c}.
We summarize these results in \cref{lem:19.2}.
\begin{lemma}
	\label{lem:19.2}
	Let $\bb{x}\in\R^m$ and consider the computation of $\hat{\bb{y}}=\fl(\bb{P_vx})$ via 
	\begin{equation*}
		\bb{y}+\bb{\Delta y} = \fl(\bb{P_v}\bb{x}) = \fl(\bb{x}-\hat{\beta}\hat{\bb{v}}\hat{\bb{v}}^{\top}\bb{x})
	\end{equation*}
	and rounding errors incurred in forming $\hat{\bb{v}}$ and $\hat{\beta}$ are expressed componentwise via $\hat{\bb{v}} = \bb{v}+\Delta \bb{v}$ and $\hat{\beta} = \beta + \Delta \beta.$
	Let us write the componentwise forward error bound as $|\bb{\Delta y}|\leq \gamma_y|\bb{y}|$.
	Then, the normwise forward and backward errors are 
	\begin{equation*}
		\|\Delta \bb{y}\|_2 \leq \gamma_y\|\bb{y}\|_2,\;\; \|\bb{P_v}\|_F \leq \gamma_y.
	\end{equation*}
	Note that in a uniform precision setting this bound is represented as $\gamma_y = \tilde{\gamma}_{m}$, where the majority of the round-off errors are attributed to inner product computations for forming $\hat{\beta}$ and $\bb{v}$.
\end{lemma}
\paragraph{Applying many successive Householder transformations}
Consider applying a sequence of transformations in the set $\{\bb{P_i}\}_{i=1}^r\subset\R^{m\times m}$ to $\bb{x}\in\R^m$, where $\bb{P_i}$'s are all Householder transformations.
This is directly applicable to HQR as $\bb{Q}=\bb{P_1}\cdots\bb{P_n}\bb{I}$ and $\bb{R} = \bb{Q}^{\top}\bb{A} = \bb{P_n}\cdots\bb{P_1}\bb{A}$.
Let us define $$\bb{Q}+\bb{\Delta Q'}\equiv \prod_{i=1}^{r}\left(\bb{P}_i +\bb{\Delta P}_i\right)$$ in the context of applying this matrix to a vector, $\bb{x}\in\R^m$ , where $\bb{\Delta Q'}^{\top}$ represents the backward error of forming $\bb{R}$, instead of the forward error of the $\bb{Q}$ factor. 
The forward error for $\bb{Q}$ is denoted as $\bb{\Delta Q} \equiv \fl(\bb{Q})-\bb{Q}$ where $\fl(\bb{Q})$ is formed via HQR.
That is, if $\bb{y} = \bb{Q}^{\top}\bb{x}$, then $\fl(\bb{y}) = \bb{y}+ \bb{\Delta y} = (\bb{Q}+\bb{\Delta Q'})^{\top}\bb{x}$.
Even though an efficient implementation would use that $\bb{P_i}$'s are applied to successively shorter vectors ($\bb{P_i}$ is left multiplied to $\bb{A}[i:m,i+1:n]$, which is equivalent to $n-i$ vectors of length $m-i+1$), we assume $\{\bb{P_i}\}_{i=1}^r\subset\R^{m\times m}$ to allow for a simpler analysis while forming a looser bound. 
We will now use Lemma 3.7 from \cite{Higham2002} to bound $\bb{\Delta Q}'$ with the Frobenius norm.
\begin{align}
\|\bb{\Delta Q}'^{\top}\|_F &= \left|\left| \prod_{r=1}^{1}\left(\bb{P}_i +\bb{\Delta P}_i\right) - \prod_{i=r}^{1}\bb{P}_i \right|\right|_F,\label{eqn:rPs}\\
&\leq \left(\prod_{i=1}^r(1+\tilde{\gamma}_m)-1\right)\prod_{i=r}^1\|\bb{P}_i\|_2 = (1+\tilde{\gamma}_m)^r-1. \label{eqn:rPs-uniform}
\end{align}
The last equality results from the orthogonality of Householder matrices, and we further reduce the last term.
Generalizing the last rule in Lemma~\ref{lem:up} yields
\begin{equation*}
(1+\tilde{\gamma}_m)^r = (1+\tilde{\gamma}_m)^{r-2}(1+\tilde{\gamma}_m)(1+\tilde{\gamma}_m) \leq  (1+\tilde{\gamma}_m)^{r-2}(1+\tilde{\gamma}_{2m}) \leq \cdots \leq (1+\tilde{\gamma}_{rm}).
\end{equation*}
Now we will use the following equivalent algebraic inequalities to get the final result.
\begin{equation}
0<a<b<1 \Leftrightarrow 1-a > 1-b \Leftrightarrow \frac{1}{1-a} <\frac{1}{1-b} \Leftrightarrow \frac{a}{1-a} < \frac{b}{1-b}
\label{eqn:algebra}
\end{equation}
In addition, we assume $r\tilde{\gamma}_m< \frac{1}{2}$, such that 
\begin{align}
(1+\tilde{\gamma}_m)^r-1 &\leq \gamma_w^{(r\tilde{z})} = \frac{r\tilde{z}u_w}{1-r\tilde{z}u_w}\qquad\text{(by definition)}\\
&\leq \frac{r\tilde{\gamma}_m}{1-r\tilde{\gamma}_m},\text{ since } r\tilde{z}u_w < r\tilde{\gamma}_m\qquad\text{(by Equation \ref{eqn:algebra})}\\
&\leq 2 r \tilde{\gamma}_m\qquad\text{(since $r\tilde{\gamma}_m< \frac{1}{2}$ implies  $\frac{1}{1-r\tilde{\gamma}_m} < 2$)}\\
&= r\tilde{\gamma}_m, \label{eqn:algebra2}
\end{align}
Therefore, we have $(1+\tilde{\gamma}_m)^r-1 \leq r\tilde{\gamma}_m$ and
\begin{equation}
	 \|\Delta \bb{Q'}\|_2\leq \|\Delta \bb{Q'}\|_F = \|\Delta \bb{Q'}^{\top}\|_F\leq r \tilde{\gamma}_{m} \label{eqn:deltQ}
\end{equation}
In this current uniform precision error analysis, the important quantity $\tilde{\gamma}_{m}$ is derived from the backward error of applying one Householder transformation.
To easily generalize this section for mixed-precision analysis, we benefit from alternatively denoting this quantity as $\tilde{\gamma}_{\bb{P}}$ with the understanding that $\tilde{\gamma}_{\bb{P}}$ will be some combination of $\tilde{\gamma}$'s of differing precisions. 
\Cref{eqn:algebra2} would then be 
\begin{equation}
	(1+\tilde{\gamma}_{\bb{P}})^r-1 \leq r\tilde{\gamma}_{\bb{P}}. \label{eqn:algebra3}
\end{equation}

Next, we apply \cref{eqn:deltQ} to the $i^{th}$ columns of $\bb{Q},\bb{R}$ and set $r=n$ for a full rank matrix, $\bb{A}$.
Then,
\begin{align*}
	\|\bb{\Delta R}[:,i]\|_2 &= \|\Delta \bb{Q'}^{\top}\bb{A}[:,i]\|_2 \leq \|\Delta \bb{Q'}\|_2 \|\bb{A}[:,i]\|_2  \leq n\tilde{\gamma}_{m} \|\bb{A}[:,i]\|_2, \\ 
	\|\bb{\Delta Q}[:,i]\|_2 &= \|\Delta \bb{Q'}\bb{I}[:,i]\|_2 \leq \|\Delta \bb{Q'}\|_2\leq n\tilde{\gamma}_{m}.
\end{align*}

These columnwise bounds can now be transformed into matrix norms as follows: 
\begin{align*}
	\|\bb{\Delta R} \|_F &= \left(\sum_{i=1}^n \|\bb{\Delta R}[:,i]\|_2^2\right)^{1/2} \leq \left(\sum_{i=1}^n n^2\tilde{\gamma}_{m}^2 \|\bb{A}[:,i]\|_2^2\right)^{1/2} = n\tilde{\gamma}_{m} \|\bb{A}\|_F, \\
	\|\bb{\Delta Q} \|_F &= \left(\sum_{i=1}^n \|\bb{\Delta Q}[:,i]\|_2^2\right)^{1/2} \leq \left(\sum_{i=1}^n \tilde{\gamma}_{m}^2 \right)^{1/2} = n^{3/2}\tilde{\gamma}_{m}.
\end{align*}
We gather these results into \cref{thm:feHQR}.
\begin{theorem}
	\label{thm:feHQR}
	Let $\bb{A}\in\R^{m\times n}$ with $m\geq n$ have full rank, $n$. 
	Let $\hat{\bb{Q}}\in\R^{m\times n}$ and $\hat{\bb{R}}\in\R^{n\times n}$ be the thin QR factors of $\bb{A}$ obtained via \cref{algo:hhQR}, defined via 
	\begin{align*}
	\hat{\bb{R}} &= \bb{R} + \bb{\Delta R} = \fl(\hat{\bb{P}}_n\cdots\hat{\bb{P}}_1 \bb{A}),\;\; n\tilde{\gamma}_{m} \|\bb{A}\|_F\\
	\hat{\bb{Q}} &= \bb{Q} + \bb{\Delta Q} = \fl(\hat{\bb{P}}_1\cdots\hat{\bb{P}}_n \bb{I}),\;\; \|\bb{\Delta Q}\|_F \leq n^{3/2} \tilde{\gamma}_{m}.
	\end{align*}
	Let $\bb{A}+\bb{\Delta A} = \hat{\bb{Q}}\hat{\bb{R}}$, where $\hat{\bb{Q}}$ and $\hat{\bb{R}}$ are obtained via Algorithm~\ref{algo:hhQR}.
	Then the backward error is
	\begin{equation}
	\|\bb{\Delta A}\|_F \leq n^{3/2}\tilde{\gamma}_{m}\|\bb{A}\|_F.
	\end{equation}
\end{theorem}

The content of this section is largely derived directly from \cite{Higham2002}, but we kept the analysis general by employing quantities denoted via $\Delta \beta$, $\bb{\Delta v}$, $\tilde{\gamma}_y$, and $\tilde{\gamma}_{\bb{P}}$. 
These quantities account for various forward and backward errors formed in computing essential components of HQR, namely the Householder constant and vector, as well as normwise errors of the action of applying Householder transformations.
In the next sections, we present blocked variants of HQR that use \cref{algo:hhQR}.
\subsection{Block HQR with partitioned columns (BQR)}\label{sec:BQR}
We refer to the blocked variant of HQR where the columns are partitioned as BQR. 
Note that this algorithm relies on the WY representation described in \cite{Bischof1987} instead of the storage-efficient version of \cite{Schreiber1989}.
%, which is widely implemented.
\subsubsection{The WY Representation}
A convenient matrix representation that accumulates $r$ Householder reflectors is known as the WY representation (see \cite{Bischof1987,golub2013matrix}).
\Cref{lem:WY} shows how to update a rank-$j$ update of the identity, $\bb{Q}^{(j)}$, with a Householder transformation, $\bb{P}$, to produce a rank-$(j+1)$ update of the identity, $\bb{Q}^{(j+1)}$. 
With the correct initialization of $\bb{W}$ and $\bb{Y}$, we can build the WY representation of successive Householder transformations as shown in \Cref{algo:buildWY}. 
\begin{lemma}\label{lem:WY}
	Suppose $\bb{Q}^{(j)}=\bb{I}-\bb{W}^{(j)}\bb{Y}^{(j)\top}\in\R^{m\times m}$ is an orthogonal matrix with $\bb{W}^{(j)},\bb{Y}^{(j)}\in\R^{m\times j}$.
	Let us define $\bb{P}=\bb{I}-\beta\bb{vv}^{\top}$ for some $\bb{v}\in\R^m$ and let $\bb{z}^{(j+1)}=\beta\bb{Q}^{(j)}\bb{v}$.
	Then, \[\bb{Q}^{(j+1)} = \bb{Q}^{(j)}\bb{P} = \bb{I} - \bb{W}^{(j+1)}\bb{Y}^{(j+1)\top}, \]where $ \bb{W}^{(j+1)} =[\bb{W}^{(j)}|\bb{z}]$ and $ \bb{Y}^{(j+1)}=[\bb{Y}^{(j)}|\bb{v}]$ are each $m$-by-$(j+1)$. 
\end{lemma}
%Let us now show the proof for \cref{lem:WY}.
%\begin{proof}
%	A direct right multiplication of $\bb{P}:=\bb{I}_m - \beta\bb{v}\bb{v}^{\top}$ onto $\bb{Q}$ can be written as
%	\begin{equation*}
%	\bb{QP}=\bb{Q}-\beta\bb{Q}\bb{v}\bb{v}^{\top}.
%	\end{equation*}
%	Let us use the WY representation of $\bb{Q}$.
%	\begin{equation*}
%	\bb{QP}= \bb{I}_m - \bb{WY}^{\top} -\beta\bb{Q}\bb{v}\bb{v}^{\top} = \bb{I}_m - \bb{WY}^{\top} - \bb{z}\bb{v}^{\top}
%	\end{equation*}
%	Now note that the two subtracted terms are exactly the updated WY factors:
%	\[ \bb{W}_+\bb{Y}_+^{\top} = [\bb{W} \quad \bb{z}]\begin{bmatrix}
%	\bb{Y}^{\top}\\ 
%	\bb{v}^{\top}
%	\end{bmatrix} = \bb{WY}^{\top} + \bb{z}\bb{v}^{\top}.\]
%\end{proof}

\begin{algorithm2e}
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	\KwIn{$\bb{V}\in\R^{m \times r}$, $\bm{\beta}\in\R^{r}$ where $m > r$.}
	
	\KwOut{$\bb{W},\bb{Y}$}
	Initialize: $\bb{W}:=\bm{\beta}_1\bb{V}[:,1]$ and $\bb{Y}:=\bb{V}[:,1]$.\\
	\For{$j=2:r$}{
		$\bb{z}\gets \bm{\beta}_j \left[\bb{V}[:,j] - \bb{W}\left(\bb{Y}^{\top}\bb{V}[:,j]\right)\right]$\\
		$\bb{W} \gets [\bb{W}\quad \bb{z}]$ \tcc*{Update $\bb{W}$.}
		$\bb{Y} \gets [\bb{Y}\quad \bb{V}[:,j]]$ \tcc*{Update $\bb{Y}$.}
		\tcp{$\bb{W}$ and $\bb{Y}$ are now $m$-by-$j$ matrices.}
	}
	\Return $\bb{W},\bb{Y}$
	\caption{$\bb{W},\bb{Y}\gets {\tt buildWY}(V, \bm{\beta})$: Given a set of householder vectors $\{\bb{V}[:,i]\}_{i=1}^r$ and their corresponding constants $\{\bm{\beta}_i\}_{i=1}^r$, form the final $\bb{W}$ and $\bb{Y}$ factors of the WY representation of $\bb{P}_1\cdots \bb{P}_r$, where $\bb{P}_i := \bb{I}_m - \bm{\beta}_i\bb{v}_i\bb{v}_i^{\top}$}
	\label{algo:buildWY}
\end{algorithm2e}

In HQR, $\bb{A}$ is transformed into an upper triangular matrix $\bb{R}$ by identifying a Householder transformation that zeros out a column below the diagonal, then applying that Householder transformation to the bottom right partition. 
For example, the $k^{th}$ Householder transformation finds an $m-k+1$ sized Householder transformation that zeros out column $k$ below the diagonal and then applies it to the $(m-k+1)$-by-$(n-k)$ partition of the matrix, $\bb{A}[k:m,k+1:n]$.
Since the $k+1^{st}$ column is transformed by the $k^{th}$ Householder transformation, this algorithm must be executed serially as shown in \cref{algo:hhQR}.
The highest computational burden at each iteration falls on \cref{algo:hhQR} line 6, which requires Level-2 BLAS operations when computed efficiently. \par

In contrast, BQR replaces this step with Level-3 BLAS operations by partitioning $\bb{A}$ by groups of columns.
Let $\bb{A} = [\bb{C}_1 \cdots  \bb{C}_N]$ where $\bb{C}_1,\cdots,\bb{C}_{N-1}$ are each $m$-by-$r$, and $\bb{C}_N$ holds the remaining columns.
The $k^{th}$ block, $\bb{C}_k$, is transformed using HQR (\cref{algo:hhQR}) while building the WY representation of $\bb{P}_{(k-1)r+1}\cdots\bb{P}_{kr} = \bb{I}_m -\bb{W_k}\bb{Y_k}^{\top}$ as in \cref{algo:buildWY}.
Note that both \cref{algo:hhQR,algo:buildWY} are rich in Level-2 BLAS operations.
Then, $\bb{I} -\bb{Y_k}\bb{W_k}^{\top}$ is applied to $[\bb{C}_2 \cdots  \bb{C}_N]$ with two Level-3 BLAS operations as shown in line 6 of \cref{algo:blockHQR}.
BQR performs approximately $1-\cO(1/N)$ fraction of its FLOPs in Level-3 BLAS operations (see section 5.2.3 of \cite{golub2013matrix}), and can reap the benefits from the accelerated matrix-matrix-multiply and accumulate technology of TensorCore. 
Note that BQR does require more FLOPs when compared to HQR, but these additional FLOPs are negligble in high precision.
%TODO: some statement about experiment about this in low/mixed precision
A pseudoalgorithm for BQR is shown in \cref{algo:blockHQR}.
\begin{algorithm2e}
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	\KwIn{$\bb{A}\in\R^{m \times n}$, $r\in\R$ where $r < n$.}
	
	\KwOut{$\bb{Q},\bb{R}$}
	$N=\lceil \frac{n}{r}\rceil$\\
	\tcp{Let $\bb{A} = [\bb{C}_1 \cdots  \bb{C}_N]$ where all blocks except $\bb{C}_N$ are $m$-by-$r$ sized.}
	%\tcp{Let $n_i=ri$ for $i=1:N-1$ and $n_N=n$.} 
	\For{$i=1:N$}{
		$\bb{V}_i,\bm{\beta}_i,\bb{C}_i\gets$ {\tt hhQR}($\bb{C}_i$)\tcc*{\Cref{algo:hhQR}}
		%$\bb{V}_i,\bm{\beta}_i,\bb{A}_{n_{i-1}+1:m,n_{i-1}+1:n_i}\gets$ {\tt hhQR}$(\bb{A}_{n_{i-1}:m,n_{i-1}+1:n_i})$\tcc*{\Cref{algo:hhQR}}
		$\bb{W}_i,\bb{Y}_i \gets $ {\tt buildWY}$(\bb{V}_i,\bm{\beta}_i)$ \tcc*{\Cref{algo:buildWY}}
		\If {$i< N$}{
			$[\bb{C}_{i+1}\cdots\bb{C}_N]$ -= $\bb{Y}_i \left(\bb{W}_i^{\top}[\bb{C}_{i+1}\cdots\bb{C}_N]\right) $ \tcc*{update the rest: BLAS-3}
		}
	}
	%	\tcp{Upper-triangularize the last set of columns and get the last WY factors.}
	%	$\bb{V}_N,\bm{\beta}_N,\bb{A}_{n_{}:m,n_{N-1}+1:n}\gets$ {\tt hhQR}$(\bb{A}_{m_N+1:m,n_{N-1}+1:n})$\tcc*{\Cref{algo:hhQR}}
	%	$\bb{W}_N,\bb{Y}_N \gets $ {\tt buildWY}$(\bb{V}_N,\bm{\beta}_N)$ \tcc*{\Cref{algo:buildWY}}
	\tcp{$\bb{A}$ has been transformed into $\bb{R}=\bb{Q}^{\top}\bb{A}$.}
	\tcp{Now build $\bb{Q}$ using level-3 BLAS operations.} 
	$\bb{Q}\gets \bb{I}$\tcc*{$\bb{I}_m$ if full QR, and $\bb{I}_{m\times n}$ if thin QR.}
	\For{$i=N:-1:1$}{
		$\bb{Q}[(i-1)r+1:m,(i-1)r+1:n]$-= $\bb{W}_i \left(\bb{Y}_i^{\top}\bb{Q}[(i-1)r+1:m,(i-1)r+1:n]\right)$
	}
	\Return $\bb{Q},\bb{A}$
	\caption{$\bb{Q},\bb{R}\gets {\tt blockHQR}(\bb{A}, r)$: Perform Householder QR factorization of matrix $\bb{A}$ with column partitions of size $r$.}
	\label{algo:blockHQR}
\end{algorithm2e}
\subsubsection{BQR: Rounding Error Analysis}
We now present the basic structure for the rounding error analysis for \cref{algo:blockHQR}, which consist of: 1)HQR, 2)building the WY representation, and 3) updating the remaining blocks with the WY representation.
\paragraph{HQR within each block}
Consider the $k^{th}$ block, $\bb{C}_k$. 
We first apply \Cref{algo:hhQR} to rows $k:m$, which is equivalent to $r$ Householder transformations of size $m-(r-1)k$ if $k < N$ and $n-r(i-1)$ Householder transformations of size $m-(N-1)k$ if $k=N$. 
The general rounding error incurred is represented by \cref{eqn:rPs} and a uniform-precision rounding error for this step is shown in \cref{eqn:rPs-uniform}.
Let $\bb{R}_k$ be the $\bb{R}$ factor acquired from applying HQR on $\bb{C}_k$ and let $\bb{\Delta Q'}_k$ represent the backward error incurred in this step:
\begin{equation*}
	\bb{R}_k = (\bb{Q}_k+\bb{\Delta Q'}_k)^{\top}\bb{C}_k.
\end{equation*}
Adjusting \cref{eqn:rPs,eqn:rPs-uniform} for $k<N$ yields
\begin{align}
\|\bb{\Delta Q'}_k^{\top}\|_F &= \left|\left| \prod_{j=r}^{1}\left(\bb{P}_{(k-1)r+j} +\bb{\Delta P}_{(k-1)r+j}\right) - \prod_{j=r}^{1}\bb{P}_{(k-1)r+j} \right|\right|_F,\label{eqn:rPs-BQR}\\
&\leq \left(\prod_{i=1}^r(1+\tilde{\gamma}_{m-(r-1)k})-1\right)\prod_{i=r}^1\|\bb{P}_i\|_2 = (1+\tilde{\gamma}_{m-(r-1)k})^r-1. \label{eqn:rPs-BQR-uniform}
\end{align}
\paragraph{Build WY at each block}
%The WY representation of $r$ Householder transformations in full precision is exactly that, just another representation. 
We now calculate the rounding errors incurred from building the WY representation when given a set of Householder vectors and constants. 
Consider the $j^{th}$ update as shown in \cref{lem:WY} for the $k^{th}$ block.
Error accumulated from forming $\bb{v}$ is represented in \cref{eqn:vbound} and we only consider $\bb{z}^{(j+1)} =\beta\bb{Q}^{(j)}\bb{v}$. 
%TODO: is W just V?? CHECK!
Since $\bb{Q}^{j} = \bb{I} - \bb{W}^{(j)}\bb{Y}^{(j)\top}$, lets first consider $\bb{W}^{(j)}\bb{Y}^{(j)\top}\bb{v}$ where we left-multiply the matrices.
In general, we can write
\begin{equation*}
\bb{z}^{(j+1)} + \bb{\Delta z}^{(j+1)} = \hat{v} - (\bb{W}^{(j)} + \bb{\Delta W}^{(j)})(\bb{Y}^{(j)}+\bb{\Delta Y}^{(j)})^{\top}\hat{v}, 
\end{equation*}
where $\hat{v}$ is expressed in \cref{eqn:vbound}, and $\bb{\Delta W}^{(j)}$ and $\bb{\Delta Y}^{(j)}$ each represent backward rounding errors from $m-(r-1)k$-length and $j$-length dot products.
In uniform precision, this yields
\begin{equation*}
	\fl(\bb{W}^{(j)}\bb{Y}^{(j)\top}\bb{v})= (1+\tilde{\tth}_{m-(k-1)r})(1+\tilde{\tth}_{j})\bb{W}^{(j)}\bb{Y}^{(j)\top}\bb{v} = (1+\tilde{\tth}_{j+m-(k-1)r})\bb{W}^{(j)}\bb{Y}^{(j)\top}\bb{v},
\end{equation*}
where we've disregarded $\bb{\Delta v}$ momentarily.
The subtraction and the multiplication by $\beta$ only add in $\dd$ each to the errors, so we simplify via $j+m-(k-1)r+2 \approx m-rk$ since $j\leq r$.
Since $|\bb{\Delta v}| \leq \tilde{\gamma}_{m-(k-1)r}|\bb{v}|$, the approximation still holds.
Thus, we result in
\begin{align*}
	\fl(\bb{z}^{(j+1)}) &= (1+\tilde{\tth}_{m-rk})\bb{z}^{(j+1)}.
\end{align*}

\subsection{Block HQR with partitioned rows : Tall-and-Skinny QR (TSQR)}\label{sec:TSQR}