%\documentclass{article}
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL submission1-of.tex   Wed May 27 16:11:17 2020
%DIF ADD main-of.tex          Wed May 27 16:15:46 2020
%DIF 2a2
 %DIF > 
%DIF -------
\documentclass[review,onefignum,onetabnum]{siamart190516}

%Packages
\usepackage[utf8]{inputenc}
\usepackage{geometry, graphicx,wrapfig}
\usepackage{enumerate}
%\usepackage{amsmath,amssymb,amsfonts,amsthm, bm}
\usepackage{amsmath,amssymb,amsfonts,bm}
\usepackage{xcolor} %just for visible comments.
\usepackage[linesnumbered,ruled,vlined,algo2e]{algorithm2e}
\usepackage[toc,page]{appendix}
\usepackage{makecell}
%DIF 14a15
\usepackage{cleveref} %DIF > 
%DIF -------

% New theorems and commands
%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{corollary}[theorem]{Corollary}
%DIF 19c21
%DIF < \newtheorem{assump}[theorem]{Assumption}
%DIF -------
\newtheorem{assump}[theorem]{MP Setting} %DIF > 
%DIF -------
\newcommand\mycommfont[1]{\ttfamily\textcolor{orange}{#1}}
%\SetCommentSty{mycommfont}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\dd}{\delta}
\newcommand{\tth}{\theta}
\newcommand{\bb}[1]{\mathbf{#1}}
\newcommand{\fl}{\mathrm{fl}}
%DIF 28a30
\newcommand{\cO}{\mathcal{O}} %DIF > 
%DIF -------

\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}

%DIF 36a39-43
\setlength{\abovedisplayshortskip}{0pt} %DIF > 
\setlength{\belowdisplayshortskip}{0pt} %DIF > 
 %DIF > 
\crefname{algocf}{alg.}{algs.} %DIF > 
\Crefname{algocf}{Algorithm}{Algorithms} %DIF > 
%DIF -------
%\theoremstyle{definition}
%\newtheorem{definition}{Definition}[section]
%DIF 38d46
%DIF < \newcommand\comment[1]{\color{blue}#1}
%DIF -------
% Document
%DIF 40d47
%DIF < %\title{Low-Precision QR Factorization: \\ Analysis, Algorithms, and Applications}
%DIF -------
\title{\DIFdelbegin \DIFdel{Mixed-Precision analysis of }\DIFdelend \DIFaddbegin \DIFadd{Rounding Error Analysis of Mixed Precision Block }\DIFaddend Householder QR Algorithms}
\author{L. Minah Yang, Alyson Fox, and Geoffrey Sanders}
\date{\today}
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF CTRADITIONAL PREAMBLE %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\RequirePackage[stable]{footmisc} %DIF PREAMBLE
\DeclareOldFontCommand{\sf}{\normalfont\sffamily}{\mathsf} %DIF PREAMBLE
\providecommand{\DIFadd}[1]{{\protect\color{blue} \sf #1}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{{\protect\color{red} [..\footnote{removed: #1} ]}} %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}

\maketitle
\begin{abstract}
	Although mixed precision arithmetic has recently garnered interest for training dense neural networks, many other applications could benefit from the  speed-ups and lower storage if applied appropriately. 
	The growing interest in employing mixed precision computations motivates the need for rounding error analysis that properly handles behavior from mixed precision arithmetic.
	We \DIFdelbegin \DIFdel{present a framework for mixed precision analysis that builds on the foundations of rounding error analysis presented in \cite{Higham2002} and demonstrate its practicality by applying the analysis to various Householder QR Algorithms.
	In addition, we present successful results from using mixed precision QR factorization for some small-scale benchmark problems in graph clustering. 
	}\DIFdelend \DIFaddbegin \DIFadd{develop mixed precision variants of existing Householder QR algorithms and show error analyses supported by numerical experiments.
	%DIF > We present a framework for mixed precision analysis that builds on the foundations of rounding error analysis presented in \cite{Higham2002} and demonstrate its practicality by applying the analysis to various Householder QR Algorithms. 
	%DIF > In addition, we present successful results from using mixed precision QR factorization for some small-scale benchmark problems in graph clustering. 
	}\DIFaddend \blfootnote{This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344 and was supported by the LLNL-LDRD Program under Project No. 17-SI-004, LLNL-JRNL-795525-DRAFT.}
\end{abstract}
%DIF > TODO: remove graph stuff.
\section{Introduction}\label{sec:intro}
%DIF > \input{intro2}
\DIFaddbegin 

\DIFaddend The accuracy of a numerical algorithm depends on several factors, including numerical stability and well-conditionedness of the problem, both of which may be sensitive to rounding errors, the difference between exact and finite-precision arithmetic. 
Low precision floats use fewer bits than high precision floats to represent the real numbers and naturally incur larger rounding errors. 
Therefore, error attributed to round-off may have a larger influence over the total error when using low precision, and some standard algorithms \DIFdelbegin \DIFdel{that are in wide use may no longer be numerically stable when using half precision floating arithmetic and storage . 
}\DIFdelend \DIFaddbegin \DIFadd{may yield insufficient accuracy when using low precision storage and arithmetic.
%DIF > TODO: MINAH comment reviewer 
%DIF > that are in wide use may no longer be numerically stable when using half precision floating arithmetic and storage. 
}\DIFaddend However, many applications exist that would benefit from the use of \DIFdelbegin \DIFdel{lower }\DIFdelend \DIFaddbegin \DIFadd{low }\DIFaddend precision arithmetic and storage that are less sensitive to floating-point round off error, such as clustering or ranking graph algorithms \cite{vonLuxburg2007} or training dense neural networks \cite{micikevicius2018mixed}\DIFdelbegin \DIFdel{, to name a few}\DIFdelend .\par

Many computing applications today require solutions quickly and often under low size, weight, and power constraints\DIFdelbegin \DIFdel{(low SWaP), e.g., }\DIFdelend \DIFaddbegin \DIFadd{, such as in }\DIFaddend sensor formation, \DIFdelbegin \DIFdel{etc. 
Computing in low-precision arithmetic }\DIFdelend \DIFaddbegin \DIFadd{where
%DIF >  (low SWaP), e.g., sensor formation, etc. 
%DIF > TODO: Reviewer #2 asks what does weight mean here.
low precision computation }\DIFaddend offers the ability to solve many problems with improvement in all four parameters.
Utilizing \DIFdelbegin \DIFdel{mixed-precision}\DIFdelend \DIFaddbegin \DIFadd{mixed precision}\DIFaddend , one can achieve similar quality of computation as high-precision and still achieve 
speed, size, weight, and power constraint improvements. 
There have been several recent demonstrations of computing using \DIFdelbegin \DIFdel{half-precision arithmetic (16 bits}\DIFdelend \DIFaddbegin \DIFadd{IEEE half precision (fp16}\DIFaddend ) achieving around half an order to an order of magnitude improvement of these categories in comparison to \DIFaddbegin \DIFadd{single and }\DIFaddend double precision (\DIFdelbegin \DIFdel{64 bits}\DIFdelend \DIFaddbegin \DIFadd{fp32, fp64}\DIFaddend ).
Trivially, the size and weight of memory required for a specific problem is 4$\times$.
Additionally, there exist demonstrations that the power consumption improvement is similar
\cite{fagan2016powerwall}.
Modern accelerators (e.g., GPUs, Knights Landing, or Xeon Phi) are able to achieve this factor or better speedup improvements.
Several examples include:
(i)   2-4$\times$ speedup in solving dense large linear equations \cite{haidar2018iterative,haidar2019tensorcore},
(ii)  12$\times$ speedup in training dense neural networks,
and
(iii) 1.2-10$\times$ speedup in small batched dense matrix multiplication \cite{abdelfattah2019batched} (up to 26$\times$ for batches of tiny matrices).
Training deep artificial neural networks by employing lower precision arithmetic to various tasks such as multiplication \cite{Courbariaux2014Mult} and storage \cite{Courbariaux2014Storage} can easily be implemented on GPUs and are \DIFdelbegin \DIFdel{already }\DIFdelend a common practice in \DIFaddbegin \DIFadd{some }\DIFaddend data science applications.\par

The low precision computing environments that we consider are \emph{mixed precision} settings, which are designed to imitate those of new GPUs that employ multiple precision types for certain tasks. 
For example, Tesla V100's \DIFdelbegin \DIFdel{Tensor Cores perform matrix-multiply-and-accumulate of half precision input data with exact products and single precision (32 bits) summation accumulate \cite{nvdia}.
}\DIFdelend \DIFaddbegin \DIFadd{TensorCores perform block Fused Multiply Add operations (bFMAs), where matrix products of fp16 input data can be computed up to $16\times$ than that of fp64.
%DIF > with exact products and single precision (32 bits) summation accumulate \cite{nvdia}.
}\DIFaddend The existing rounding error analyses are built within what we call a \emph{uniform precision} setting, which is the assumption that all arithmetic operations and storage are performed via the same precision.
%DIF <  TODO: just say what we do, deterministic worst-case bounds, and that it is a necessary first step.   leave the discussion of probabilistic bounds for the conclusion. -Geoff
%DIF < %% One way to approximate mixed precision procedures with uniform precision error analysis is to pick the lowest precision within the mixed precision setting and use that precision for all operations.
%DIF < %% While this yields overtly pessimistic error bounds, any other way of employing uniform precision error analysis for mixed precision procedures could result in optimistic bounds that may not hold for all possible cases.
%DIF < or the lowest precision within the mixed precision setting yields either overtly optimistic or pessimistic bounds, and cannot accurately represent mixed precision settings such as Tensor Cores.
%DIF < %% the existing rounding error bounds are known to be pessimistic even for uniform precision settings (cite), we can expect these bounds to be especially pessimistic for mixed precision settings. 
%DIF < %%One form of battling the pessimistic nature of deterministic error bounds is probabilistic error bounds (c.f. \cite{higham2018new}), but these also suffer from being restricted to uniform precision procedures.
In this work, we develop \DIFdelbegin \DIFdel{a framework for deterministic mixed-precision rounding error analysis, and explore half-precision Householder QR factorization (HQR) algorithms for data and graph analysis applications. 
QR factorization is known to provide a backward stable solution to the linear least squares problem and thus, is ideal for mixed-precision. 
However, additional analysis is needed as the additional round-off error will effect orthogonality, and thus the accuracy of the solution. 
Here, we focus on analyzing specific algorithms in a specific set of types}\DIFdelend \DIFaddbegin \DIFadd{mixed precision variants of existing Householder }\DIFaddend (\DIFdelbegin \DIFdel{IEEE754 half, single, and double), but the framework we develop 
could be used on different algorithms or different }\DIFdelend \DIFaddbegin \DIFadd{HH) QR factorization algorithms and perform mixed precision error analysis. 
%DIF > a framework for deterministic mixed precision rounding error analysis, and explore half-precision Householder QR factorization (HQR) algorithms for data and graph analysis applications. 
%DIF > QR factorization is known to provide a backward stable solution to the linear least squares problem and thus, is ideal for mixed precision. 
%DIF > TODO: Remove it and just explain in response to reviewers. didn't serve purpose of paper mixed precision is an active area of research
%DIF > TODO: algorithms with better stability should be looked at first in mixed precision. 
}

%DIF > However, additional analysis is needed as the additional round-off error will effect orthogonality, and thus the accuracy of the solution. 
\DIFadd{This work focuses on analyzing a few algorithms that use fp16/fp32 as the low/high precision types, but the error analysis can be easily modified for different }\DIFaddend floating point types (such as \DIFdelbegin \DIFdel{fp16 or bfloat }\DIFdelend \DIFaddbegin \DIFadd{bfloat16 in }\DIFaddend \cite{tagliavini2018floating}).
\DIFdelbegin %DIFDELCMD < \par
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{This work discusses several aspects of using mixed-precision arithmetic: (i) error analysis that can more accurately describe mixed-precision arithmetic than existing analyses, (ii) algorithmic design that is more resistant against lower numerical stability associated with lower precision types, and (iii) an example where mixed-precision implementation performs as sufficiently as double-precision implementations}\DIFdelend %DIF > This work discusses several aspects of using mixed precision arithmetic: (i) error analysis that can more accurately describe mixed precision arithmetic than existing analyses, (ii) algorithmic design that is more resistant against lower numerical stability associated with lower precision types.
%DIF > , and (iii) an example where mixed precision implementation performs as sufficiently as double-precision implementations. 
\DIFaddbegin \DIFadd{The standard HH QR algorithm and its block variants that partition the columns (level-3 BLAS variant, see \cite{golub2013matrix,Higham2002}) and those that partition the columns (communication-avoiding algorithms of \cite{Demmel2012}) are presented in \cref{sec:algo}, then modified to support bFMAs and an ad hoc mixed precision setting that mimics NVIDIA TensorCores in \cref{sec:mpanalysis}}\DIFaddend .
Our key findings are that \DIFdelbegin \DIFdel{the new mixed-precision error analysis produces }\DIFdelend \DIFaddbegin \DIFadd{mixed precision error analyses produce }\DIFaddend tighter error bounds \DIFdelbegin \DIFdel{, that some block QR algorithms by Demmel et al. \cite{Demmel2012} are able to operate in low precision more robustly }\DIFdelend \DIFaddbegin \DIFadd{as supported by experiments in \cref{sec:NE}, algorithms that utilize level-3 BLAS operations can easily be modified to incorporate TensorCore bFMAs, and a row partition block algorithm operates  more robustly in mixed precision }\DIFaddend than non-block techniques \DIFdelbegin \DIFdel{, and that some small-scale benchmark graph clustering problems can successfully solved with mixed-precision arithmetic.
}\DIFdelend \DIFaddbegin \DIFadd{in certain regimes.
%DIF > that some small-scale benchmark graph clustering problems can be successfully solved with mixed precision arithmetic.
%DIF > TODO: MINAH rewrite outline of paper paragraph, and remove graph references in above paragraph.
%DIF > In Section~\ref{sec:FPREA}, we will give an overview of the modern developments in hardware that motivates rounding error analysis that supports multiple precision types, and we will present a set of error analysis tools. 
%DIF > The HQR factorization algorithm and a mixed precision rounding error analysis of its implementation is discussed in Section~\ref{sec:HQRf}.
%DIF > In Section~\ref{sec:TSQR}, we present the TSQR algorithm as well as numerical experiments that show that TSQR can be useful in low precision environments. Section~\ref{sec:Apps} explores the use of low and mixed precision QR algorithms as subroutines for an application: spectral clustering.
}\DIFaddend 


%DIF < Since communication-avoiding, parallelizable QR algorithms already exist for tall-and-skinny matrices, we study how those algorithms behave in half-precision. 
%DIF < We simulate half-precision arithmetic in our experiments in various ways that include conversions into single precision for computation and half precision for storage.
%DIF < While the standard HQR factorization algorithms are highly unstable in half-precision, our numerical simulations show that simulated mixed-precision implementation outperforms the pessimistic error bound and the Tall-and-Skinny QR (TSQR) algorithm often reduces the backward error of QR factorization. 
%DIF < These results motivate detailed numerical analysis of half precision block QR factorization both for the purposes of replacing higher-precision QR (in applications less sensitive to error) and using the half precision versions to produce warm starts that initialize higher precision QR factorization.\par
%DIF < We incorporated mixed-precision QR factorization into two applications: spectral clustering and sparse regression in the context of discovery of equations. 
%DIF < When using subspace iteration for graph clustering applications, half precision accuracy in forming the eigenspace is sufficient for clustering with high precision and recall for some small-scale benchmark problems. 
%DIF < Similarly, single precision accuracy in data-driven discovery of a simple system of ODEs is comparable to results from using double precision, and may even be more robust in noisy systems.\par
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < TODO: only answer why these might want mixed-precision, and how they could use this error analysis
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < Although we do not include any time/clockin
%DIF < Algorithms that followed the standardization of IEEE 754 floating point numbers relied on ... such that using these same algorithms for half precision is often infeasible and become unstable quickly even with ``small'' problems. 
%DIF < An algorithm is \emph{stable} if small perturbations in the input result in a small perturbation of the output, and \emph{unstable} when they admit large changes in the output.
%DIFDELCMD < 

%DIFDELCMD < %%%
\subsection{\DIFdel{Preliminaries}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
%DIF < TODO: save space and add clarity by only talking about thin QR.
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Given a matrix $\bb{A} \in \R^{m \times n}$ for $m\geq n$, we consider performing the }\DIFdelend \DIFaddbegin \section{\DIFadd{Background: Build up to rounding error analysis for inner products}}\label{sec:background}
%DIF > \input{background}
\DIFadd{In this section, we introduce the basic motivations and tools for mixed precision rounding error analysis needed for the }\DIFaddend {\it QR factorization}\DIFdelbegin \DIFdel{, 
where }\begin{displaymath}\DIFdel{$\bb{A} = \bb{QR},
\qquad
\bb{Q} \in \R^{m \times m},
\qquad
\bb{R} \in \R^{m \times n},$}\end{displaymath}
%DIFAUXCMD
\DIFdel{$\bb{Q}$ is orthogonal , $\bb{Q}^\top \bb{Q} = \bb{I}_{m\times m}$ , and $\bb{R}$ is upper-trapezoidal, $\bb{R}_{ij} = 0$ }\DIFdelend \DIFaddbegin \DIFadd{.
A matrix $\bb{A} \in \R^{m \times n}$ }\DIFaddend for \DIFdelbegin \DIFdel{$i>j$.
The above formulation is }\DIFdelend \DIFaddbegin \DIFadd{$m\geq n$ can be written as
}\[\DIFadd{\bb{A} = \bb{QR}=\begin{bmatrix}\bb{Q}_{1} & \bb{Q}_2\end{bmatrix} \begin{bmatrix}\bb{R}_{1} \\ \bb{0}_{m-n \times n}\end{bmatrix} = \bb{Q}_{1}\bb{R}_{1},\vspace{-0.3cm}%DIF > \qquad\bb{Q} \in \R^{m \times m},\qquad\bb{R} \in \R^{m \times n},
}\]
\DIFadd{where an orthogonal $\bb{Q}\in\R^{m\times m}$ and an upper trapezoidal $\bb{R}$ form }\DIFaddend a \emph{full} QR factorization, \DIFdelbegin \DIFdel{whereas a more efficient }\DIFdelend \DIFaddbegin \DIFadd{and $\bb{Q}_{1}\in\R^{m\times n},\bb{R}_{1}\in\R^{n\times n}$ form a }\DIFaddend \emph{thin} QR factorization\DIFdelbegin \DIFdel{results in $\bb{Q}\in\R^{m\times n}$ }\DIFdelend \DIFaddbegin \DIFadd{.
%DIF >  results in , that is
%DIF > \[
%DIF > \bb{A} = \bb{QR} = \begin{bmatrix}\bb{Q}_{1} & \bb{Q}_2\end{bmatrix} \begin{bmatrix}\bb{R}_{1} \\ \bb{0}_{m-n \times n}\end{bmatrix} = \bb{Q}_{1}\bb{R}_{1}.
%DIF > \]
If $\bb{A}$ is full rank then the columns of $\bb{Q}_{1}$ are orthonormal
%DIF >  (i.e. $\bb{Q}_{1}^{\top}\bb{Q}_{1}=\bb{I}_{n\times n}$) 
}\DIFaddend and \DIFdelbegin \DIFdel{$\bb{R}\in\R^{n\times n}$, that is }\[
\DIFdel{$\bb{A} = \bb{QR} = \begin{bmatrix}\bb{Q}_1 & \bb{Q}_2\end{bmatrix} \begin{bmatrix}\bb{R}_1 \\ \bb{0}_{m-n \times n}\end{bmatrix} = \bb{Q}_1\bb{R}_1.
$}\]
%DIFAUXCMD
\DIFdel{Here, $\bb{Q}_1\bb{R}_1$ is the \emph{thin} QR factorization, where the columns of $\bb{Q}_1$ are orthonormal
, and }\DIFdelend $\bb{R}_1$ is \DIFdelbegin \DIFdel{upper-triangular}\DIFdelend \DIFaddbegin \DIFadd{upper triangular}\DIFaddend .
In many applications, computing the \emph{thin} decomposition requires less computation and is sufficient in performance. 
While important definitions are stated explicitly in the text, Table~\ref{table:notation} serves to establish basic notation.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{In Section~\ref{sec:FPREA}, we will give an overview of the modern developments in hardware that motivates rounding error analysis that supports multiple precision types, and we will present a set of error analysis tools. 
The HQR factorization algorithm and a mixed-precision rounding error analysis of its implementation is discussed in Section~\ref{sec:HQRf}.
In Section~\ref{sec:TSQR}, we present the TSQR algorithm as well as numerical experiments that show that TSQR can be useful in low precision environments. Section~\ref{sec:Apps} explores the use of low and mixed precision QR algorithms as subroutines for an application: spectral clustering. 
}%DIFDELCMD < \begin{table}[h!]
%DIFDELCMD < 	\centering
%DIFDELCMD < 	\begin{tabular}{|m{3cm}|m{9cm}|c|}
%DIFDELCMD < 		\hline
%DIFDELCMD < 		%%%
%DIF < DONE: change table have the following 3 columns, add sections
		\DIFdelFL{Symbol(s) }%DIFDELCMD < & %%%
\DIFdelFL{Definition(s) }%DIFDELCMD < & %%%
\DIFdelFL{Section(s) }%DIFDELCMD < \\
%DIFDELCMD < 		\hline
%DIFDELCMD < 		%%%
\DIFdelFL{$\bb{Q}$  }%DIFDELCMD < & %%%
\DIFdelFL{Orthogonal factor of matrix $\bb{A}\in\R^{m\times n}$: $m$-by-$m$ (full) or $m$-by-$n$ (thin)  }%DIFDELCMD < & %%%
\DIFdelFL{\ref{sec:intro}}%DIFDELCMD < \\
%DIFDELCMD < 		%%%
\DIFdelFL{$\bb{R}$ }%DIFDELCMD < & %%%
\DIFdelFL{Upper triangular or trapezoidal factor of matrix $\bb{A}\in\R^{m\times n}$:  $m$-by-$n$ (full) or $n$-by-$n$ (thin)  }%DIFDELCMD < &  %%%
\DIFdelFL{\ref{sec:intro}}%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{$\bb{A}^{(k)}$ }%DIFDELCMD < & %%%
\DIFdelFL{Matrix $\bb{A}$ after $k$ Householder transformations. }%DIFDELCMD < &%%%
\DIFdelFL{\ref{sec:HQRfA}}%DIFDELCMD < \\
%DIFDELCMD < 		\hline
%DIFDELCMD < 		%%%
\DIFdelFL{$\fl(\bb{x})$, $\hat{\bb{x}}$ }%DIFDELCMD < & %%%
\DIFdelFL{Quantity $\bb{x}$ calculated from floating point operations }%DIFDELCMD < & %%%
\DIFdelFL{\ref{sec:FPREA} }%DIFDELCMD < \\
%DIFDELCMD < 		%%%
\DIFdelFL{$b$, $t$, $\mu$, $\eta$  }%DIFDELCMD < & %%%
\DIFdelFL{Base/precision/mantissa/exponent bits }%DIFDELCMD < & %%%
\DIFdelFL{\ref{sec:FPREA} }%DIFDELCMD < \\
%DIFDELCMD < 		\tt{Inf} & %%%
\DIFdelFL{Values outside the range of representable numbers }%DIFDELCMD < & %%%
\DIFdelFL{\ref{sssec:NormalizeHV} }%DIFDELCMD < \\ %%%
%DIF < https://www.doc.ic.ac.uk/~eedwards/compsys/float/nan.html
		\DIFdelFL{$k$ }%DIFDELCMD < & %%%
\DIFdelFL{Number of FLOPs }%DIFDELCMD < &  %%%
\DIFdelFL{\ref{sec:FPREA}}%DIFDELCMD < \\
%DIFDELCMD < 		%%%
\DIFdelFL{$u_q$ }%DIFDELCMD < & %%%
\DIFdelFL{Unit round-off for precision $t$ and base $b$: $\frac{1}{2}b^{1-t}$ }%DIFDELCMD < & %%%
\DIFdelFL{\ref{sec:FPREA} }%DIFDELCMD < \\  
%DIFDELCMD < 		%%%
%DIF < TODO: this beta is different from householder beta.
		\DIFdelFL{$\dd_{q}$ }%DIFDELCMD < &%%%
\DIFdelFL{Quantity bounded by: $|\dd_{q}| < u_q$ }%DIFDELCMD < &  %%%
\DIFdelFL{\ref{sec:FPREA} }%DIFDELCMD < \\
%DIFDELCMD < 		%%%
\DIFdelFL{$\gamma_{q}^{(k)}$,  $\tth_{q}^{(k)}$}%DIFDELCMD < & %%%
\DIFdelFL{$\frac{ku_q}{1-ku_q}$, Quantity bounded by: $|\tth_{q}^{(k)}|\leq\gamma_q^{(k)}$ }%DIFDELCMD < &  %%%
\DIFdelFL{\ref{sec:FPREA} }%DIFDELCMD < \\
%DIFDELCMD < 		%%%
%DIF < $\tth_{q}^{(k)}$ & Quantity bounded by: $|\tth_{q}^{(k)}|\leq\gamma_q^{(k)}$ &  \ref{sec:FPREA} \\ 
		%DIFDELCMD < \hline
%DIFDELCMD < 		%%%
\DIFdelFL{${\bb x}$, ${\bb A}$  }%DIFDELCMD < & %%%
\DIFdelFL{Vector, matrix  }%DIFDELCMD < & %%%
\DIFdelFL{\ref{sec:FPREA} }%DIFDELCMD < \\
%DIFDELCMD < 	%%%
%DIF < 	${\bb A}$ & Matrix & \ref{sec:intro}\\
		\DIFdelFL{$m$, $n$ }%DIFDELCMD < & %%%
\DIFdelFL{Number of rows, columns of matrix, or length of vector}%DIFDELCMD < &  %%%
\DIFdelFL{\ref{sec:intro}}%DIFDELCMD < \\
%DIFDELCMD < 		%%%
\DIFdelFL{$i$, $j$ }%DIFDELCMD < & %%%
\DIFdelFL{Row, column index of matrix or vector }%DIFDELCMD < & %%%
\DIFdelFL{\ref{sec:HQRfA} }%DIFDELCMD < \\
%DIFDELCMD < 		%%%
\DIFdelFL{$\|{\bf x}\|_2$, $\|\bb{A}\|_2$ }%DIFDELCMD < & %%%
\DIFdelFL{Vector operator 2-norm }%DIFDELCMD < & %%%
\DIFdelFL{\ref{sec:HQRf}}%DIFDELCMD < \\
%DIFDELCMD < 		%%%
\DIFdelFL{$|c|$, $|\bb{x}|$ ,$|\bb{A}|$ }%DIFDELCMD < & %%%
\DIFdelFL{Absolute value of constant, all elements of vector, matrix }%DIFDELCMD < & %%%
\DIFdelFL{\ref{sec:HQRf} }%DIFDELCMD < \\
%DIFDELCMD < 		%%%
\DIFdelFL{$\bb{x}_i$, $\hat{e}_i$  }%DIFDELCMD < & %%%
\DIFdelFL{$i^{th}$ element of vector $\bb{x}$, cardinal vector }%DIFDELCMD < &  %%%
\DIFdelFL{\ref{sec:HQRfA}, \ref{sec:HQRf}}%DIFDELCMD < \\
%DIFDELCMD < 		%%%
%DIF < $\bb{A}[a:b, c:d]$ &Rows $a$ to $b$ and columns $c$ to $d$ of matrix $\bb{A}$ & \ref{sec:HQRfA}\\
		\DIFdelFL{$\bb{A}[a:b,:]$, $\bb{A}[:,c:d]$ }%DIFDELCMD < & %%%
\DIFdelFL{Rows $a$ to $b$, columns $c$ to $d$ of matrix $\bb{A}$}%DIFDELCMD < & %%%
\DIFdelFL{\ref{sec:HQRfA}}%DIFDELCMD < \\
%DIFDELCMD < 		%%%
\DIFdelFL{$\bb{0}_{m\times n}$, $\bb{I}_{n}$ }%DIFDELCMD < & %%%
\DIFdelFL{$m$-by-$n$ zero matrix, $n$-by-$n$ identity  matrix }%DIFDELCMD < &  %%%
\DIFdelFL{\ref{sec:intro}}%DIFDELCMD < \\
%DIFDELCMD < 		%%%
%DIF < $\bb{I}_{n} & $n$-by-$n$ identity  matrix  & \ref{sec:HQRfA} \\
		\DIFdelFL{$\bb{I}_{m\times n}$ }%DIFDELCMD < & %%%
\DIFdelFL{$[\bb{I}_{n} \quad \bb{0}_{n \times (m-n)}]^{\top}$ }%DIFDELCMD < & %%%
\DIFdelFL{\ref{sec:TSQR}}%DIFDELCMD < \\ \hline
%DIFDELCMD < 		%%%
\DIFdelFL{$\bb{P}_{\bb{v}}$, $\bb{P}_i$ }%DIFDELCMD < & %%%
\DIFdelFL{Householder transformation define by $\bb{v}$, $i^{th}$ Householder transformation in HQR}%DIFDELCMD < & %%%
\DIFdelFL{\ref{sec:HQRfA}}%DIFDELCMD < \\
%DIFDELCMD < 		%%%
%DIF < $\bb{P}_i$ & $i^{th}$ Householder transformation in the HQR algorithm & \ref{sec:HQRfA} \\ 
		%DIFDELCMD < \hline		
%DIFDELCMD < 		%%%
\DIFdelFL{$u_s, u_p, u_w$ }%DIFDELCMD < & %%%
\DIFdelFL{Unit round-off for sum, product, and storage (write) }%DIFDELCMD < & %%%
\DIFdelFL{\ref{ssec:IP}}%DIFDELCMD < \\ 
%DIFDELCMD < %%%
%DIF < 		$\gamma_{p,q}^{(k_p,k_q)}$ & $(1+\gamma_p^{(k_p)})(1+\gamma_q^{(k_q)})-1$ & \ref{ssec:IP}\\
%DIF < 		$\tth_{p,q}^{(k_p,k_q)}$ & Quantity bounded by: $|\tth_{p,q}^{(k_p,k_q)}|<\gamma_{p,q}^{(k_p,k_q)}$ & \ref{ssec:IP}\\
		%DIFDELCMD < \hline
%DIFDELCMD < 	\end{tabular}
%DIFDELCMD < 	%%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{Basic definitions}}
	%DIFAUXCMD
%DIFDELCMD < \label{table:notation}
%DIFDELCMD < \end{table}
%DIFDELCMD < 

%DIFDELCMD < %%%
\section{\DIFdel{Floating Point Numbers and Error Analysis Tools}}%DIFAUXCMD
\addtocounter{section}{-1}%DIFAUXCMD
%DIFDELCMD < \label{sec:FPREA}
%DIFDELCMD < %%%
%DIF < TODO: due to space limitations a few sentences and refs in the intro will have to suffice.   -Geoff
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \input{deftable}
%DIF > \Cref{sec:backgroundRE} introduces basic concepts for rounding error analysis, and \Cref{sec:backgroundIP} exemplifies the need for mixed precision rounding error analysis for the inner product.
\DIFaddend \subsection{\DIFdelbegin \DIFdel{Representation }\DIFdelend \DIFaddbegin \DIFadd{Basic rounding error analysis }\DIFaddend of \DIFdelbegin \DIFdel{Real Numbers}\DIFdelend \DIFaddbegin \DIFadd{floating point operations}\DIFaddend }%DIF < %% Consider floating point number systems that are defined by
%DIF < %% \begin{equation}
%DIF < %% \text{significand} \times \text{base}^{\text{exponent}}.
%DIF < %% \end{equation}
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < %% This is the generic form of floating point representations, including the IEEE 754 Standard which was established in 1985 and has been been accepted and followed by most modern machines since. %\cite?
\DIFdelend \DIFaddbegin \label{sec:backgroundRE}
\DIFaddend We use and analyze the \DIFdelbegin \DIFdel{typical }\DIFdelend IEEE 754 Standard floating point number systems\DIFaddbegin \DIFadd{, shown in \cref{table:ieee}}\DIFaddend .
Let $\F \subset \R$ denote the space of some floating point number system with base $b\in\mathbb{N}$, precision $t\in\mathbb{N}$, significand $\mu\in\mathbb{N}$, and exponent range $[\eta_{\text{min}}, \eta_{\text{max}}]\subset \mathbb{Z}$.
Then every element $y$ in $\F$ can be written as 
\begin{equation}
y = \pm \mu\times b^{\eta-t},
%DIF < = \pm \frac{\mu}{b^t}b^{\eta},
\label{eqn:FPbasic}
\end{equation} 
where $\mu$ is any integer in $[0,b^{t}-1]$ and $\eta$ is an integer in  $[\eta_{\text{min}}, \eta_{\text{max}}]$.
\DIFdelbegin \DIFdel{While base, precision, and exponent range are fixed and define a floating point number system, the sign, significand, and exponent identifies a single number within that system:
}%DIFDELCMD < 

%DIFDELCMD < \vspace{.2cm}
%DIFDELCMD < \begin{center}
%DIFDELCMD < 	%%%
\DIFdelend %DIF > While base, precision, and exponent range are fixed and define a floating point number,the sign, significand, and exponent identifies a unique number within that system.
\DIFaddbegin \DIFadd{Although operations we use on $\R$ cannot be replicated exactly due to the finite cardinality of $\F$, we can still approximate the accuracy of analogous floating point operations (FLOPs).
We adopt the rounding error analysis tools described in \cite{Higham2002}, which allow a relatively simple framework for formulating error bounds for complex linear algebra operations. 
An analysis of FLOPs (see Theorem 2.2 \cite{Higham2002}) shows that the relative error is 
controlled by the unit round-off, $u:=\frac{1}{2}b^{1-t}$ in uniform precision settings. 
In mixed precision settings we denote the higher precision unit round-off with $u^{(h)}$ (h for high) and the lower precision unit round-off with $u^{(l)}$ (l for low).}\par 
\vspace{-.3cm}
\begin{table}[H]
	\DIFaddendFL \begin{tabular}{||l|c|c|c|c|c|c||} 
		\hline 
		Name & $b$ & $t$ & \# of exponent bits & $\eta_{\text{min}}$ & $\eta_{\text{max}}$ & unit round-off $u$ \\ \hline 
		\DIFaddbeginFL \DIFaddFL{fp16 (}\DIFaddendFL IEEE754 half\DIFaddbeginFL \DIFaddFL{)}\DIFaddendFL & 2 & 11 & 5 & -15 & 16  & {\tt 4.883e-04} \\ \hline 
		\DIFaddbeginFL \DIFaddFL{fp32 (}\DIFaddendFL IEEE754 single\DIFaddbeginFL \DIFaddFL{)}\DIFaddendFL & 2 & 24 & 8 & -127 & 128  & {\tt 5.960e-08} \\ \hline 
		\DIFaddbeginFL \DIFaddFL{fp64 (}\DIFaddendFL IEEE754 double\DIFaddbeginFL \DIFaddFL{)}\DIFaddendFL & 2 & 53 & 11 & -1023 & 1024 & {\tt 1.110e-16} \\ \hline 
	\end{tabular}
	\DIFdelbeginFL %DIFDELCMD < \end{center}
%DIFDELCMD < \vspace{.2cm}
%DIFDELCMD < %%%
%DIF < %%	\caption{IEEE754 formats and their primary attributes.} % with $j$ exponent bits ranging from $1-2^{j-1}$ to $2^{j-1}$.}
%DIF < %%	\label{table:ieee}
%DIF < %%\end{table}
\DIFdelendFL %DIF > \end{center}
	\DIFaddbeginFL \caption{\DIFaddFL{IEEE754 formats and their primary attributes.}} %DIF >  with $j$ exponent bits ranging from $1-2^{j-1}$ to $2^{j-1}$.}
	\label{table:ieee}
\end{table}
\vspace{-.8cm}
\DIFaddend 

\DIFdelbegin \DIFdel{Although operations we use on $\R$ cannot be replicated exactly due to the finite cardinality of $\F$, we can still approximate the accuracy of analogous floating point operations.
We adopt the rounding error analysis tools described in \cite{Higham2002}, which allow a relatively simple framework for formulating error bounds for complex linear algebra operations. 
A short analysis of floating point operations (see Theorem 2.2 \cite{Higham2002}) shows that the relative error is 
controlled by the unit round-off, $u:=\frac{1}{2}b^{1-t}$. }%DIFDELCMD < \par 
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend Let `op' be any basic operation \DIFdelbegin \DIFdel{between 2 floating point numbers }\DIFdelend from the set OP $=\{+, -, \times, \div\}$ \DIFaddbegin \DIFadd{and let $x,y\in \R$}\DIFaddend .
The true value $(x\text{ op }y)$ lies in $\R$, and it is rounded using some conversion to a floating point number, $\fl(x\text{ op }y)$, admitting a rounding error. 
The IEEE 754 Standard requires \emph{correct rounding}, which rounds the exact solution $(x\text{ op }y)$ to the closest floating point number and, in case of a tie, to the floating point number that has a mantissa ending in an even number.
\emph{Correct rounding} gives us an assumption for the error model where a single basic floating point operation yields a relative error, $\dd$, bounded in the following sense:
\begin{equation}
\fl(x\text{ op }y) = (1 + \dd)(x\text{ op }y),\quad |\dd|\leq u, \quad \text{op}\in\{+, -, \times, \div\}. \label{eqn:singlefpe}
\end{equation}
%DIF < The true value $(x\text{ op }y)$ lies in $\R$ and it is rounded to the nearest floating point number, $\fl(x\text{ op }y)$, admitting a rounding error. 
%DIF < A short analysis (cf. Theorem 2.2 \cite{Higham2002}) shows that the relative error $|\dd|$ is bounded by the unit round-off, $u:=\frac{1}{2}b^{1-t}$. \par
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{We use Equation \ref{eqn:singlefpe} }\DIFdelend \DIFaddbegin \DIFadd{We use \cref{eqn:singlefpe} }\DIFaddend as a building block in accumulating errors from successive \DIFdelbegin \DIFdel{floating point operations (FLOPs).
For example, consider computing $x+y+z$, where $x,y,z\in\R$.
Assume that the machine can only compute one operation at a time.
We take the convention of computing the left-most operation first.
Then there is a rounding error in computing $\hat{s_1}:= \fl(x+y) = (1+\dd)(x+y)$, and another rounding error in computing $\hat{s_2}:= \fl(\hat{s_1}+ z) = (1+\tilde{\dd})(\hat{s_1}+z)$, where $|\dd|,|\tilde{\dd}|<u$.
Then, 
%DIF < %% Let's expand the final result:
}\begin{displaymath}
\DIFdel{$\fl(x+y+z) = (1+\tilde{\dd})(1+\dd) (x+y) + (1+\tilde{\dd}) z.$%DIFDELCMD < \label{eqn:FPbasic3}%%%
%DIF < %% &= \fl(\fl(x+y)+z) = \fl((1+\dd)(x+y)+z)\\
%DIF < %% &= (1+\tilde{\dd})\left((1+\dd)(x+y) + z\right) \\
}\end{displaymath}
%DIFAUXCMD
%DIF < %% We can see that keeping track of rounding errors from each operation can quickly grow to be challenging, even with just two successive operations. 
\DIFdel{Multiple successive }\DIFdelend \DIFaddbegin \DIFadd{FLOPs.
Successive }\DIFaddend operations introduce multiple rounding error terms, and keeping track of all errors is challenging.
%DIF < A way of simplifying complicated expressions like Equation~{\ref{eqn:FPbasic3} is crucial in developing error analyses for complex linear algebra operations.
Lemma \ref{lem:gamma} introduces a convenient and elegant bound that simplifies accumulation of rounding error. 
\begin{lemma}[Lemma 3.1 \cite{Higham2002}]
	\label{lem:gamma}
	Let $|\dd_i|<u$\DIFaddbegin \DIFadd{, $\rho_i =\pm 1$ for $i=1:k$, }\DIFaddend and \DIFdelbegin \DIFdel{$\rho_i \in\{-1, +1\}$, for $i = 1 , \cdots, k$ and }\DIFdelend $ku < 1$. Then, 
	\begin{equation}	\prod_{i=1}^k (1+\dd_i)^{\rho_i} = 1 + \tth\DIFdelbegin \DIFdel{^{(k)}}\DIFdelend \DIFaddbegin \DIFadd{_{k}}\DIFaddend ,
	\qquad \DIFdelbegin \DIFdel{\mbox{where} }%DIFDELCMD < \qquad
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \DIFadd{\mbox{where } }\DIFaddend |\tth\DIFdelbegin \DIFdel{^{(k)}}\DIFdelend \DIFaddbegin \DIFadd{_{k}}\DIFaddend |\leq \frac{ku}{1-ku}=:\gamma\DIFdelbegin \DIFdel{^{(k)}}\DIFdelend \DIFaddbegin \DIFadd{_{k}}\DIFaddend .
	\end{equation}
	%DIF <  NOTE: I recommend skipping as \gamma^{(ck)} should suffice. -Geoff
	%DIF < We also use 
	%DIF < \begin{equation*}
	%DIF < \tilde{\gamma}^{(k)} = \frac{cku}{1-cku},
	%DIF < \end{equation*}
	%DIF < where $c>0$ is a small integer.  
\DIFaddbegin $$\DIFadd{\mbox{Additionally, we define $\tilde{\tth}_k$ that satisfies }|\tilde{\tth}_k| \leq \tilde{\gamma}_k,\mbox{ where } \tilde{\gamma}_{k} = \frac{cku}{1-cku} \mbox{ for a small integer, $c>0$.}}$$
	%DIF > 	We also use 
	%DIF > 	\begin{equation*}
	%DIF > 	\tilde{\gamma}_{k} = \frac{cku}{1-cku},
	%DIF > 	\end{equation*}
	%DIF > 	where $c>0$ is a small integer and further extend this to $\tth$ so that $|\tilde{\tth}_k| \leq \tilde{\gamma}_k$.
\DIFaddend \end{lemma}
%In other words, \DIFdelbegin \DIFdel{$\tth^{(k)}$ }\DIFdelend \DIFaddbegin \DIFadd{$\tth_{k}$ }\DIFaddend represents the accumulation of rounding errors from $k$ successive operations, and it is bounded by \DIFdelbegin \DIFdel{$\gamma^{(k)}$. 
%Allowing $\tth^{(k)}$'s to be any arbitrary value within the corresponding $\gamma^{(k)}$ bounds further aids in keeping a clear, simple error analysis}\DIFdelend \DIFaddbegin \DIFadd{$\gamma_{k}$. 
%%DIF > Allowing $\tth_{k}$'s to be any arbitrary value within the corresponding $\gamma_{k}$ bounds further aids in keeping a clear, simple error analysis. 
%In more complicated routines shown in later sections, we use the tilde notation ($\tilde{\gamma}_k$) to permit only keeping track of the leading order error terms}\DIFaddend .
%Applying this lemma to \DIFdelbegin \DIFdel{our example of adding three numbers results in
%}\DIFdelend \DIFaddbegin \DIFadd{the computation of $x+y+z$, where $x,y,z\in\R$, results in
%%DIF > For example, consider computing $x+y+z$, where $x,y,z\in\R$ with a machine that can only compute one operation at a time.
%%DIF > Then, there is a rounding error in computing $\hat{s_1}:= \fl(x+y) = (1+\dd)(x+y)$, and another rounding error in computing $\hat{s_2}:= \fl(\hat{s_1}+ z) = (1+\dd')(\hat{s_1}+z)$, where $|\dd|,|\dd'|<u$.
%%DIF > Then, 
%%DIF > \begin{equation}
%%DIF > \fl(x+y+z) = (1+\tilde{\dd})(1+\dd) (x+y) + (1+\tilde{\dd}) z.\label{eqn:FPbasic3}
%%DIF > \end{equation}
%}\DIFaddend \begin{equation}
%\fl(x+y+z) = (1+\DIFdelbegin %DIFDELCMD < \tilde{\dd}%%%
%\DIFdelend \DIFaddbegin \DIFadd{\dd'}\DIFaddend )\DIFaddbegin \left(\DIFaddend (1+\dd) (x+y) +\DIFdelbegin \DIFdel{(1+}%DIFDELCMD < \tilde{\dd}%%%
%\DIFdel{)}\DIFdelend z\DIFaddbegin \right) \DIFaddend = (1+\tth\DIFdelbegin \DIFdel{^{(2)}}\DIFdelend \DIFaddbegin \DIFadd{_{2}}\DIFaddend )(x+y) + (1+\tth\DIFdelbegin \DIFdel{^{(1)}}\DIFdelend \DIFaddbegin \DIFadd{_{1}}\DIFaddend )z\DIFdelbegin \DIFdel{. }\DIFdelend \DIFaddbegin \DIFadd{, }\DIFaddend \label{eqn:FPbasic4}
%\DIFdelbegin %DIFDELCMD < \end{equation}
%%DIFDELCMD < %%%
%\DIFdel{Since $|\tth^{(1)}| \leq \gamma^{(1)} < \gamma^{(2)}$}\DIFdelend \DIFaddbegin }
%\DIFadd{where $|\dd|,|\dd'|<u$. 
%Since $|\tth_{1}| \leq \gamma_{1} < \gamma_{2}$}\DIFaddend , we can further simplify \DIFdelbegin \DIFdel{Equation~\ref{eqn:FPbasic4} to
%}\DIFdelend \DIFaddbegin \DIFadd{\cref{eqn:FPbasic4} to
%}\DIFaddend \begin{equation}
%\fl(x+y+z) = (1+\DIFdelbegin %DIFDELCMD < \tilde{\tth}%%%
%\DIFdel{^{(2)}}\DIFdelend \DIFaddbegin \DIFadd{\tth'_{2}}\DIFaddend )(x+y+z), \quad \mbox{where} \quad \DIFdelbegin %DIFDELCMD < \tilde{\tth}%%%
%\DIFdel{^{(2)} }\DIFdelend \DIFaddbegin \DIFadd{|\tth'_{2}| }\DIFaddend \leq \gamma\DIFdelbegin \DIFdel{^{(2)}. }\DIFdelend \DIFaddbegin \DIFadd{_{2}, }\DIFaddend \label{eqn:FBbasic5}
%\end{equation}
%%DIF <  NOTE: probably nitpicky, but I find it better to use different symbols for non-equal things 
%%DIF <  (I know this isn't how the notation works in this field historically, though)  -Geoff
%\DIFaddbegin \DIFadd{at the cost of a slightly larger upper bound. 
%Note that both $|\tth_2|,|\tth'_2|$ are bounded above by $\gamma_2$.
%}\DIFaddend Typically, error bounds formed in the fashion of \DIFdelbegin \DIFdel{Equation~\ref{eqn:FBbasic5} }\DIFdelend \DIFaddbegin \DIFadd{\cref{eqn:FBbasic5} }\DIFaddend are converted to relative errors in order to put the error magnitudes in perspective.
%\DIFdelbegin \DIFdel{In our example , for nonzero $(x+y+z)$, we have:
%}\begin{displaymath}
%\DIFdel{\frac{|(x+y+z) - \fl(x+y+z)|}{|x+y+z|} \leq \gamma^{(2)}.
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdelend \DIFaddbegin \DIFadd{The relative error bound for our example is
%}\begin{equation*}
%\DIFadd{|(x+y+z) - \fl(x+y+z)|\leq \gamma_{2}|x+y+z|,\;\; x+y+z\neq 0.
%}\end{equation*}
%%DIF > when we assume $x+y+z\neq 0$.\par
%\DIFaddend 
%
%Although Lemma~\ref{lem:gamma} \DIFdelbegin \DIFdel{only }\DIFdelend requires $ku<1$, we actually need $ku <\frac{1}{2}$ \DIFdelbegin \DIFdel{, which implies $\gamma^{(k)} <1$, in order }\DIFdelend to maintain a meaningful relative error bound \DIFdelbegin \DIFdel{.
%While this assumption , $\gamma^{(k)} < 1$, is easily satisfied by fairly large $k$ in higher precision floating point numbers, it is a problem even for small $k$ in }\DIFdelend \DIFaddbegin \DIFadd{as this assumption implies $\gamma_k < 1$ and guarantees a relative error below 100\%. 
%Since higher precision types have smaller unit round-offs, they can tolerate more successive FLOPs than }\DIFaddend lower precision floating \DIFdelbegin \DIFdel{point numbers.
%Table \ref{table:ieeen} shows the maximum value of $k$ that still guarantees a relative error below $100\%$ ($\gamma^{(k)} < 1$).
%}%DIFDELCMD < \begin{table}[h]
%%DIFDELCMD < 	\centering
%%DIFDELCMD < 	\begin{tabular}{||c|c|c||} 
%%DIFDELCMD < 		\hline
%%DIFDELCMD < 		%%%
%\DIFdelFL{precision }%DIFDELCMD < &%%%
%\DIFdelFL{$\tilde{k} = \mathrm{argmax}^{(k)}(\gamma^{(k)} \leq 1)$ }%DIFDELCMD < \\ \hline
%%DIFDELCMD < 		%%%
%\DIFdelFL{half }%DIFDELCMD < & {\tt %%%
%\DIFdelFL{512}%DIFDELCMD < }\\
%%DIFDELCMD < 		%%%
%\DIFdelFL{single }%DIFDELCMD < & %%%
%\DIFdelFL{$\approx$ }%DIFDELCMD < {\tt %%%
%\DIFdelFL{4.194e06}%DIFDELCMD < } \\ 
%%DIFDELCMD < 		%%%
%\DIFdelFL{double }%DIFDELCMD < &  %%%
%\DIFdelFL{$\approx$ }%DIFDELCMD < {\tt %%%
%\DIFdelFL{2.252e15}%DIFDELCMD < }\\ \hline 
%%DIFDELCMD < 	\end{tabular}
%%DIFDELCMD < 	%%%
%%DIFDELCMD < \caption{%
%{%DIFAUXCMD
%\DIFdelFL{Upper limits of meaningful relative error bounds in the $\gamma^{(k)}$ notation.}}
%	%DIFAUXCMD
%%DIFDELCMD < \label{table:ieeen}
%%DIFDELCMD < \end{table}
%%DIFDELCMD < %%%
%\DIFdelend \DIFaddbegin \DIFadd{types before reaching $\gamma_m=1$.
%For example, the IEEE types introduced in \cref{table:ieee} meet this requirement at $1/2=2^{10}u^{(\text{fp16})}=2^{23}u^{(\text{fp32})}=2^{52}u^{(\text{fp64})}$.
%%DIF > Table \ref{table:ieeen} shows the maximum number of successive floating point operations that still guarantees a relative error below $100$\% for various floating point types. 
%%DIF > \begin{table}[h]
%%DIF > 	\centering
%%DIF > 	\begin{tabular}{||c|c|c|c||} 
%%DIF > 		\hline
%%DIF > 		precision & fp16 & fp32 & fp64 \\ \hline
%%DIF > 		$\arg\max_{k}(\gamma_k \leq 1)$ & {\tt 512} & $\approx$ {\tt 4.194e06} & $\approx$ {\tt 2.252e15} \\ \hline
%%DIF > 	\end{tabular}
%%DIF > 	\caption{Upper limits of meaningful relative error bounds in the $\gamma^{(k)}$ notation.}
%%DIF > 	\label{table:ieeen}
%%DIF > \end{table}
%%DIF > \vspace*{-10pt}
%}\DIFaddend Thus, accumulated rounding errors in lower precision types \DIFdelbegin \DIFdel{lead to }\DIFdelend \DIFaddbegin \DIFadd{can lead to an }\DIFaddend instability with fewer operations in comparison to higher precision types \DIFdelbegin \DIFdel{.
%As $k$ represents the number of FLOPs, this constraint restricts low-precision floating point operations to smaller problem sizes and lower complexity algorithms.
%%DIF < That such a small number of operations, $k=512$, leads to $\gamma_{\text{IEEE 754 half}}^{(k=512)} = 1$ 
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%%DIF < Thus, low-precision floats face problems within the rounding error analysis framework established in \cite{Higham2002} with smaller $k$-values than high-precision floats.
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%%DIF < That small values of $k$ lead to $\gamma^{(k)} = 1$ can be interpreted as:
%%DIF < \begin{itemize}
%%DIF < 	\item Accumulated rounding errors in lower precision types grow unstable very quickly and with fewer operations in comparison to higher precision types.
%%DIF < 	\item The upper bound given by $\gamma^{(k)}$ is less pessimistic in low precision than in high precision. 
%%DIF < 	Take the fact that 512 half precision operations and $2^{22}$ single precision operations both result in a 100\% relative error bound ($\gamma_{\text{half}}^{(512)} = \gamma_{\text{single}}^{(2^{22})} = 1$).
%%DIF < 	This error bound is at its tightest when every single one of the rounding errors admitted at each step were the largest-possible, worst-case scenarios.
%%DIF < 	Arguably, $512$ successive instances of the largest possible errors is more probable than $2^{22}$ successive instances of the largest possible errors. 
%%DIF < 	% This second point may not be necessary.
%%DIF < 	%TODO: If we keep this point, right after this itemize might be a good place to give brief summary of probablistic error.
%%DIF < \end{itemize}
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%%DIF < This reflects on two sources of difficulty: the larger round-off error 
%%DIF < 1) Accumulated rounding errors in lower precision types grow unstable with fewer operations, and 2) the upper bound given by $\gamma^{(k)}$ becomes pessimistic faster.% in low precision.
%%DIF < First, rounding errors admitted at each operation are much larger in low precision floating point arithmetic.
%%DIF < For example, the unit round-off value for half precision floats is almost four decimal orders of magnitude larger than that of single precision floats. 
%%DIF < This implies that it would take almost $10^4$ as few operations in half precision than in single precision to reach the same order of magnitude in accumulated rounding error. 
%%DIF < Second, the convenient error bound presented in Lemma~\ref{lem:gamma} gives the worst-case upper bound, which is given by assuming that the worst-case rounding occurred at each of the $k$ successive floating point operations. 
%%DIF < As $k$ grows larger, the probability that the largest possible rounding occurs at every operation also grows smaller, and therefore, $\gamma^{(k)}$ upper bound becomes more pessimistic.
%%DIFDELCMD < \par
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{To }\DIFdelend \DIFaddbegin \DIFadd{and prompts us to evaluate whether existing algorithms can be naively adapted for mixed precision arithmetic.
%%DIF > Thus, accumulated rounding errors in lower precision types can lead to an instability with fewer operations in comparison to higher precision types.
%%DIF > As $k$ represents the number of FLOPs, this constraint restricts low-precision floating point operations to smaller problem sizes and lower complexity algorithms.\par
%%DIF > TODO: mention in comment to reviewers 
%}\subsection{\DIFadd{Rounding Error Example for the Inner Product}}\label{sec:backgroundIP}
%\DIFadd{We now consider computing the inner product of two vectors to }\DIFaddend clearly illustrate how this situation restricts rounding error analysis in \DIFdelbegin \DIFdel{half precision, we now consider performing the dot product of two vectors. 
%A forward }\DIFdelend \DIFaddbegin \DIFadd{fp16. 
%An }\DIFaddend error bound for \DIFdelbegin \DIFdel{dot products is
%}\DIFdelend \DIFaddbegin \DIFadd{an inner product of $m$-length vectors is
%}\DIFaddend \begin{equation}
%\DIFdelbegin \DIFdel{\frac{|\bb{x}^{\top}\bb{y} - \fl(\bb{x}^{\top}\bb{y})|}{|\bb{x}|^{\top}|\bb{y}|} }\DIFdelend \DIFaddbegin \DIFadd{|\bb{x}^{\top}\bb{y} - \fl(\bb{x}^{\top}\bb{y})| }\DIFaddend \leq \gamma\DIFdelbegin \DIFdel{^{(m)}}\DIFdelend \DIFaddbegin \DIFadd{_{m} |\bb{x}|^{\top}|\bb{y}|}\DIFaddend , \quad \bb{x},\bb{y}\in\R^{m} \DIFdelbegin \DIFdel{,
%}\DIFdelend \label{eqn:DDerr}
%\end{equation}
%\DIFdelbegin \DIFdel{where details and proof for this statement can be found in Section 3.1 of \cite{Higham2002}.
%While this result does not guarantee a high relative accuracy when $|\bb{x}^{\top}\bb{y}| \ll |\bb{x}|^{\top}|\bb{y}|$, high relative accuracy is expected in some special cases.
%For example, let $\bb{x}=\bb{y}$.
%Then we have exactly $|\bb{x}^{\top}\bb{x}| = |\bb{x}|^{\top}|\bb{x}|=\|\bb{x}\|_2^2$, which leads to
%}\begin{displaymath}
%\DIFdel{\left|\frac{\|\bb{x}\|_2^2 - \fl(\|\bb{x}\|_2^2)}{\|\bb{x}\|_2^2}\right| \leq \gamma_p^{(d+2)}.
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdelend \DIFaddbegin \DIFadd{as shown in \cite{Higham2002}.
%%DIF > While this result does not guarantee a high relative accuracy when $|\bb{x}^{\top}\bb{y}| \ll |\bb{x}|^{\top}|\bb{y}|$, high relative accuracy is expected in some special cases.
%%DIF > For example, let $\bb{x}=\bb{y}$.
%%DIF > Then we have exactly $|\bb{x}^{\top}\bb{x}| = |\bb{x}|^{\top}|\bb{x}|=\|\bb{x}\|_2^2$, which leads to a forward error: $\left|\|\bb{x}\|_2^2 - \fl(\|\bb{x}\|_2^2)\right| \leq \gamma_m \|\bb{x}\|_2^2$.
%}\DIFaddend Since vectors of length $m$ accumulate rounding errors that are bounded by \DIFdelbegin \DIFdel{$\gamma^{(m)}$, the worst-case relative error bound for a dot product of vectors of length $512$ is already at }\DIFdelend \DIFaddbegin \DIFadd{$\gamma_{m}$, dot products of vectors computed in fp16 already face a }\DIFaddend 100\% \DIFdelbegin \DIFdel{($\gamma_{\text{half}}^{(512)}=1$)}\DIFdelend \DIFaddbegin \DIFadd{relative error bound when $m=1024$}\DIFaddend . \par
%
%\DIFdelbegin \DIFdel{We present a }\DIFdelend \DIFaddbegin \DIFadd{A }\DIFaddend simple numerical experiment \DIFdelbegin \DIFdel{that }\DIFdelend shows that the standard deterministic error bound is too pessimistic and cannot be practically used to approximate rounding error for half-precision arithmetic. 
%In this experiment, we generated 2 million random \DIFdelbegin \DIFdel{half-precision }\DIFdelend \DIFaddbegin \DIFadd{fp16 }\DIFaddend vectors of length \DIFdelbegin \DIFdel{$512$ }\DIFdelend \DIFaddbegin \DIFadd{$1024$ }\DIFaddend from two random distributions: the standard normal distribution, $N(0,1)$, and the uniform distribution over $(0,1)$.
%Half precision arithmetic was simulated by calling \DIFdelbegin \DIFdel{Algorithm~\ref{algo:simulate} for every multiplication and summation step required in calculating the dot product, $\fl(\bb{x}^{\top}\bb{y})$.
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%%DIF < Half precision arithmetic was simulated by: 1) casting all half precision floats up to single precision representation, 2) computing single precision operation, and 3) casting back down to half precision.
%%DIFDELCMD < 
%
%%DIFDELCMD < \begin{algorithm2e}[H]
%%DIFDELCMD < 	\DontPrintSemicolon %%%
%%DIF <  Some LaTeX compilers require you to use \dontprintsemicolon instead
%	%DIFDELCMD < \KwIn{$\bb{x}_{\text{half}}, \bb{y}_{\text{half}}\in\F_{\text{half}}^m$, $f:\R^{m}\times \R^m \rightarrow \R^n$}
%%DIFDELCMD < 	\KwOut{$\fl(f(\bb{x}_{\text{half}}, \bb{y}_{\text{half}}))\in\F_{\text{half}}^n$}
%%DIFDELCMD < 	%%%
%\DIFdel{$\bb{x}_{\text{single}}, \bb{y}_{\text{single}} \gets$ }%DIFDELCMD < {\tt %%%
%\DIFdel{castup}%DIFDELCMD < }%%%
%\DIFdel{$([\bb{x}_{\text{half}},\bb{y}_{\text{half}}])$}%DIFDELCMD < \\
%%DIFDELCMD < 	%%%
%\DIFdel{$\bb{z}_{\text{single}} \gets \fl(f(\bb{x}_{\text{single}}, \bb{y}_{\text{single}}))$}%DIFDELCMD < \\
%%DIFDELCMD < 	%%%
%\DIFdel{$\bb{z}_{\text{half}} \gets$ }%DIFDELCMD < {\tt %%%
%\DIFdel{castdown}%DIFDELCMD < }%%%
%\DIFdel{$(\bb{z}_{\text{single}})$}%DIFDELCMD < \\
%%DIFDELCMD < 	\Return %%%
%\DIFdel{$\bb{z}_{\text{half}}$}%DIFDELCMD < \\
%%DIFDELCMD < 	%%%
%%DIFDELCMD < \caption{%
%{%DIFAUXCMD
%\DIFdel{$\bb{z}_{\text{half}} = {\tt simHalf}(f, \bb{x}_{\text{half}}, \bb{y}_{\text{half}})$ Simulate function $f\in$ OP$\cup \{{\tt dot\_product} \}$ in half precision arithmetic given input variables $\bb{x},\bb{y}$. Function }%DIFDELCMD < {\tt %%%
%\DIFdel{castup}%DIFDELCMD < } %%%
%\DIFdel{converts half precision floats to single precision floats, and }%DIFDELCMD < {\tt %%%
%\DIFdel{castdown}%DIFDELCMD < } %%%
%\DIFdel{converts single precision floats to half precision floats by rounding to the nearest half precision float.}}
%	%DIFAUXCMD
%%DIFDELCMD < \label{algo:simulate}
%%DIFDELCMD < \end{algorithm2e}
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%%DIF < These 3 steps were executed at every multiplication and addition operation in calculating the dot product, $\fl(\bb{x}^{\top}\bb{y})$.
%\DIFdel{The casting up step is exact since all half precision numbers can be exactly represented in single precision, $\F_{\text{half}}\subset \F_{\text{single}}$; the second step incurs a rounding error from a single precision arithmetic operation; and the casting down step incurs a rounding error from casting down to half precision.
%Note that using Algorithm~\ref{algo:simulate} for any operation in OP results in simulating half precision arithmetic, whereas using it with }\DIFdelend \DIFaddbegin \DIFadd{\cref{algo:simulate}, which was proven to be a faithful simulation in \cite{HighamPranesh2019b}, for every FLOP (multiplication and addition for }\DIFaddend the dot product\DIFdelbegin \DIFdel{results in simulating mixed precision arithmetic instead.
%}\DIFdelend \DIFaddbegin \DIFadd{).
%%DIF > TODO: comment reviewer
%}\DIFaddend The relative error in this experiment is formulated as the \DIFdelbegin \DIFdel{left hand side of the inequality }\DIFdelend \DIFaddbegin \DIFadd{LHS }\DIFaddend in Equation \ref{eqn:DDerr} \DIFdelbegin \DIFdel{, where }\DIFdelend \DIFaddbegin \DIFadd{divided by $|\bb{x}|^{\top}|\bb{y}|$ and }\DIFaddend all operations outside of calculating $\fl(\bb{x}^{\top}\bb{y})$ are executed by casting up to \DIFdelbegin \DIFdel{double precision format and using double precision }\DIFdelend \DIFaddbegin \DIFadd{fp64 and using fp64 }\DIFaddend arithmetic.
%Table \ref{table:HPdoterr} shows \DIFaddbegin \DIFadd{some }\DIFaddend statistics from computing the relative error for simulated \DIFdelbegin \DIFdel{half precision dot productsof $512$-length random vectors.
%}\DIFdelend \DIFaddbegin \DIFadd{fp16 dot products.
%%DIF >  of random $1024$-length vectors. 
%}\DIFaddend \begin{table}[h]
%	\DIFaddbeginFL \vspace*{-0cm}
%	\DIFaddendFL \centering
%	\begin{tabular}{||c|c|c|c||} 
%		\hline
%		Random Distribution & Average & \DIFdelbeginFL %DIFDELCMD < \makecell{Standard\\deviation}%%%
%\DIFdelendFL \DIFaddbeginFL \makecell{Stan. Dev.}\DIFaddendFL & Maximum\\ \hline
%		Standard normal &{\tt \DIFdelbeginFL \DIFdelFL{1.627}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{1.621}\DIFaddendFL e-04} & {\tt \DIFdelbeginFL \DIFdelFL{1.640}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{1.635}\DIFaddendFL e-04 } & {\tt \DIFdelbeginFL \DIFdelFL{2.838}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{3.204}\DIFaddendFL e-03}\\ \hline
%		Uniform $(0,1)$ & {\tt \DIFdelbeginFL \DIFdelFL{2.599}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{6.904}\DIFaddendFL e-03}& {\tt \DIFdelbeginFL \DIFdelFL{1.854}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{3.265}\DIFaddendFL e-03} & {\tt \DIFdelbeginFL \DIFdelFL{1.399}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{2.447}\DIFaddendFL e-02}\\ \hline
%		%DIF > 		0.00016210232423560295 sn
%		%DIF > 		0.00016358278045005122 sn
%		%DIF > 		0.003204148981042573  sn
%		%DIF > 		0.006904074639006074 u
%		%DIF > 		0.0032653963135219504 u
%		%DIF > 		0.024475014917168688 u
%	\end{tabular}
%	%DIF > 	\begin{tabular}{||c|c|c|c||} 
%	%DIF > 		\hline
%	%DIF > 		Random Distribution & Average & \makecell{Stan. Dev.}& Maximum\\ \hline
%	%DIF > 		Standard normal &{\tt 1.627e-04} & {\tt 1.640e-04 } & {\tt 2.838e-03}\\ \hline
%	%DIF > 		Uniform $(0,1)$ & {\tt 2.599e-03}& {\tt 1.854e-03} & {\tt 1.399e-02}\\ \hline
%	%DIF > 	\end{tabular}
%	\caption{\DIFdelbeginFL \DIFdelFL{Statistics }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Forward error statistics }\DIFaddendFL from \DIFaddbeginFL \DIFaddFL{experiment of }\DIFaddendFL dot \DIFdelbeginFL \DIFdelFL{product backward relative error in for 512-length vectors stored in half-precision and }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{products }\DIFaddendFL computed in simulated \DIFdelbeginFL \DIFdelFL{half-precision from 2 million realizations}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{half precision}\DIFaddendFL .}
%	\label{table:HPdoterr}
%	%DIF > \vspace*{-1cm}
%\end{table}
%%DIF < TODO: why is the standard dev for the first row larger than average? data should be nonneg.
%\DIFaddbegin 
%
%\DIFaddend We see that the inner products of vectors sampled from the standard normal distribution have backward relative errors that do not deviate much from the unit round-off (\DIFaddbegin \DIFadd{$\cO$(}\DIFaddend {\tt \DIFdelbegin \DIFdel{4.883e-4}\DIFdelend \DIFaddbegin \DIFadd{1e-4}\DIFaddend })\DIFaddbegin \DIFadd{)}\DIFaddend , whereas the vectors sampled from the uniform distribution tend to accumulate larger errors \DIFaddbegin \DIFadd{on average ($\cO$(}{\tt \DIFadd{1e-3}}\DIFadd{))}\DIFaddend . 
%Even so, the theoretical upper error bound of 100\% is \DIFdelbegin \DIFdel{too pessimistic, and it is difficult to predict the kind of results this experimentshows.
%}\DIFdelend \DIFaddbegin \DIFadd{far too pessimistic as the maximum relative error does not even meet 2\% in this experiment.
%%DIF > TODO: comment reviewer
%}\DIFaddend Recent work in developing probabilistic bounds on rounding errors of floating point operations \DIFaddbegin \DIFadd{(see \cite{Higham2019a,Ipsen2019}) }\DIFaddend have shown that the inner product relative backward error for the conditions used for this experiment is bounded by {\tt 5.466e-2} with probability 0.99.
%\DIFdelbegin %DIFDELCMD < \par
%%DIFDELCMD < %%%
%%DIF < While the probabilistic error bound does get the correct order of magnitude for a maximal error (with probability 99\%), it is not enough to describe the probability distribution of inner product errors. \par
%%DIF < TODO: state that probablistic bounds have been introduced and why they have been.
%%DIF < TODO: cite Theo Mary/N Higham paper and briefly mention probabilistic bound? https://personalpages.manchester.ac.uk/staff/theo.mary/doc/ProbErrAna.pdf
%\DIFdelend 
%
%\DIFaddbegin \begin{algorithm2e}
%	\DontPrintSemicolon %DIF >  Some LaTeX compilers require you to use \dontprintsemicolon instead
%	\KwIn{$\bb{x}^{(\text{fp16})}$, $\bb{y}^{(\text{fp16})}$, $f$ \hfill\textbf{Output: } $\bb{z}^{(\text{fp16})}=\fl_{\text{fp16}}(f(\bb{x}^{(\text{fp16})}, \bb{y}^{(\text{fp16})}))$}
%	\DIFadd{$[\bb{x}^{(\text{fp32})}, \bb{y}^{(\text{fp32})}] \gets$ }{\tt \DIFadd{castup}}\DIFadd{$([\bb{x}^{(\text{fp16})},\bb{y}^{(\text{fp16})}])$}\tcp*{Convert input vars to fp32.}
%	\DIFadd{$\bb{z}^{(\text{fp32})} \gets \fl(f(\bb{x}^{(\text{fp32})}, \bb{y}^{(\text{fp32})}))$ }\tcp*{Perform fp32 arithmetic.}
%	\DIFadd{$\bb{z}^{(\text{fp16})} \gets$ }{\tt \DIFadd{castdown}}\DIFadd{$(\bb{z}^{(\text{fp32})})$}\tcp*{Convert result to fp16.}
%	\Return \DIFadd{$\bb{z}^{(\text{fp16})}$
%	}\caption{\DIFadd{$\bb{z}^{(\text{fp16})} = {\tt simHalf}(f, \bb{x}^{(\text{fp16})}, \bb{y}^{(\text{fp16})})$. Given fp16 input variables $\bb{x},\bb{y}$, perform function $f\in$ OP$\cup \{{\tt dot\_product} \}$ in simulated fp16 arithmetic. }}
%	%DIF > Function {\tt castup} converts fp16 to fp32, and {\tt castdown} converts fp32 to fp16 by rounding to the nearest half precision float.}
%	\label{algo:simulate}
%\end{algorithm2e}
%
%\DIFaddend Most importantly, \DIFdelbegin \DIFdel{no rounding error bounds (deterministic or probabilistic) allow flexibility in the precision types used for different operations. 
%This restriction is the biggest obstacle in gaining an understanding of rounding errors to expect from }\DIFdelend \DIFaddbegin \DIFadd{we need error analysis that allows flexibility in precision in order to better our understanding of the impact of rounding errors on }\DIFaddend computations done on emerging hardware \DIFdelbegin \DIFdel{that support mixed-precision such as GPUs}\DIFdelend \DIFaddbegin \DIFadd{(i.e. GPUs) that support mixed precision. 
%%DIF > Most importantly, no rounding error bounds (deterministic or probabilistic) allow flexibility in the precision types used for different operations. 
%%DIF > This restriction is the biggest obstacle in gaining an understanding of rounding errors to expect from computations done on emerging hardware that support mixed precision such as GPUs that employ mixed precision arithmetic.
%%DIF > In this paper, we extend the rounding error analysis framework established in \cite{Higham2002} to mixed precision arithmetic operations. 
%%DIF > TODO: consider rewriting above sentence to suit the revisions since we may have to emphasize our role in doing mp analysis for QR (the LU paper also does mixed preicion rounding error analysis.\cite{Blanchard2019}.)
%We start by introducing some additional rules from \cite{Higham2002} }\DIFaddend that \DIFdelbegin \DIFdel{employ mixed-precision arithmetic. 
%In this paper, we extend the rounding error analysis framework established in \cite{Higham2002} to mixed-precision arithmetic operations. 
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%%DIF < Nevertheless, the error analysis within the framework given by Lemma \ref{lem:gamma} best allows us to keep the analysis simple.
%%DIF < We will use it to study mixed-precision block QR factorization methods. \par 
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%%DIF < TODO: include statement about assumption that floats are not subnormal numbers.
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%%DIF <  DONE: replace $w$ with $w$that does not contain $s$ or $p$?
%\DIFdel{Lemma \ref{lem:up} shows rules from Lemma 3.3 in \cite{Higham2002} that }\DIFdelend \DIFaddbegin \DIFadd{build on \cref{lem:gamma} in \cref{lem:up}. 
%These rules }\DIFaddend summarize how to accumulate errors represented by $\tth$'s and $\gamma$'s \DIFdelbegin \DIFdel{.
%}\DIFdelend \DIFaddbegin \DIFadd{in a \emph{uniform precision} setting.
%%DIF > These relations aid in writing clear and simpler error analyses.
%%DIF > In \cref{lem:mp}, we present just a few rules adapted for multiple precisions from \cref{lem:up} that we will use repeatedly in future sections. 
%%DIF > Thus, \cref{lem:mp} allows low and high precision types and is a simple modification of \cref{lem:up}.
%%DIF > The rules for $\tth$ allows us to keep track of the two precision types separately and the rules we present for $\gamma$ were chosen to be useful for casting down to the lower of the two precisions, a pertinent procedure in our mixed precision analysis in the later sections. 
%}
%
%\DIFaddend \begin{lemma}
%	\label{lem:up}
%	For any positive integer $k$, let \DIFdelbegin \DIFdel{$\tth^{(k)}$ }\DIFdelend \DIFaddbegin \DIFadd{$\tth_{k}$ }\DIFaddend denote a quantity bounded according to \DIFdelbegin \DIFdel{$|\tth^{(k)}|\leq \frac{k u }{1-ku} =:\gamma^{(k)}$}\DIFdelend \DIFaddbegin \DIFadd{$|\tth_{k}|\leq \frac{k u }{1-ku} =:\gamma_{k}$}\DIFaddend . The following relations hold for positive integers \DIFdelbegin \DIFdel{$i$, $j$, }\DIFdelend \DIFaddbegin \DIFadd{$j,n$ }\DIFaddend and nonnegative integer $k$.
%	Arithmetic operations between \DIFdelbegin \DIFdel{$\tth^{(k)}$'s: 
%%DIF < %%	\begin{align*}
%}\DIFdelend \DIFaddbegin \DIFadd{bounded terms, $\tth_{k}$'s, are: 
%	}\DIFaddend \begin{equation}
%	(1+\tth\DIFdelbegin \DIFdel{^{(k)}}\DIFdelend \DIFaddbegin \DIFadd{_{k}}\DIFaddend )(1+\tth\DIFdelbegin \DIFdel{^{(j)}}\DIFdelend \DIFaddbegin \DIFadd{_{j}}\DIFaddend )%DIF < %%&
%=(1+\DIFdelbegin %DIFDELCMD < \tilde{\tth}%%%
%\DIFdel{^{(k+j)}}\DIFdelend \DIFaddbegin \DIFadd{\tth_{k+j}}\DIFaddend )%DIF < %%\\
%\qquad \mbox{and} \qquad\DIFdelbegin \DIFdel{\frac{1+\tth^{(k)}}{1+\tth^{(j)}} %DIF < %%&
%}\DIFdelend \DIFaddbegin \DIFadd{\frac{1+\tth_{k}}{1+\tth_{j}} }\DIFaddend =
%	\DIFdelbegin %DIFDELCMD < \begin{cases}
%%DIFDELCMD < 	1+\tth^{(k+j)},& j \leq k\\
%%DIFDELCMD < 	1+\tth^{(k+2j)},& j > k\\
%%DIFDELCMD < 	\end{cases} 
%%DIFDELCMD < %%%
%%DIF < %%	\end{align*}
%\DIFdelend \DIFaddbegin \begin{cases}
%	1+\tth_{k+j},& j \leq k\\
%	1+\tth_{k+2j},& j > k\\
%	\end{cases} \DIFadd{.
%	}\DIFaddend \end{equation}
%	\DIFdelbegin \DIFdel{Operations on }\DIFdelend \DIFaddbegin \DIFadd{If $\rm{max}_{(j,k)} u \leq \frac{1}{2}$ and $n \leq \frac{1}{uk}$, the operations on the bounds, }\DIFaddend $\gamma$'s\DIFdelbegin \DIFdel{: 
%	}\DIFdelend \DIFaddbegin \DIFadd{, are:
%	}\DIFaddend \begin{align*}
%	\gamma\DIFdelbegin \DIFdel{^{(k)}}\DIFdelend \DIFaddbegin \DIFadd{_{k}}\DIFaddend \gamma\DIFdelbegin \DIFdel{^{(j)} }%DIFDELCMD < &%%%
%\DIFdelend \DIFaddbegin \DIFadd{_{j} }\DIFaddend \leq \gamma_{\rm{min}(k,j)}\DIFaddbegin &\DIFaddend ,\DIFdelbegin %DIFDELCMD < \quad%%%
%\DIFdel{\text{for } }%DIFDELCMD < \rm{max}%%%
%\DIFdel{_{(j,k)} u \leq \frac{1}{2}, }%DIFDELCMD < \\
%%DIFDELCMD < 	%%%
%\DIFdelend \DIFaddbegin \qquad \DIFaddend n\gamma\DIFdelbegin \DIFdel{^{(k)} }%DIFDELCMD < &%%%
%\DIFdelend \DIFaddbegin \DIFadd{_{k} }\DIFaddend \leq \gamma\DIFdelbegin \DIFdel{^{(nk)}}\DIFdelend \DIFaddbegin \DIFadd{_{nk}}\DIFaddend ,\DIFdelbegin %DIFDELCMD < \quad %%%
%\DIFdel{\mbox{for} }%DIFDELCMD < \quad %%%
%\DIFdel{n \leq \frac{1}{uk},}\DIFdelend \\
%	\gamma\DIFdelbegin \DIFdel{^{(k)} }\DIFdelend \DIFaddbegin \DIFadd{_{k} }\DIFaddend + u \DIFdelbegin %DIFDELCMD < &%%%
%\DIFdelend \leq \gamma\DIFdelbegin \DIFdel{^{(k+1)}}\DIFdelend \DIFaddbegin \DIFadd{_{k+1}}&\DIFaddend ,\DIFdelbegin %DIFDELCMD < \\ 
%%DIFDELCMD < 	%%%
%\DIFdelend \DIFaddbegin \qquad \DIFaddend \gamma\DIFdelbegin \DIFdel{^{(k)}}\DIFdelend \DIFaddbegin \DIFadd{_{k}}\DIFaddend +\gamma\DIFdelbegin \DIFdel{^{(j)}}\DIFdelend \DIFaddbegin \DIFadd{_{j}}\DIFaddend +\gamma\DIFdelbegin \DIFdel{^{(k)}}\DIFdelend \DIFaddbegin \DIFadd{_{k}}\DIFaddend \gamma\DIFdelbegin \DIFdel{^{(j)} }%DIFDELCMD < & %%%
%\DIFdelend \DIFaddbegin \DIFadd{_{j} }\DIFaddend \leq \gamma\DIFdelbegin \DIFdel{^{(k+j)}}\DIFdelend \DIFaddbegin \DIFadd{_{k+j}}\DIFaddend .
%	\end{align*}
%	\DIFaddbegin \DIFadd{Note that all the rules hold when replaced by $\tilde{\gamma}$'s, but result in looser bounds.
%}\DIFaddend \end{lemma}
%%DIF <  TODO: make side by side to save space ?
%
%\DIFdelbegin \DIFdel{In Lemma~\ref{lem:mp}}\DIFdelend \DIFaddbegin \DIFadd{We define two mixed precision settings that we use in \cref{sec:mpanalysis}.
%In \cref{sec:mp-3}}\DIFaddend , we present \DIFdelbegin \DIFdel{modified versions of the rules in Lemma~\ref{lem:up}.
%This mixed-precision error analysis relies on the framework given by Lemma~\ref{lem:gamma}, which best allows us to keep a simple analysis. 
%These relations allow us to easily accumulate errors in terms of $\tth$'s }\DIFdelend \DIFaddbegin \DIFadd{the block Fused Multiply-Add (bFMA) of NVIDIA's TensorCore (TC) technology, which computes matrix-matrix multiply and accumulate for $4$-by-$4$ blocks, and incorporate it into \cref{algo:blockHQR,algo:par_tsqr}.
%Here, we introduce an ad hoc mixed precision setting (MP Setting) which we use in \cref{sec:mp-2}.
%This is explicitly defined in \cref{assump:mp} and is a level-2 BLAS variant of the TC bFMA. 
%%DIF > we use an  that is similar to TensorCore but works at the level of a dot product, which we introduce here.
%Both mixed precision settings define how inner products are computed although the bFMA is only applicable to inner products within matrix products and uses fp16 }\DIFaddend and \DIFdelbegin \DIFdel{$\gamma$'s and aid in writing clear and simpler error analyses. 
%The modifications support multiple precision types, whereas Lemma \ref{lem:up} assumes that }\DIFdelend \DIFaddbegin \DIFadd{fp32 whereas our 
%%DIF > the main difference lies in that the block FMA in TensorCore is for matrix-matrix products (level-3 BLAS) for fp16 and fp32, but our 
%ad hoc mixed precision setting is applicable to all inner products with any two precision types.}\par  
%%DIF > is a level-2 BLAS operation to work on any two different precision types.\par
%
%\DIFadd{Although our analysis concerns accuracy and stability and leaves out timing results of various hardwares, we add a general timing statement to \cref{assump:mp} that is analogous to that of TC: the mixed precision FMA inner product performs at least 2 times faster than the inner product in }\DIFaddend the \DIFdelbegin \DIFdel{same precision is used in all operations. 
%We distinguish between the different precisiontypes using subscripts--- these types include products ($p$), sums ($s$}\DIFdelend \DIFaddbegin \DIFadd{higher precision.
%Note that TCs perform matrix-matrix multiply and accumulate up to 8 times faster than fp32, and up to 16 times faster than fp64 (see }\DIFaddend ), and \DIFdelbegin \DIFdel{storage formats ($w$). 
%}%DIFDELCMD < 
%
%%DIFDELCMD < \begin{lemma}%%%
%%DIF < [Mixed precision version of Lemma 3.3 from \cite{Higham2002}]
%	%DIFDELCMD < \label{lem:mp}
%%DIFDELCMD < 	%%%
%\DIFdel{For any nonnegative integer $k$ and some precision $q$, let $\tth_{q}^{(k)}$ denote a quantity bounded according to $|\tth_q^{(k)}|\leq \frac{k u_q }{1-ku_q} =:\gamma_{q}^{(k)}$.  
%The following relations hold for two precisions $s$ and $p$, positive integers, $j_s$, $j_p$, non-negative integers $k_s$, and $k_p$, }\DIFdelend \DIFaddbegin \DIFadd{our ad hoc timing assumption is in conservative in comparison. 
%%DIF > TODO: cite
%Nonetheless, this gives a vague insight into the trade-offs between speediness and accuracy from some mixed precision computations.  
%%DIF > TODO: BLAS and FMA should have been introduced in the introduction section.
%%DIF > TODO: talk more about other FMA operations and cite properly.
%}
%
%\DIFadd{The full precision multiplication in Assumption~\ref{assump:mp} is exact when the low precision type is fp16 and the high precision type of fp32 due to their precisions }\DIFaddend and \DIFdelbegin \DIFdel{$c>0$:
%	%DIF < Most of these result from commutativity. 
%	}\begin{displaymath}
%	\DIFdel{(1+\tth_{p}^{(k_p)})(1+\tth_{p}^{(j_p)})(1+\tth_{s}^{(k_s)})(1+\tth_{s}^{(j_s)})=(1+\tth_{p}^{(k_p+j_p)})(1+\tth_{s}^{(k_s+j_s)}), }\\
%	\end{displaymath}
%	%DIFAUXCMD
%\begin{align*}
%	\DIFdel{\frac{(1+\tth_{p}^{(k_p)})(1+\tth_{s}^{(k_s)})}{(1+\tth_{p}^{(j_p)})(1+\tth_{s}^{(j_s)})} }&\DIFdel{=\left\{\begin{alignedat}{2}
%	(1+\tth_{s}^{(k_s+j_s)})(1+\tth_{p}^{(k_p+j_p)})}&\DIFdel{,\quad}& \DIFdel{j_s \leq k_s, j_p \leq k_p,}\\
%	\DIFdel{(1+\tth_{s}^{(k_s+2j_s)})(1+\tth_{p}^{(k_p+j_p)})}&\DIFdel{,\quad}& \DIFdel{j_s \leq k_s, j_p > k_p,}\\
%	\DIFdel{(1+\tth_{s}^{(k_s+j_s)})(1+\tth_{p}^{(k_p+2j_p)})}&\DIFdel{,\quad}& \DIFdel{j_s > k_s, j_p \leq k_p,}\\
%	\DIFdel{(1+\tth_{s}^{(k_s+2j_s)})(1+\tth_{p}^{(k_p+2j_p)})}&\DIFdel{,\quad}& \DIFdel{j_s > k_s, j_p > k_p.
%	\end{alignedat}\right.
%	}\end{align*}
%	%DIFAUXCMD
%\DIFdel{Without loss of generality, let $1 \gg u_p \gg u_s>0$.
%Let $d$, a nonnegative integer, }\DIFdelend \DIFaddbegin \DIFadd{exponent ranges. 
%As a quick proof, consider $x^{(\text{fp16})} = \pm\mu_x2^{\eta_x -11},y^{(\text{fp16})} = \pm\mu_y2^{\eta_y -11}$ where $\mu_x,\mu_y\in[0,2^{11}-1]$ }\DIFaddend and \DIFdelbegin \DIFdel{$r\in[0, \lfloor\frac{u_p}{u_s}\rfloor]$ be numbers that satisfy $k_su_s = d u_p + r u_s$. 
%	Alternatively, $d$ can be defined by $d := \lfloor\frac{k_su_s}{u_p}\rfloor$.
%	Then
%	}\begin{align*}
%	\DIFdel{\gamma_{s}^{(k_s)}\gamma_{p}^{(k_p)} }&\DIFdel{\leq \gamma_{p}^{(k_p)}, \quad\text{for } k_p u_p \leq \frac{1}{2}  }\\
%	\DIFdel{\gamma_{s}^{(k_s)}+u_p }&\DIFdel{\leq \gamma_{p}^{(d+2)} }\\
%	\DIFdel{\gamma_{p}^{(k_p)} + u_{s} }&\DIFdel{\leq \gamma_{p}^{(k_p+1)} }\\ %DIF < \quad{\color{blue}\text{(A loose bound)}}
%	\DIFdel{\gamma_{p}^{(k_p)}+\gamma_{s}^{(k_s)}+\gamma_{p}^{(k_p)}\gamma_{s}^{(k_s)} }& \DIFdel{< \gamma_{p}^{(k_p+ d+ 1)}. %DIFDELCMD < \label{lem:mp1}%%%
%	}\end{align*} 
%%DIFAUXCMD
%%DIFDELCMD < \end{lemma}
%%DIFDELCMD < %%%
%\DIFdel{A proof for Equation \ref{lem:mp1} is provided in Appendix \ref{appendix:A}. 
%We use these principles to establish a mixed-precision rounding error analysis for computing the dot product , which is crucial in many linear algebra routines such as the QR factorization.
%%DIF <  TODO: make side by side to save space ?
%%DIF <  TODO GEOFF and MINAH.   Meet and work out exact wording to fully formalize.   Should the wording be: there exists a \theta bounded by a \gamma (which is a specific value)?
%}\subsection{\DIFdel{Inner product Mixed-Precision error}}
%%DIFAUXCMD
%\addtocounter{subsection}{-1}%DIFAUXCMD
%%DIFDELCMD < \label{ssec:IP}
%%DIFDELCMD < %%%
%\DIFdel{We will see in Section~\ref{sec:HQRf} that the inner product is a building block of the HQR factorization (HQR)algorithm, which was introduced in \cite{Householder1958}.
%More generally,
%it is used widely in most linear algebra tools such as matrix-vector multiply and projections.
%Thus,
%we will generalize classic }\DIFdelend \DIFaddbegin \DIFadd{$\eta_x,\eta_y\in[-15,16]$, and note that the significand and exponent ranges for fp32 are $[0, 2^{24}-1]$ and $[-127,128]$.
%Then the product in full precision is
%}\[\DIFadd{\footnotesize x^{(\text{fp16})}y^{(\text{fp16})} = \pm\mu_x\mu_y 2^{\eta_x+\eta_y+2-24},}\]
%\DIFadd{where  $\mu_x\mu_y \in[0,(2^{11}-1)^2] \subseteq [0,2^{24}-1]$ and $\eta_x+\eta_y +2\in[-28,34]\subseteq[-127,128]$, and therefore is exact.
%Thus, the summation and the final cast down operations are the only sources of rounding error in this inner product scheme.
%}\begin{assump}
%	\label{assump:mp}
%	\DIFadd{Let $l$ and $h$ each denote low and high precision types with unit }\DIFaddend round-off \DIFdelbegin \DIFdel{error analysis of inner products to algorithms that may employ different precision types to different operations. 
%Specifically, we consider performing an inner product with the storage precision , $u_w$, being lower than the summation precision, $u_s$.
%This choice was made to provide a more accurate rounding error analysis of mixed precision floating point operations present in recent GPU technologies such as NVIDIA's TensorCore. 
%Currently, TensorCore computes the inner product of vectors stored in half-precision by employing full precision multiplications and a single-precision accumulator.
%%DIF < TODO: citation?
%As the majority of rounding errors from computing inner products occur during summation (see Section 3.1, \cite{Higham2002}), }\DIFdelend \DIFaddbegin \DIFadd{values $u^{(l)}$ and $u^{(h)}$, where $1 \gg u^{(l)} \gg u^{(h)} >0$.
%	Consider an FMA operation for inner products that take vectors stored in precision $l$, compute products in full precision, and sum the products in precision $h$. 
%	Finally, the result is then cast back down to precision $l$.
%	Furthermore, we expect this procedure to be approximately twice as fast as if it were done entirely in the higher precision, and about the same as if it were done entirely in the lower precision. 
%}\end{assump}
%%DIF > Regardless of the specific details of a mixed precision setting, a rounding error analysis for mixed precision arithmetic must support at least two different precision types. 
%\DIFadd{We now analyze the rounding error for }\DIFaddend the \DIFdelbegin \DIFdel{single precision accumulator immensely reduces the error in comparison to using only half-precision operations. 
%This increase in accuracy combined with its speedy performance motivates 1)to study how to best utilize mixed-precision arithmetic in algorithms and 2) to develop more accurate error analyses appropriate for mixed-precision algorithms. 
%%DIF < TODO: was precision actually defined before? -Aly ( I think this was take care off)
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{Lemma \ref{lem:ip_a} and Corollary \ref{lem:ip_b} present two mixed-precision forward error bounds for inner products, which show a tighter bound than the existing error bounds. 
%In both cases, we assume storage in the lowest precisionwith round-off value, $u_w$, and summation performed with a higher precision with round-off value, $u_s$, and let $d \approx m u_s / u_w$}\DIFdelend \DIFaddbegin \DIFadd{inner product scheme described in \cref{assump:mp} and hypothesize that the guaranteed accuracy for this mixed precision inner product should be better than that of the low precision inner product and worse than that of the high precision inner product.
%%DIF > innif it were computed entirely in the lower precision and worse than if it were computed entirely in the high precision.
%Let $\bb{x}^{(l)},\bb{y}^{(l)}$ be $m$-length vectors stored in a low precision type ($\F_l^m$)}\DIFaddend , %DIF <  be the ratio between $m\times u_s$ and $u_w$
%\DIFaddbegin \DIFadd{$s_k$ be the exact $k^{th}$ partial sum, and $\hat{s_k}$ be $s_k$ computed with FLOPs.
%Then the first three partial sums are,
%}\begin{align*}
%\DIFadd{\hat{s}_1 }&\DIFadd{= \fl (\bb{x}[1]\bb{y}[1]) = \bb{x}[1]\bb{y}[1],\quad \hat{s}_2 = \fl(\hat{s_1} + \bb{x}[2]\bb{y}[2]) = \left(\bb{x}[1]\bb{y}[1]+ \bb{x}[2]\bb{y}[2]\right)(1+\dd_{1}^{(h)}),}\\
%\DIFadd{\hat{s}_3 }&\DIFadd{= \fl(\hat{s_2}+\bb{x}[3]\bb{y}[3]) = \left[\left(\bb{x}[1]\bb{y}[1] + \bb{x}[2]\bb{y}[2]\right)(1+\dd_{1}^{(h)})  + \bb{x}[3]\bb{y}[3]\right](1+\dd_{2}^{(h)}).
%}\end{align*}
%\DIFadd{We see a pattern emerging. 
%The error for an $m$-length vector dot product is then
%}\begin{equation}
%\DIFadd{\label{eqn:dperr_2}
%\hat{s}_m = (\bb{x}[1]\bb{y}[1]+\bb{x}[2]\bb{y}[2])\prod_{k=1}^{m-1}(1+\dd_{k}^{(h)}) + \sum_{i=3}^n \bb{x}[i]\bb{y}[i]\left(\prod_{k=i-1}^{m-1}(1+\dd_{k}^{(h)})\right).
%}\end{equation}
%\DIFadd{Using Lemma \ref{lem:gamma}, we further simplify and form componentwise backward errors with
%}\begin{equation}
%\DIFadd{\fl(\bb{x}^{\top}\bb{y}) =(\bb{x}+\Delta\bb{x})^{\top}\bb{y} = \bb{x}^{\top}(\bb{y}+\Delta\bb{y})\quad\text{for }|\Delta \bb{x}| \leq \gamma^{(h)}_{m-1}|\bb{x}|,\;\; |\Delta \bb{y}|  \leq \gamma_{m-1}^{(h)}|\bb{y}|. \label{eqn:beforecd}
%}\end{equation}
%%DIF > Suppose that $|\fl(\bb{x}^{\top}\bb{y})|$ is smaller than the largest representable number in the lower precision.
%\DIFadd{Casting down to $\F_l$ without underflow or overflow results in backward errors, 
%}\begin{equation}
%\DIFadd{\text{\tt castdown}(\fl(\bb{x}^{\top}\bb{y})) = (\bb{x}+\Delta\bb{x}+\tilde{\Delta}\bb{x})^{\top}\bb{y} = \bb{x}^{\top}(\bb{y}+\Delta\bb{y}+\tilde{\Delta}\bb{y}), \label{eqn:aftercd}
%}\end{equation}
%\DIFaddend where \DIFaddbegin \DIFadd{$|\Delta\bb{x} + \tilde{\Delta} \bb{x}| \leq ((1+u^{(l)})(1+\gamma_{m-1}^{(h)})-1)|\bb{x}|$ and $|\Delta\bb{y}+\tilde{\Delta} \bb{y}| \leq ((1+u^{(l)})(1+\gamma_{m-1}^{(h)})-1)|\bb{y}|$.
%Our hypothesis is indeed true since,
%}\[\DIFadd{\gamma_m^{(h)}<u^{(l)}+\gamma_{m-1}^{(h)}+u^{(l)}\gamma_{m-1}^{(h)}<\gamma_{m}^{(l)},}\]
%\DIFadd{where the lower and upper bounds are derived from the uniform precision error bound in \cref{eqn:DDerr}. 
%\Cref{eqn:aftercd} shows us that the two larger error terms are from the higher precision summation, $\gamma_{m-1}^{(h)}$, and the cast down operation, $u^{(l)}$.
%We can measure the impact of the cast down step relative to the length of the vector, }\DIFaddend $m$\DIFaddbegin \DIFadd{, and the disparity in the two precisions, $M_{l,h}:=u^{(l)}/u^{(h)}$, since these two factors determine which one of $u^{(l)}$ and $mu^{(h)}$ }\DIFaddend is the \DIFdelbegin \DIFdel{length of }\DIFdelend \DIFaddbegin \DIFadd{leading order term. 
%We consider 3 cases.}\\
%%DIF > There are 3 cases to consider. \\
%\DIFadd{\textbf{Case 1: ($m\ll M_{l,h}$)} The leading order term is $u^{(l)}$.
%The mixed precision inner product has a smaller worst case error bound than the bound of the low precision inner product ($mu^{(l)}$) with no apparent improvements in speed.
%On the other hand, $u^{(l)}$ is a larger upper bound than that of the high precision inner product ($mu^{(h)}=\frac{m}{M_{l,h}}u^{(l)}$), although it was computed approximately twice as fast. 
%It is likely that this factor of $M_{l,h}/m$ increase in the worst case error bound is unwanted even when considering the speed-up. 
%%DIF > It is hard to determine that the speed-up in the FMA in \cref{assump:mp} exceeds that of some hardware with low precision arithmetic built in. 
%%DIF >  Thus, when comparing to the low precision inner product, the mixed precision inner product successfully reduces the error from $mu^{(l)})$ to $u^{(l)}$ with no apparent improvements in speed.
%}\\
%\DIFadd{\textbf{Case 2: ($m = M_{l,h}$)}
%Both terms are now leading order. 
%This is still an improvement in comparison to the lower precision arithmetic as the error bound is reduced from $mu^{(l)}$ to $2u^{(l)}$.
%Comparing this to the high precision inner product shows that the error bound has doubled from $mu^{(h)}$ to $2mu^{(h)}$, but gained a factor of 2 in speed instead. 
%One can argue that the loss in accuracy guarantee and the improvement in speed cancel each other out especially if $2mu^{(h)}\ll 1$ or if the speed-up greatly exceeds a factor of 2. }\\
%\DIFadd{\textbf{Case 3: ($m \gg M_{l,h}$)}
%Now $\gamma_{m-1}^{(h)}$ is the leading order term. 
%As in the above two cases, this is an improvement in the context of the low precision accuracy since the error has been reduced from $\gamma_m^{(l)}$ to $\gamma_{m/M_{l,h}}^{(l)}\equiv \gamma_m^{(h)}$. 
%Since $u^{(l)} = M_{l,h}u^{(h)} \ll mu^{(h)}$, the mixed precision error bound has the same \emph{order} as the error bound from carrying the computation out in the higher precision. 
%Therefore, we can expect about the same level of accuracy but a factor of 2 or greater reduction in speed when compared to the higher precision.}\par
%\DIFadd{While the above cases establish 3 regimes of trade-offs between accuracy and speed in mixed precision computing, }\DIFaddend the \DIFdelbegin \DIFdel{vectors.
%Although there are additional differing assumptions in these two lemmas,
%results from both show a strong dependence on $d$.
%%DIF < Both lemmas show a dependence on $d$
%}\DIFdelend \DIFaddbegin \DIFadd{remainder of this paper focuses only on accuracy and does not consider the impact of mixed precision computations on speed.
%%DIF > Readers should refer to timing studies such as .....
%%DIF > TODO: cite
%Finally, we present alternative representations of the error bound in \cref{eqn:aftercd},
%%DIF > This bound can be simplified using the following rules,
%}\begin{align*}
%\DIFadd{(1+u^{(l)})(1+\gamma_{m-1}^{(h)})-1 }&\DIFadd{\leq \gamma_{M_{l,h}+m-1}^{(h)}=\gamma_{1+(m-1)/M_{l,h}}^{(l)}, \;\; M_{l,h} = u^{(l)}/u^{(h)},}\\
%\DIFadd{(1+u^{(l)})(1+\gamma_{m-1}^{(h)})-1 }&\DIFadd{\leq  u^{(l)} + \gamma_{m-1}^{(h)} + \min\{u^{(l)}, \gamma_{m-1}^{(h)}\},\;\; \gamma_{m-1}^{(h)} < 1,
%}\end{align*}
%\DIFadd{where the rules from \cref{lem:up} were directly applied.
%Both alternative bounds are only slightly larger than the original bound shown on the LHS and remain in the same order.
%The first is useful when comparing against the low or the high precision, whereas the second keeps track of the error bounds in both precisions.
%%DIF > These bounds can be useful in different contexts.
%%DIF > Nevertheless, they both indicate that the error is th$\cO(u^{(l)} + mu^{(h)} + mu^{(l)}u^{(h)})$ and are slightly larger than the original bound, on the LHS.
%We summarize these ways of combining $\gamma$ terms of different precisions in \cref{lem:mp},
%%DIF >  which is a simple modification of some of the rules from \cref{lem:up}.
%}\DIFaddend \begin{lemma}\DIFdelbegin %DIFDELCMD < \label{lem:ip_a}
%%DIFDELCMD < 	%%%
%\DIFdel{Let $w$, $p$, and $s$ each represent floating point precisionsfor storage,
%product, and summation, where the varying precisions are defined by their }\DIFdelend \DIFaddbegin \label{lem:mp}
%	\DIFadd{For any nonnegative integers $k_l$, $k_h$ and some precision $q$ defined with respect to the }\DIFaddend unit round-off\DIFdelbegin \DIFdel{values denoted by $u_w$, $u_p$, }\DIFdelend \DIFaddbegin \DIFadd{, $u^{(q)}$, define $\gamma^{(q)}_{k} := \frac{k u^{(q)} }{1-ku^{(q)}}$.
%	%DIF > , let $\tth^{(q)}_{k}$ denote a quantity bounded according to $|\tth^{(q)}_{k}|\leq \frac{k u^{(q)} }{1-ku^{(q)}} =:\gamma^{(q)}_{k}$.
%	%DIF > 	The following relations hold for two precisions l (low) and h (high), positive integers, $j_{l}$,$j_{h}$, non-negative integers $k_{l}$, and $k_{h}$, and $c>0$:
%	%DIF > 	\begin{equation}
%	%DIF > 	(1+\tth^{(l)}_{k_{l}})(1+\tth^{(l)}_{j_{l}})(1+\tth^{(h)}_{k_{h}})(1+\tth^{(h)}_{j_{h}})=(1+\tth^{(l)}_{k_{l}+j_{l}})(1+\tth^{(h)}_{k_{h}+j_{h}}), \\
%	%DIF > 	\end{equation}
%	%DIF > 	and rules for the quotient $\frac{(1+\tth^{(l)}_{k_{l}})(1+\tth^{(h)}_{k_{h}})}{(1+\tth^{(l)}_{j_{l}})(1+\tth^{(h)}_{j_{h}})}$ follow similarly. 
%	%DIF > %	\begin{align}
%	%DIF > %	\frac{(1+\tth^{(l)}_{k_{l}})(1+\tth^{(h)}_{k_{h}})}{(1+\tth^{(l)}_{j_{l}})(1+\tth^{(h)}_{j_{h}})} &=\left\{\begin{alignedat}{2}
%	%DIF > %	(1+\tth^{(h)}_{k_{h}+j_{h}})(1+\tth^{(l)}_{k_{l}+j_{l}})&,\quad& j_{h} \leq k_{h}, j_{l} \leq k_{l},\\
%	%DIF > %	(1+\tth^{(h)}_{k_{h}+2j_{h}})(1+\tth^{(l)}_{k_{l}+j_{l}})&,\quad& j_{h} \leq k_{h}, j_{l} > k_{l},\\
%	%DIF > %	(1+\tth^{(h)}_{k_{h}+j_{h}})(1+\tth^{(l)}_{k_{l}+2j_{l}})&,\quad& j_{h} > k_{h}, j_{l} \leq k_{l},\\
%	%DIF > %	(1+\tth^{(h)}_{k_{h}+2j_{h}})(1+\tth^{(l)}_{k_{l}+2j_{l}})&,\quad& j_{h} > k_{h}, j_{l} > k_{l}.
%	%DIF > %	\end{alignedat}\right.
%	%DIF > %	\end{align}
%	%DIF > Without loss of generality, let $1 \gg u_{l} \gg u_{h}>0$.
%	Consider a low precision }\DIFaddend and \DIFdelbegin \DIFdel{$u_s$.
%	Let $\bb{x},\bb{y}\in \F_w^{m}$ be two arbitrary vectors stored in $w$ precision .
%	If an inner product performs multiplications in precision $p$ and addition of the products using precision $s$, then
%	}\begin{displaymath}
%	\DIFdel{\fl(\bb{x}^{\top}\bb{y}) = (\bb{x}+\bb{\Delta x}) \bb{y} = \bb{x}(\bb{y}+\bb{\Delta y}),
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{where $|\bb{\Delta x}|\leq \gamma_{p,s}^{(1,m-1)}|\bb{x}|$, $|\bb{\Delta y}|\leq \gamma_{p,s}^{(1,m-1)}|\bb{y}|$ componentwise, and }\begin{displaymath}\DIFdel{\gamma_{p,s}^{(1,m-1)} := (1+u_p)(1+\gamma_s^{(m-1)})-1.}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{This result is then stored in precision $w$, and, if we further assume that $u_w=u_p>u_s$, then $|\bb{\Delta x}|\leq \gamma_w^{(d+2)}|\bb{x}|$ }\DIFdelend \DIFaddbegin \DIFadd{a high precision where $1 \gg u^{(l)} \gg u^{(h)}>0$, }\DIFaddend and \DIFdelbegin \DIFdel{$|\bb{\Delta y}|\leq \gamma_w^{(d+2)}|\bb{y}|$, where $d:=\lfloor\frac{(m-1)u_s}{u_w}\rfloor$.
%	}\DIFdelend \DIFaddbegin \DIFadd{$k_l$, $k_h$ that satisfy $\max\{\gamma^{(h)}_{k_{h}},\gamma^{(l)}_{k_{l}}\} < 1/2$.
%	Then the following rules help us accumulate $\gamma$'s of different precisions,
%	%DIF > 	Let $d$, a nonnegative integer, and $r\in[0, \lfloor\frac{u_l}{u_h}\rfloor]$ be numbers that satisfy $k_{h}u_h = d u_l + r u_h$. 
%	%DIF > 	Alternatively, $d$ can be defined by $d := \lfloor\frac{k_{h}u_h}{u_l}\rfloor$.
%	%DIF > 	Then,
%	}\begin{align}
%	\DIFadd{\gamma^{(h)}_{k_{h}}\gamma^{(l)}_{k_{l}} }&\DIFadd{\leq \min\{\gamma^{(h)}_{k_{h}},\gamma^{(l)}_{k_{l}} \},}\\ 
%	\DIFadd{(1+\tilde{\gamma}_{k_l}^{(l)})(1+\tilde{\gamma}_{k_h}^{(h)}) -1 }&\DIFadd{= \tilde{\gamma}_{k_l}^{(l)}+\tilde{\gamma}_{k_h}^{(h)}. \label{eqn:mpgamma}
%	%DIF > 	\gamma^{(h)}_{k_{h}}+u^{(l)} &\leq \gamma^{(l)}_{d_1}, \\
%	%DIF > 	\gamma^{(l)}_{k_{l}}+\gamma^{(h)}_{k_{h}}+\gamma^{(l)}_{k_{l}}\gamma^{(h)}_{k_{h}} & < \gamma^{(l)}_{k_{l}+ d_2},
%	}\end{align} 
%	%DIF > 	where $d_1 = \lceil(u^{(l)}+k_hu^{(h)})/u^{(l)}\rceil$ and $d_2 = \lceil k_hu^{(h)}/u^{(l)}\rceil$, and 
%	%DIF > 	\begin{equation}
%	%DIF > 		(1+\tilde{\gamma}_{k_l}^{(l)})(1+\tilde{\gamma}_{k_h}^{(h)}) -1 = \tilde{\gamma}_{k_l}^{(l)}+\tilde{\gamma}_{k_h}^{(h)}. \label{eqn:mpgamma}
%	%DIF > 	\end{equation}
%\DIFaddend \end{lemma}
%\DIFaddbegin \DIFadd{Note that \cref{eqn:mpgamma} drops the term $\tilde{\gamma}_{k_l}^{(l)}\tilde{\gamma}_{k_h}^{(h)}$ since both $\tilde{\gamma}_{k_l}^{(l)}$ and $\tilde{\gamma}_{k_h}^{(h)}$ are larger than their product and this product can be swept under the small integer $c > 0$ assumption implicitly included in the tilde notation.
%%DIF > We show the proof for \cref{eqn:mp1}.
%%DIF > \begin{proof}
%%DIF > 	%We wish to round up to the lower precision, $p$, since $1\gg u^{(l)} \gg u^{(h)}$.  
%%DIF > 	%Recall that $d := \left\lfloor k_h u^{(h)}  / u^{(l)} \right\rfloor$ and $r \leq \left\lfloor u^{(l)}  / u^{(h)} \right\rfloor$,
%%DIF > 	Since
%%DIF > 	%%% \begin{equation*}
%%DIF > 	$ k_lu^{(l)}+k_hu^{(h)} = (k_l+d)u_l + r u_h \leq (k_l+d+1)u_l$, 
%%DIF > 	%%% \end{equation*}
%%DIF > 	\begin{align*}
%%DIF > 	\gamma^{(l)}_{k_l}+\gamma^{(h)}_{k_h}+\gamma^{(l)}_{k_l}\gamma^{(h)}_{k_h} 
%%DIF > 	&= \frac{k_lu^{(l)}}{1-k_lu^{(l)}} + \frac{k_hu^{(h)}}{1-k_hu^{(h)}} + \frac{k_lu^{(l)}}{1-k_lu^{(l)}}\frac{k_hu^{(h)}}{1-k_hu^{(h)}} \\
%%DIF > 	= \frac{k_lu^{(l)}+k_hu^{(h)}-k_lk_hu^{(l)}u^{(h)}}{1-(k_lu^{(l)}+k_hu^{(h)})+k_lk_hu^{(l)}u^{(h)}} %%% \\
%%DIF > 	&\leq \frac{(k_l+d+1)u^{(l)}-k_lk_hu^{(l)}u^{(h)}}{1-(k_l+d+1)u^{(l)}+k_lk_hu^{(l)}u^{(h)}} \\
%%DIF > 	&< \frac{(k_l+d+1)u^{(l)}}{1-(k_l+d+1)u^{(l)}} = \gamma^{(l)}_{k_l+d+1}
%%DIF > 	\end{align*}
%%DIF > \end{proof}
%%DIF > We use these principles to establish a mixed precision rounding error analysis for computing the dot product, which is crucial in many linear algebra routines such as the QR factorization.
%\Cref{eqn:beforecd,eqn:aftercd} are crucial for our analysis in \cref{sec:mpanalysis} since  
%%DIF > since they closely resemble the TensorCore FMA block which can return a matrix product in fp16 or fp32. 
%%DIF > Consider matrices $\bb{A}\in\F_{\text{fp16}}^{p\times m}$ and $\bb{B}\in\F_{\text{fp16}}^{m\times q}$, and $\bb{C}=\bb{A}\bb{B}\in\F_{\text{fp16}}^{p\times q}$.
%%DIF > If $\fl(\bb{C})$ is desired in fp16, then each component of that matrix incurs rounding errors as shown in \cref{eqn:aftercd} and if it is desired in fp32, the componentwise rounding error is given by \cref{eqn:beforecd}.
%%DIF > Similarly,
%the two mixed precision settings add }{\tt \DIFadd{castdown}} \DIFadd{operations at different parts of the HQR algorithms we consider.
%%DIF > We use error bounds similar to \cref{eqn:beforecd} up until a {\tt castdown} occurs, at which point we use error bounds in the fashion of \cref{eqn:aftercd}.
%%DIF > we could consider other mixed precision algorithms that cast down at various points within the algorithm instead after every matrix multiplication and still take advantage of the better storage properties of lower precision types. 
%In general, error bounds in the fashion of \cref{eqn:beforecd} can be used before the cast down operations and the action of the cast down is best represented by error bounds similar to \cref{eqn:aftercd}.}\par
%\DIFaddend 
%
%%DIF <  TODO $\|\|_2$ for norms, What if x and or why have some zeros, or very small values?
%%DIF <  TODO is it really componentwise or in the infinity norm?   
%\DIFdelbegin \DIFdel{Corollary \ref{lem:ip_b} presents another mixed-precision forward error bound for mixed-precision inner products with additional constraints.
%Here,
%	we assume that the vectors are being stored in a lower precision than the precision types being used for multiplications and additions.
%This scenario is similar to how TensorCore technology works in GPUs.}%DIFDELCMD < 
%
%%DIFDELCMD < \begin{corollary}
%%DIFDELCMD < 	\label{lem:ip_b}
%%DIFDELCMD < 	%%%
%\DIFdel{In addition to the assumptions in Lemma~\ref{lem:ip_a}, assume $1\gg u_w \gg u_s>0$, and thus for any two numbers $x,y$ }\DIFdelend \DIFaddbegin \DIFadd{We have demonstrated a need for rounding error analysis that is accurate for mixed precision procedures and analyzed the inner product }\DIFaddend in \DIFdelbegin \DIFdel{$\F_w$, their product $xy$ is in $\F_s$.
%	Let $\bb{x},\bb{y}\in \F_w^m$ be two arbitrary vectors stored in $w$ precision .
%	If an inner product performs multiplications in full precision }\DIFdelend \DIFaddbegin \DIFadd{an ad hoc mixed precision inner product that mimics the TensorCore bFMA.
%We will use this to analyze various Householder (HH) QR factorization algorithms.
%Algorithms and the general framework for the standard rounding error analysis for these algorithms are introduced in \cref{sec:algo}, }\DIFaddend and \DIFdelbegin \DIFdel{addition of the products using precision $s$, then
%	}\begin{displaymath}
%	\DIFdel{\fl(\bb{x}^{\top}\bb{y}) = (\bb{x}+\bb{\Delta x}) \bb{y} = \bb{x}(\bb{y}+\bb{\Delta y}),
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{where $|\Delta x|\leq \gamma_w^{(d+1)}|x|$, $|\Delta y|\leq \gamma_w^{(d+1)}|y|$ componentwise and $d:=\lfloor\frac{(m-1)u_s}{u_w}\rfloor$.
%}%DIFDELCMD < \end{corollary}
%%DIFDELCMD < %%%
%\DIFdelend \DIFaddbegin \DIFadd{both are modified to meet different mixed precision assumptions in \cref{sec:mpanalysis}.
%}\DIFaddend 
%
%
%\DIFdelbegin \DIFdel{Proofs for Lemma \ref{lem:ip_a} and Corollary \ref{lem:ip_b} are shown in Appendix \ref{appendix:A}. 
%The analyses for these two differ only in }\DIFdelend \DIFaddbegin \section{\DIFadd{Algorithms and existing round-off error analyses}}\label{sec:algo}
%%DIF > \input{algo}
%\DIFadd{aWe introduce the Householder QR factorization algorithm (HQR) in \cref{sec:HQR} and two block variants that use HQR within the block in \cref{sec:BQR,sec:TSQR}. 
%The blocked HQR (BQR) in \cref{sec:BQR} partitions the columns of }\DIFaddend the \DIFdelbegin \DIFdel{type of mixed-precision arithmetic performed within the inner product subroutine, and the difference is revealed to result in either $\gamma_w^{(d+1)}$ or $\gamma_w^{(d+2)}$.
%For the rest of this paper, we will refer to the forward error bound for the inner product as $\gamma_w^{d+z}$ for $z=1,2$ to generalize the analysis for varying assumptions.
%This simplification allows us to use the same analysis for the remaining steps of the HQR algorithm presented in the following sections.
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\section{\DIFdel{Mixed-Precision HQR Factorization}}
%%DIFAUXCMD
%\addtocounter{section}{-1}%DIFAUXCMD
%%DIFDELCMD < \label{sec:HQRf}
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdelend \DIFaddbegin \DIFadd{target matrix and is a well-known algorithm that uses the WY representation of \cite{Bischof1987} that utilizes mainly level-3 BLAS operations.
%%DIF > \cite{Schreiber1989}.
%In contrast, the Tall-and-Skinny QR (TSQR) in \cref{sec:TSQR} partitions the rows and takes a communication-avoiding divide-and-conquer approach that can be easily parallelized (see \cite{Demmel2007}).
%We present the standard rounding error analysis of these algorithms which will be tweaked for various mixed precision assumptions in \cref{sec:mpanalysis}.
%%DIF > \input{algo-HQR}
%}\subsection{\DIFadd{Householder QR (HQR)}}\label{sec:HQR}
%\DIFaddend The HQR algorithm uses %DIF < a special type of linear transformations called 
%\DIFdelbegin \DIFdel{Householder }\DIFdelend \DIFaddbegin \DIFadd{HH }\DIFaddend transformations to zero out elements below the diagonal of a matrix \DIFaddbegin \DIFadd{(see \cite{Householder1958})}\DIFaddend . 
%We present this %DIF < the Householder transformation in the context of 
%as zeroing out all but the first element of some vector, $\bb{x}\in\R^m$.
%\DIFdelbegin %DIFDELCMD < 
%
%%DIFDELCMD < %%%
%%DIF <  TODO: this part could be expository, not a lemma. -GEOFF
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdelend \begin{lemma}
%	Given vector $\bb{x}\in\R^{m}$, there exist \DIFdelbegin \DIFdel{Householder }\DIFdelend \DIFaddbegin \DIFadd{a HH }\DIFaddend vector , $\bb{v}$, and \DIFdelbegin \DIFdel{Householder }\DIFdelend \DIFaddbegin \DIFadd{a HH constant, $\beta$, that define the HH }\DIFaddend transformation matrix, \DIFdelbegin \DIFdel{$\bb{P}_{\bb{v}}$}\DIFdelend \DIFaddbegin \DIFadd{$\bb{P}_{\bb{v}}:=\bb{I}_{m} - \beta \bb{v}\bb{v}^{\top}$}\DIFaddend , such that $\bb{P}_{\bb{v}}$ zeros out $\bb{x}$ below the first element. 
%	\DIFdelbegin \begin{displaymath}
%	\DIFdel{\begin{alignedat}{3} 
%	\sigma =}& \DIFdel{-\rm{sign}(\bb{x}_1)\|\bb{x}\|_2, }&&\DIFdel{\quad  \bb{v} = \bb{x} -\sigma \hat{e_1},}\\
%	\DIFdel{\beta = }& \DIFdel{\frac{2}{\bb{v}^{\top}\bb{v}}=-\frac{1}{\sigma\bb{v}_1}, }&& \DIFdel{\quad \bb{P}_{\bb{v}}=  \bb{I}_{m} - \beta \bb{v}\bb{v}^{\top}.
%	\end{alignedat}
%	%DIFDELCMD < \label{eqn:HH} %%%
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{The }\DIFdelend \DIFaddbegin \DIFadd{The HH vector and constant are defined via
%	}\begin{equation}
%	\DIFadd{\sigma = -\rm{sign}(\bb{x}[1])\|\bb{x}\|_2, \quad  \bb{v} = \bb{x} -\sigma \hat{\bb{e}}_{1},\mbox{ and } \beta =\frac{2}{\bb{v}^{\top}\bb{v}}=-\frac{1}{\sigma\bb{v}[1]}.
%	\label{eqn:HH} 
%	\vspace*{-.3cm}
%	}\end{equation}
%	%DIF > Note that $\bb{P}_{\bb{v}}$ is symmetric and orthogonal, $\bb{P}_{\bb{v}}=\bb{P}_{\bb{v}}^{\top}=\bb{P}_{\bb{v}}^{-1}$.
%	%DIF > As a result, t
%	\DIFadd{The }\DIFaddend transformed vector, \DIFdelbegin \DIFdel{$\bb{P_vx}$}\DIFdelend \DIFaddbegin \DIFadd{$\bb{P_vx}=\sigma\hat{\bb{e}_1}$}\DIFaddend , has the same 2-norm as $\bb{x}$ since \DIFdelbegin \DIFdel{Householder transformations are orthogonal: $\bb{P}_{\bb{v}}\bb{x} = \sigma\hat{\bb{e}_1}$.
%	In addition, $\bb{P}_{\bb{v}}$ is symmetric and orthogonal, }\DIFdelend $\bb{P}_{\bb{v}}=\bb{P}_{\bb{v}}^{\top}=\bb{P}_{\bb{v}}^{-1}$\DIFdelbegin \DIFdel{, and therefore, $\bb{P}_{\bb{v}}^2=\bb{I}$}\DIFdelend .
%	\label{lem:hhvec}
%\end{lemma}
%\DIFdelbegin %DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\subsection{\DIFdel{HQR Factorization Algorithm}}
%%DIFAUXCMD
%\addtocounter{subsection}{-1}%DIFAUXCMD
%%DIFDELCMD < \label{sec:HQRfA}
%%DIFDELCMD < %%%
%\DIFdelend \DIFaddbegin \subsubsection{\DIFadd{HQR: Algorithm}}
%\DIFaddend Given $\bb{A}\in\R^{m\times n}$ and Lemma \ref{lem:hhvec}, HQR is done by repeating the following processes until only an upper triangle matrix remains.
%For $i = 1, 2, \cdots, n,$
%\begin{enumerate}[Step 1)]
%	\item Compute $\bb{v}$ and $\beta$ that zeros out the $i^{th}$ column of $\bb{A}$ beneath $a_{ii}$ \DIFaddbegin \DIFadd{(see \cref{algo:hh_v2})}\DIFaddend , and
%	\item Apply $\bb{P}_{\bb{v}}$ to the bottom right partition, $\bb{A}[i:m, i:n]$ \DIFaddbegin \DIFadd{(lines 4-6 of \cref{algo:hhQR})}\DIFaddend .
%\end{enumerate}
%%DIF < %% until only an upper triangular matrix remains. 
%
%Consider the following $4$-by-$3$ matrix example adapted from \cite{Higham2002}. 
%Let \DIFdelbegin \DIFdel{$\bb{P}_i$ }\DIFdelend \DIFaddbegin \DIFadd{$\bb{P}_{i}$ }\DIFaddend represent the $i^{th}$ \DIFdelbegin \DIFdel{Householder }\DIFdelend \DIFaddbegin \DIFadd{HH }\DIFaddend transformation of this algorithm. 
%\[\DIFaddbegin \footnotesize\DIFaddend \bb{A} = \left[ \begin{array}{ccc}
%\times & \times & \times \\
%\times & \times & \times \\
%\times & \times & \times \\
%\times & \times & \times
%\end{array}
%\right]\DIFdelbegin \DIFdel{\xrightarrow{\text{apply $\bb{P}_1$ to $\bb{A}^{(0)}:=\bb{A}$}}}\DIFdelend \DIFaddbegin \DIFadd{\xrightarrow{\bb{P}_{1}\bb{A}}}\DIFaddend \left[ \begin{array}{c|cc}
%\times & \times & \times \\ \hline
%0 & \times & \times \\
%0 & \times & \times \\
%0 & \times & \times
%\end{array}
%\right]
%\DIFdelbegin \DIFdel{\xrightarrow{\text{apply $\bb{P}_2$ to ($\bb{A}^{(1)}:=\bb{P}_1\bb{A}$)}}}\DIFdelend \DIFaddbegin \DIFadd{\xrightarrow{\bb{P}_{2}\bb{P}_{1}\bb{A}}}\left[
%\begin{array}{cc|c}
%\times & \times & \times \\
%0 & \times & \times \\ \hline
%0 & 0 & \times \\
%0 & 0 & \times 
%\end{array} \right]
%\DIFadd{\xrightarrow{\bb{P}_{3}\bb{P}_{2}\bb{P}_{1}\bb{A}} }\left[ \begin{array}{ccc}
%\times & \times & \times \\
%0 & \times & \times \\
%0 & 0 & \times \\
%0 & 0 & 0 
%\end{array}\right]\DIFaddend \]
%\DIFdelbegin \[ \DIFdel{\left[
%\begin{array}{cc|c}
%\times & \times & \times \\
%0 & \times & \times \\ \hline
%0 & 0 & \times \\
%0 & 0 & \times 
%\end{array} \right]
%\xrightarrow{\text{apply $\bb{P}_3$ to ($\bb{A}^{(2)}:=\bb{P}_2\bb{P}_1\bb{A}$)}} \left[ \begin{array}{ccc}
%\times & \times & \times \\
%0 & \times & \times \\
%0 & 0 & \times \\
%0 & 0 & 0 
%\end{array}\right] }\] 
%%DIFAUXCMD
%%DIF <  TODO: make fit on one line by writing A^(0)=[blah], P_1 A = [blah], P_2 P_1 A = [blah], etc ?? 
%\DIFdel{Since the final matrix $ \bb{P}_3\bb{P}_2\bb{P}_1\bb{A}$ is upper-triangular, this result is }\DIFdelend \DIFaddbegin \DIFadd{The resulting matrix is }\DIFaddend the $\bb{R}$ factor\DIFdelbegin \DIFdel{of the QR decomposition.
%Set $\bb{Q}^{\top}:=\bb{P}_3\bb{P}_2\bb{P}_1$. 
%Then, we can formulate  $\bb{Q}$ as
%}\begin{displaymath}
%\DIFdel{\bb{Q} = (\bb{P}_3\bb{P}_2\bb{P}_1)^{\top} = \bb{P}_1^{\top}\bb{P}_2^{\top}\bb{P}_3^{\top} = \bb{P}_1\bb{P}_2\bb{P}_3,
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdel{where the last equality results from the symmetric property of $\bb{P}_i$'s. 
%%DIF < In addition, this is orthogonal because $\bb{Q}^{\top}=\bb{P}_3\bb{P}_2\bb{P}_1 =  \bb{P}_3^{\top}\bb{P}_2^{\top}\bb{P}_1^{\top} =  \bb{P}_3^{-1}\bb{P}_2^{-1}\bb{P}_1^{-1}=(\bb{P}_1\bb{P}_2\bb{P}_3)^{-1}=\bb{Q}^{-1}$, where the third equality results from the orthogonal property of $\bb{P}_i$'s.
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{Returning to the general case, we have
%}\begin{displaymath}
%\DIFdel{\bb{Q}_{\text{full}} = \bb{P}_1 \cdots \bb{P}_n\quad \text{and} \quad \bb{R}_{\text{full}} = \bb{Q}^{\top}\bb{A} = \bb{P}_n\cdots \bb{P}_1\bb{A},
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdel{for the orthogonal factor in }\DIFdelend \DIFaddbegin \DIFadd{, $\bb{R}:= \bb{P}_{3}\bb{P}_{2}\bb{P}_{1}\bb{A}$, and the $\bb{Q}$ factor for }\DIFaddend a full QR factorization \DIFdelbegin \DIFdel{, and
%}\begin{displaymath}
%\DIFdel{\bb{Q}_{\text{thin}} = \bb{P}_1 \cdots \bb{P}_n\bb{I}_{m\times n}\quad \text{and} \quad \bb{R}_{\text{thin}} = \bb{I}_{m\times n}^{\top}\bb{Q}^{\top}\bb{A} = \bb{I}_{m\times n}^{\top}\bb{P}_n\cdots \bb{P}_1\bb{A}.
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdelend \DIFaddbegin \DIFadd{is $\bb{Q}:=\bb{P}_{1}\bb{P}_{2}\bb{P}_{3}$ since $\bb{P}_{i}$'s are symmetric.
%The thin factors for a general matrix $\bb{A}\in\R^{m\times n}$ are
%}\begin{equation}
%\DIFadd{\bb{Q}_{\text{thin}} = \bb{P}_{1} \cdots \bb{P}_{n}\bb{I}_{m\times n}\quad \text{and} \quad \bb{R}_{\text{thin}} = \bb{I}_{m\times n}^{\top}\bb{P}_{n}\cdots \bb{P}_{1}\bb{A}.
%}\end{equation}
%\DIFaddend 
%
%\DIFdelbegin \subsubsection{\DIFdel{HQR Factorization Implementation}}
%%DIFAUXCMD
%\addtocounter{subsubsection}{-1}%DIFAUXCMD
%%DIFDELCMD < \label{sssec:HQRfI}
%%DIFDELCMD < %%%
%\DIFdel{The Householder transformation is implemented by a series of inner and outer products, since Householder matrices are rank-1 updates of the identity.
%This approach is much less costly than forming $\bb{P}_{\bb{v}}$, and then performing matrix-vector or matrix-matrix multiplications.
%For some $\bb{P}_{\bb{v}}=\bb{I}-\beta \bb{v}\bb{v}^{\top}$, we result in the following computation:
%}\begin{displaymath}
%\DIFdel{%DIFDELCMD < \label{eqn:hqrIO}%%%
%\bb{P}_{\bb{v}} \bb{x} = (\bb{I}-\beta \bb{v}\bb{v}^{\top})\bb{x} = \bb{x} - (\beta \bb{v}^{\top}\bb{x})\bb{v}.
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdel{The routine in Equation \ref{eqn:hqrIO} is used in forming $\bb{R}$  and $\bb{Q}$. 
%Given a vector $\bb{x}\in\R^{m}$, Algorithm \ref{algo:hh_v2} calculates the Householder constant, $\beta$, and Householder vector, $\bb{v}$, that zero out $\bb{x}$ below the first element and also returns $\sigma$. 
%Algorithm \ref{algo:hhQR} is the HQR algorithm where information necessary to build $\bb{Q}$ is returned instead of explicitly forming $\bb{Q}$; the Householder vector and constant at the $k^{th}$ step are stored as the $k^{th}$ column of matrix $\bb{V}\in\R^{m\times n}$ and the $k^{th}$ element of vector $\bm{\beta}\in\R^n$. 
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{Finally, the $\bb{Q}$ factor can be built using Algorithm \ref{algo:hh_mult}.
%While this algorithm shows how to left multiply $\bb{Q}$ to any input matrix $\bb{B}$ given $\bb{V}$ and $\bm{\beta}$, putting in $\bb{B}\equiv I_{m\times n}$ will yield $\bb{Q}_{\text{thin}}$.
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdelend \begin{algorithm2e}[H]
%	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
%	\DIFdelbegin %DIFDELCMD < \KwIn{$\bb{x}\in\R^m$}
%%DIFDELCMD < 	\KwOut{$\bb{v}\in\R^m$, and $\sigma, \beta\in\R$ such that $(I-\beta \bb{v}\bb{v}^{\top})\bb{x} = \pm \|\bb{x}\|_2 \hat{e_1} = \sigma\hat{e_1}$ }
%%DIFDELCMD < 	\tcc{We choose the sign of sigma to avoid cancellation of $\bb{x}_1$ (As is the standard in LAPACK, LINPACK packages \cite{Higham2002}). This makes $\beta>0$.}
%%DIFDELCMD < 	%%%
%\DIFdel{$\bb{v}\gets \bb{x}$}\DIFdelend \DIFaddbegin \KwIn{$\bb{x}$ \hfill \textbf{Output: }$\bb{v}$, $\sigma$, and $\beta $}
%	%DIF >  such that $(I-\beta \bb{v}\bb{v}^{\top})\bb{x} = \pm \|\bb{x}\|_2 \hat{\bb{e}}_{1} = \sigma\hat{\bb{e}}_{1}$}
%	%DIF > \tcc{We choose the sign of sigma to avoid cancellation of $\bb{x}[1]$ (As is the standard in LAPACK, LINPACK packages \cite{Higham2002}). This makes $\beta>0$.}
%	\DIFadd{$\bb{v}\gets$ }{\tt \DIFadd{copy}}\DIFadd{($\bb{x}$)}\DIFaddend \\
%	\DIFdelbegin \DIFdel{$\sigma \gets -\rm{sign}(\bb{x}_1)\|\bb{x}\|_2$}\DIFdelend \DIFaddbegin \DIFadd{$\sigma \gets -\rm{sign}(\bb{x}[1])\|\bb{x}\|_2$}\DIFaddend \\
%	\DIFdelbegin \DIFdel{$\bb{v}_1 \gets \bb{x}_1-\sigma$ }%DIFDELCMD < \tcp*{This is referred to as $\bb{\tilde{v}}_1$ later on.} 
%%DIFDELCMD < 	%%%
%\DIFdel{$\beta \gets -\frac{\bb{v}_1}{\sigma}$}\DIFdelend \DIFaddbegin \DIFadd{$\bb{v}[1] \gets \bb{x}[1]-\sigma$ }\DIFaddend \\
%	\DIFdelbegin \DIFdel{$\bb{v} \gets \frac{1}{\bb{v}_1}\bb{v}$}\DIFdelend %DIF > \tcp*{This is referred to as $\bb{\tilde{v}}_1$ later on.} 
%	\DIFaddbegin \DIFadd{$\beta \gets -\frac{\bb{v}[1]}{\sigma}$}\DIFaddend \\
%	\DIFaddbegin \DIFadd{${\bf v} \leftarrow {\bf v} / {\bf v}[1]$}\\
%	%DIF > $\bb{v} \gets \frac{1}{\bb{v}[1]}\bb{v}$\\
%	\DIFaddend \Return $\beta$, $\bb{v}$, $\sigma$
%	\caption{$\beta$, $\bb{v}$, \DIFdelbegin \DIFdel{$\sigma = {\tt hh\_vec}(\bb{x})$}\DIFdelend \DIFaddbegin \DIFadd{$\sigma = {\tt hhvec}(\bb{x})$}\DIFaddend . Given a vector \DIFdelbegin \DIFdel{$\bb{x}\in\R^n$}\DIFdelend \DIFaddbegin \DIFadd{$\bb{x}\in\R^m$}\DIFaddend , return \DIFdelbegin \DIFdel{the Householder vector, $\bb{v}$; a Householder constant, $\beta$; }\DIFdelend \DIFaddbegin \DIFadd{$\bb{v}\in\R^m$ }\DIFaddend and \DIFdelbegin \DIFdel{$\sigma$ such }\DIFdelend \DIFaddbegin \DIFadd{$\beta,\sigma\in\R$ }\DIFaddend that \DIFdelbegin \DIFdel{$(I-\beta \bb{v}\bb{v}^{\top})\bb{x} =\sigma(\hat{e_1})$ }\DIFdelend \DIFaddbegin \DIFadd{satisfy $(I-\beta \bb{v}\bb{v}^{\top})\bb{x} =\sigma\hat{\bb{e}}_{1}$ }\DIFaddend and \DIFdelbegin \DIFdel{$\bb{v}_1=1$, }\DIFdelend \DIFaddbegin \DIFadd{$\bb{v}[1]=1$ }\DIFaddend (see \cite{LAPACK, Higham2002}).}
%	\label{algo:hh_v2}
%\end{algorithm2e}
%
%\begin{algorithm2e}
%	\DontPrintSemicolon
%	%DIF <  Some LaTeX compilers require you to use \dontprintsemicolon instead
%	\DIFdelbegin %DIFDELCMD < \KwIn{$A\in\R^{m \times n}$ where $m \geq n$.}
%%DIFDELCMD < 	
%
%%DIFDELCMD < 	\KwOut{$\bb{V}$,$\bm{\beta}$, $\bb{R}$}
%%DIFDELCMD < %%%
%\DIFdelend \DIFaddbegin \KwIn{$\bb{A}$ \hfill \textbf{Output: }$\bb{V}$,$\bm{\beta}$, $\bb{R}$}
%	\DIFaddend %	\tcc{$\bb{v}_i = V[i:m, i] \in \R^{m-(i-1)}$ and $\bb{B}_i = \bb{B}[i:m, i:d] \in \R^{(m-(i-1))\times(d-(i-1))}$.}
%	\DIFdelbegin \DIFdel{$\bb{V}, \bm{\beta} \gets \bb{0}_{m\times n}, \bb{0}_m$ }\DIFdelend \DIFaddbegin \DIFadd{Initialize $\bb{V} \gets \bb{0}_{m\times n}$, $\bm{\beta} \gets \bb{0}_m$ }\DIFaddend \\
%
%	\DIFdelbegin %DIFDELCMD < \For{$i=1 : n$}{
%%DIFDELCMD < 		$\bb{v}, \beta, \sigma \gets \mathrm{hh\_vec}(\bb{A}[i:\mathrm{end}, i])$\\	
%%DIFDELCMD < 		$\bb{V}[i:\mathrm{end},i]$, $\bm{\beta}_i$,  $\bb{A}[i,i] \gets \bb{v}, \beta, \sigma$\tcp*{Stores the Householder vectors and constants.}
%%DIFDELCMD < 		\tcc{The next two steps update $\bb{A}$.}
%%DIFDELCMD < 		$\bb{A}[i+1:\mathrm{end}, i]\gets \mathrm{zeros}(m-i)$\\
%%DIFDELCMD < 		$\bb{A}[i:\mathrm{end}, i+1:\mathrm{end}]\gets \bb{A}[i:\mathrm{end}, i+1:\mathrm{end}] - \beta \bb{v} \bb{v}^{\top}\bb{A}[i:\mathrm{end}, i+1:\mathrm{end}]$
%%DIFDELCMD < 		
%
%%DIFDELCMD < 	}
%%DIFDELCMD < 	%%%
%\DIFdelend \DIFaddbegin \For{$i=1 : n$}{
%		$\bb{v}, \beta, \sigma \gets$ {\tt hhvec}$(\bb{A}[i:\mathrm{end}, i])$ \tcc*{\Cref{algo:hh_v2}}
%		$\bb{V}[i:\mathrm{end},i]$, $\bm{\beta}_i$,  $\bb{A}[i,i] \gets \bb{v}, \beta, \sigma$\\
%		%\tcp*{Stores the HH vectors and constants.}
%		%\tcc{The next two steps update $\bb{A}$.}
%		$\bb{A}[i+1:\mathrm{end}, i]\gets \mathrm{zeros}(m-i)$\\
%		$\bb{A}[i:\mathrm{end}, i+1:\mathrm{end}]\gets \bb{A}[i:\mathrm{end}, i+1:\mathrm{end}] - \beta \bb{v} \bb{v}^{\top}\bb{A}[i:\mathrm{end}, i+1:\mathrm{end}]$
%
%	}
%	\DIFaddend \Return $\bb{V}$, $\bm{\beta}$, $\bb{A}[1:n, 1:n]$
%	%DIF < 	\caption{$\bb{V}$, $\bm{\beta}$, $\bb{R}$ = ${\tt qr}(A)$. Given a matrix $A\in\R^{m\times n}$ where $m\geq n$, return matrix $\bb{V}\in\R^{m\times n}$, vector $\bm{\beta}\in\R^{n}$, and upper triangular matrix $\bb{R}$. An orthogonal matrix $\bb{Q}$ can be generated from $\bb{V}$ and $\bm{\beta}$, and $\bb{QR}=\bb{A}$.}
%	\DIFdelbegin %DIFDELCMD < 
%
%%DIFDELCMD < 	%%%
%\DIFdelend \DIFaddbegin \caption{\DIFadd{$\bb{V}$, $\bm{\beta}$, $\bb{R} =$ }{\tt \DIFadd{HQR2}}\DIFadd{$(\bb{A})$. A Level-2 BLAS implementation of HQR. Given a matrix $\bb{A}\in\R^{m\times n}$ where $m\geq n$, return matrix $\bb{V}\in\R^{m\times n}$, vector $\bm{\beta}\in\R^{n}$, and upper triangular matrix $\bb{R}$. The orthogonal factor $\bb{Q}$ can be generated from $\bb{V}$ and $\bm{\beta}$.}}
%	\DIFaddend \label{algo:hhQR}
%\end{algorithm2e}
%\DIFdelbegin %DIFDELCMD < 
%
%%DIFDELCMD < 	\begin{algorithm2e}
%%DIFDELCMD < 		\DontPrintSemicolon %%%
%%DIF <  Some LaTeX compilers require you to use \dontprintsemicolon instead
%		%DIFDELCMD < \KwIn{$\bb{V}\in\R^{m \times n}$, $\bm{\beta}\in\R^{n}$ where $m \geq n$. $\bb{B} \in\R^{m\times d}$.  }
%%DIFDELCMD < 		
%
%%DIFDELCMD < 		\KwOut{$\bb{Q}\bb{B}$}
%%DIFDELCMD < 		\tcc{$\bb{v}_i = V[i:m, i] \in \R^{m-(i-1)}$ and $\bb{B}_i = \bb{B}[i:\mathrm{end}, i:\mathrm{end}] \in \R^{(m-(i-1))\times(d-(i-1))}$.}
%%DIFDELCMD < 		\For{$i=1 : n$}{
%%DIFDELCMD < 			$\bb{B}_i \gets \bb{B}_i - \bm{\beta}_i \bb{v}_i(\bb{v}_i^{\top}\bb{B}_i)$}
%%DIFDELCMD < 		\Return %%%
%\DIFdel{$\bb{B}$
%		}%DIFDELCMD < \caption{%
%{%DIFAUXCMD
%\DIFdel{$\bb{Q}\bb{B}\gets {\tt hh\_mult}(V, \bb{B})$: Given a set of householder vectors $\{\bb{v}_i\}_{i=1}^n$ and their corresponding constants $\{\bm{\beta}_i\}_{i=1}^n$, compute $\bb{P}_1\cdots \bb{P}_n\bb{B}$, where $\bb{P}_i := \bb{I} - \bm{\beta}_i\bb{v}_i\bb{v}_i^{\top}$}}
%		%DIFAUXCMD
%%DIFDELCMD < \label{algo:hh_mult}
%%DIFDELCMD < 	\end{algorithm2e}
%%DIFDELCMD < 
%
%%DIFDELCMD < 		
%%DIFDELCMD < %%%
%\DIFdelend \subsubsection{\DIFdelbegin \DIFdel{Normalization of Householder Vectors}\DIFdelend \DIFaddbegin \DIFadd{HQR: Rounding Error Analysis}\DIFaddend }
%\DIFdelbegin %DIFDELCMD < \label{sssec:NormalizeHV}
%%DIFDELCMD < %%%
%\DIFdel{Equation \ref{eqn:HH} gives a single Householder transformation matrix $\bb{P}_{\bb{v}'}$ for all $\bb{v}'$ in $\mathrm{Span}(\bb{v})$, which allows for many different ways of normalizing the Householder vectors as well as the choice of not normalizing them.
%However, this equivalence ($\bb{P}_{\bb{v}}\equiv \bb{P}_{\bb{v'}}$ for all $\bb{v}' \in \mathrm{Span}(\bb{v})$)  is not guaranteed due to rounding errors when using floating point numbers and operations.
%When using high precision floating point numbers such as double-precision floats, rounding errors that accumulate from the normalization of Householder vectors rarely and barely contribute to }\DIFdelend \DIFaddbegin \DIFadd{Now we present an error analysis for \cref{algo:hhQR} by keeping track of }\DIFaddend the \DIFdelbegin \DIFdel{overall stability of the HQR algorithm performed.
%In contrast, lower precision floating point numbers with limited dynamic range may be more sensitive to the un/normalization choice.
%For example, if we leave the Householder vectors unnormalized while using half-precision, it is possible to accumulate $\tt{Inf}$'s in inner products of ``large'' vectors.
%As a result, picking a normalization scheme for $\bb{v}$ is important in low-precision calculations.
%Some methods and reasons for the normalization of $\bb{v}$ are as follows:
%}%DIFDELCMD < 
%
%%DIFDELCMD < \begin{itemize}
%%DIFDELCMD < 	\item %%%
%\DIFdel{Set the first element of $\bb{v}$,  $\bb{v}_1$, as $1$ for efficient storage of many Householder vectors,
%	}%DIFDELCMD < \item %%%
%\DIFdel{Set the 2-norm of $\bb{v}$ to $\sqrt{2}$ to always have $\beta=1$, or
%	}%DIFDELCMD < \item %%%
%\DIFdel{Set }\DIFdelend \DIFaddbegin \DIFadd{different operations of \cref{algo:hh_v2} and \cref{algo:hhQR}.
%We follow the analysis of \cite{Higham2002} and modify it for the variant where $\bb{v}[1]$ is set to $1$.
%The goal of this section is to present the basic steps of the standard error analysis for HQR so that we modify them easily in \cref{sec:mpanalysis} for different mixed precision settings.
%}\paragraph{\DIFadd{Calculating the $i^{th}$ HH vector and constant}} 
%\DIFadd{In \cref{algo:hhQR}, we compute the HH vector and constant by using \cref{algo:hh_v2} to $\bb{A}[i:m,i]$.
%For now, consider zeroing out any vector $\bb{x}\in\R^m$ below its first component with a HH transformation.
%%DIF > $i^{th}$ HH vector shares all but the first component with the target column, . 
%We first calculate $\sigma$ as is implemented in line 2 of \cref{algo:hh_v2}.
%}\begin{equation}
%\DIFadd{\label{eqn:sigma}
%\fl(\sigma) = \hat{\sigma} = \rm{fl}(-\rm{sign}(\bb{x}[1])\|\bb{x}\|_2) = \sigma + \Delta \sigma,\quad |\Delta\sigma| \leq \gamma_{m+1}|\sigma|.
%}\end{equation}
%\DIFadd{Note that the backward error incurred here accounts for an inner product of a vector in $\R^{m}$ with itself and a square root operation to get }\DIFaddend the 2-norm\DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{. 
%Let $\bb{v}'[1]\equiv \bb{x}[i]-\sigma$, the penultimate value $\bb{v}[1]$ held. 
%The subtraction adds a single additional rounding error via
%}\begin{equation}
%\DIFadd{\fl(\bb{v}'}[\DIFadd{1}]\DIFadd{) =\bb{v}'}[\DIFadd{1}] \DIFadd{+ \Delta \bb{v}'}[\DIFadd{1}] \DIFadd{= (1+\dd) (\bb{x}[i]-\sigma-\Delta\sigma)= (1+\tth_{m+2})\bb{v}'}[\DIFadd{1}]
%\end{equation}
%\DIFadd{where the last equality is granted because the sign of $\sigma$ is chosen to prevent cancellation.  
%%DIF > TODO: comment reviewer
%%DIF > For the sake of simplicity, we write $|\Delta \bb{\tilde{v}}[1] |\leq \tilde{\gamma}_{m-i+1}|\bb{\tilde{v}}[1]|$ even though a tighter relative upper bound is $\tth_{m-i+2}$.
%%DIF > We sweep that minor difference (in comparison to $\cO(m-i)$) under the our use of the $\tilde{\gamma}$ notation defined in \cref{lem:gamma}.
%Since \cref{algo:hh_v2} normalizes the HH vector so that its first component is $1$, the remaining components of }\DIFaddend $\bb{v}$ \DIFdelbegin \DIFdel{to $1$ to prevent extremely large values, and to always have $\beta=2$. 
%}%DIFDELCMD < \end{itemize}
%%DIFDELCMD < %%%
%\DIFdel{LINPACK and its successor LAPACK are benchmark software libraries for performing numerical linear algebra \cite{LAPACK}. 
%The LAPACK implementation of the HQR factorization uses  the first method of normalizing via setting $\bb{v}_1$ to $1$ and is shown in Algorithm \ref{algo:hh_v2}.  
%%DIF < http://www.netlib.org/lapack/explore-html/df/dc5/group__variants_g_ecomputational_ga3766ea903391b5cf9008132f7440ec7b.html
%The first normalizing method adds an extra rounding error to $\beta$ and $\bb{v}$ each, whereas the remaining methods incur no rounding errorin forming $\beta$, since $1$ and $2$ can be represented exactly.
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%%DIF < The error analysis in the subsequent section assumes that there may exist errors in both $\beta$ and $\bb{v}$ to get the worse-case scenario and to be consistent with the LINPACK implementation. 
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%%DIF <  where machine precision is approximately $10^{-3}$ and the largest number is $65,504$, a careful selection of the normalization may be necessary to acquire higher stability in the HQR algorithm.
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\subsection{\DIFdel{Rounding Error Analysis}}
%%DIFAUXCMD
%\addtocounter{subsection}{-1}%DIFAUXCMD
%%DIFDELCMD < \label{sec:HQRre}
%%DIFDELCMD < %%%
%\DIFdel{We present an error analysis for the HQR factorization where all inner products are performed with mixed-precision, and all other calculations are done in the storage precision, $w$.
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{Assumption~\ref{assump:mp} lays out the generalized mixed-precision inner product we will be using over and over again in the remainder of this paper.
%}%DIFDELCMD < 
%
%%DIFDELCMD < \begin{assump}
%%DIFDELCMD < 	%DIFDELCMD < \label{assump:mp}%%%
%%DIFDELCMD < 	%%%
%\DIFdel{Let $w$, $p$, and $s$ each represent floating point precisions for storage, product, and summation, where the varying precisions are defined by their unit round-off values denoted by $u_w$, $u_p$, and $u_s$, and we can assume $1\gg u_w \gg u_s$ and $u_p\in (0, u_w]$. 
%	Within the inner product subroutine, products are done in precision $p$, summation is done in precision $s$, and the result stored in precision $w$.
%All operations other than inner products are done in the storage precision, $w$}\DIFdelend \DIFaddbegin \DIFadd{are divided by $\fl(\tilde{\bb{v}}_1)$ incurring another single rounding error}\DIFaddend .
%\DIFdelbegin %DIFDELCMD < \end{assump}
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\subsubsection{\DIFdel{Error analysis for forming Householder Vector and Constant}}
%%DIFAUXCMD
%\addtocounter{subsubsection}{-1}%DIFAUXCMD
%\DIFdel{Calculating the Householder vector and constant is a major routine for the HQR factorization. 
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\paragraph{\DIFdel{Error analysis for $\bb{v}$}}
%%DIFAUXCMD
%\addtocounter{paragraph}{-1}%DIFAUXCMD
%\DIFdel{In this section, we show how to bound the error when employing the mixed precision dot product procedure for Algorithm \ref{algo:hh_v2}.
%We begin by extending the inner-product error shown in Lemmas~\ref{lem:ip_a} and \ref{lem:ip_b} to the 2-norm error . 
%}%DIFDELCMD < \par
%%DIFDELCMD < 
%
%%DIFDELCMD < \begin{lemma}[2-norm round-off error]
%%DIFDELCMD < 	\label{lem:2norm_a}
%%DIFDELCMD < 	%%%
%\DIFdel{Consider a mixed-precision scheme as is outlined in Assumption~\ref{assump:mp}.
%	Let $\bb{x}\in \F_w^{m}$ be an arbitrary $n$-length vector stored in $w$ precision.
%	The forward error bound for computing the 2-norm of }\DIFdelend \DIFaddbegin \DIFadd{As a result, the components of }\DIFaddend $\bb{v}$ \DIFdelbegin \DIFdel{is
%	}\begin{displaymath}
%	\DIFdel{\fl(\|\bb{x}\|_2)= (1+\tth_w^{(d+z+1)})\|\bb{x}\|_2,
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdelend \DIFaddbegin \DIFadd{computed with FLOPs have error $\fl(\bb{v}[j])	= \bb{v}[j] + \Delta \bb{v}[j]$ }\DIFaddend where 
%\DIFdelbegin \DIFdel{$|\tth_w^{(d+z+1)}|\leq \gamma_w^{(d+z+1)}|\bb{x}|$ for $z\in\{1,2\}$ and $d:=\lfloor\frac{(m-1)u_s}{u_w}\rfloor$.
%}%DIFDELCMD < \end{lemma} 
%%DIFDELCMD < %%%
%\DIFdel{There is no error incurred in evaluating the sign of a number or flipping the sign.
%Therefore, }\DIFdelend \DIFaddbegin \begin{equation}
%\DIFadd{|\Delta \bb{v}[j]|\leq \gamma_{1+2(m+2)}|\bb{v}[j]| =\tilde{\gamma}_{m}|\bb{v}[j]|\quad j=2:m-i+1,\label{eqn:vbound}
%}\end{equation}
%\DIFadd{and $|\Delta {\bf v}[1]| = 0$.
%Since $1+2(m+2) += \cO(m)$, we have swept that minor difference between under our use of }\DIFaddend the \DIFdelbegin \DIFdel{error bound for computing $\sigma = -\rm{sign}(\bb{x}_1)\|\bb{x}\|_2$ is exactly the same as that for the 2-norm,i.
%e.
%,
%}\begin{displaymath}
%\DIFdel{%DIFDELCMD < \label{eqn:sigma}%%%
%\fl(\sigma) = \hat{\sigma} = \rm{fl}(-\rm{sign}(\bb{x}_1)\|\bb{x}\|_2) = \sigma + \Delta \sigma,\quad |\Delta\sigma| \leq \gamma_w^{(d+z+1)}|\sigma|.
%}\end{displaymath}
%%DIFAUXCMD
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{Let $\bb{\tilde{v}}_1$ be the penultimate value $\bb{v}_1$ held ($\bb{\tilde{v}}_1 = \bb{x}_1-\sigma$).
%We can now show the round-off error for $\bb{\tilde{v}}_1$ and }\DIFdelend \DIFaddbegin \DIFadd{$\tilde{\gamma}$ notation defined in \cref{lem:gamma}.
%Next, we consider the HH constant, $\beta$, as is computed in line 4 of \cref{algo:hh_v2}.
%}\begin{align}
%\DIFadd{\hat{\beta} = \fl\left(-\bb{v}'}[\DIFadd{1}]\DIFadd{/\hat{\sigma}\right) }&\DIFadd{=-(1+\dd)\frac{\bb{v}'[1]+\Delta \bb{v}'[1]}{\sigma + \Delta\sigma} = \frac{(1+\dd)(1+\tth_{m+2})}{(1+\tth_{m+1})}\beta \label{eqn:beta}}\\
%&\DIFadd{= (1+\tth_{3m+5})\beta= \beta + \Delta \beta,\text{ where } |\Delta\beta| \leq \tilde{\gamma}_{m} \beta\label{eqn:beta3}.
%}\end{align}
%\DIFadd{We have shown \cref{eqn:beta} to keep our analysis simple in \cref{sec:mpanalysis} and \cref{eqn:beta3} show that the error incurred from calculating of $\|\bb{x}\|_2$ accounts for the vast majority of the rounding error so far.
%At iteration $i$, we replace $\bb{x}$ with $\bb{A}[i:m,i]\in\R^{m-i+1}$ and the $i^{th}$ HH constant and vector ($\hat{\beta}_i$,}\DIFaddend $\bb{v}_i$\DIFdelbegin \DIFdel{, where $i=2 , \cdots, n$. 
%Then the round-off errors for $\bb{\tilde{v}}_1$ and $\bb{v}_i$'s are
%}\begin{align*}
%\DIFdel{\fl(\bb{v}_1)}&\DIFdel{=\hat{\bb{v}_1} = \bb{\tilde{v}}_1 + \bb{\Delta \tilde{v}}_1, }\\
%&\DIFdel{= \fl(\bb{x}_1-\hat{\sigma})= (1+\dd_w) (\sigma + \Delta\sigma) = (1+\tth_w^{(d+z+2)})\bb{\tilde{v}}_1,
%}\end{align*}
%%DIFAUXCMD
%\DIFdel{and
%}\begin{displaymath}
%\DIFdel{\fl(\bb{v}_i)=\hat{\bb{v}_i} = \fl\left(\frac{\bb{x}_i}{\hat{\bb{v}_1}}\right) = (1+\dd_w)\frac{\bb{x}_i}{\bb{\tilde{v}}_1 + \bb{\Delta \tilde{v}}_1}=(1+\theta_w^{(1+2(d+z+2))})\bb{\tilde{v}}_i.
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdel{The above equalities (as opposed to inequalities) are permitted since $\tth$ values are allowed to be flexible within the corresponding $\gamma$ bounds.
%%DIF < , and we summarize the above results for the whole vector in Lemma~\ref{lem:HQRv}.
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\paragraph{\DIFdel{Error analysis for $\beta$}}
%%DIFAUXCMD
%\addtocounter{paragraph}{-1}%DIFAUXCMD
%\DIFdel{Now we show the derivation of round-off error }\DIFdelend \DIFaddbegin \DIFadd{) both have errors bounded by $\tilde{\gamma}_{m-i+1}$.
%}\paragraph{\DIFadd{Applying a Single HH Transformation}}
%\DIFadd{Now we consider lines 4-6 of \cref{algo:hhQR}.
%At iteration $i$, we set $\bb{A}[i+1:m,:]$ to zero and replace $\bb{A}[i,i]$ with $\sigma$ computed from \cref{algo:hh_v2}.
%Therefore, we now need to calculate the errors }\DIFaddend for \DIFaddbegin \DIFadd{applying a HH transformation to }\DIFaddend the \DIFdelbegin \DIFdel{Householder constant, $\beta$:
%}\begin{align*}
%\DIFdel{\hat{\beta} = \fl\left(-\frac{\hat{\bb{v}_1}}{\hat{\sigma}}\right) }&\DIFdel{=-(1+\dd_w)\frac{\bb{\tilde{v}}_1+\bb{\Delta \tilde{v}}_1}{(\sigma + \Delta\sigma)} %DIF < %%\\
%%DIF < %%&
%= -(1+\tth_w^{(1)})\frac{ (1+\tth_w^{(d+z+2)})\bb{v}_1}{(1+\tth_w^{(d+z+1)})\sigma} %DIF < %%\\
%%DIF < %%&
%= (1+\tth_w^{(d+z+3+2(d+z+1))})\beta,}\\
%&\DIFdel{= (1+\tth_w^{(3d+3z+5)})\beta,
%}\end{align*}
%%DIFAUXCMD
%\DIFdel{where $z=1$ or $z=2$, depending on which mixed-precision inner product procedure was used. 
%These two results are formalized in Lemma~\ref{lem:HQRv} below.
%}%DIFDELCMD < \begin{lemma}
%%DIFDELCMD < 	%DIFDELCMD < \label{lem:HQRv}%%%
%%DIFDELCMD < 	%%%
%\DIFdel{Given $\bb{x}\in\R^{m}$, consider the constructions of $\beta\in\R$ and $\bb{v}\in\R^{m}$ such that $\bb{P}_{\bb{v}}\bb{x}=\sigma\hat{e_1}$ (see Lemma~\ref{lem:hhvec}) by using Algorithm~\ref{algo:hh_v2}.
%	Then the forward error of forming $\bb{v}$ and $\beta$ with the floating point arithmetic with the
%	mixed-precision scheme outlined in Assumption~\ref{assump:mp} are
%	}\begin{displaymath}
%	\DIFdel{\|\hat{\bb{v}}\|_2 = (1+\theta_w^{(1+2(d+z+2))})\|\bb{v}\|_2 \qquad \mbox{and} \qquad
%	\hat{\beta} = (1+\tth_w^{(3d+3z+5)})\beta,
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{where $z\in\{1,2\}$ and $d=\lfloor\frac{(m-1)u_s}{u_w}\rfloor$.
%}%DIFDELCMD < \end{lemma}
%%DIFDELCMD < %%%
%%DIF < TODO: when \beta is 1 or \sqrt(2), is there any error for beta?   If not, comment regarding this...   -Geoff
%%DIF < \sqrt{2} is NOT exact, but 1 is. -Minah
%%DIF < TODO: present theorem from Higham, and make comparison at the very end of this section. 
%%DIF < \paragraph{Comparison to uniform precision analysis:}
%%DIF < In this paper, uniform precision refers to using the same precision for all floating point operations. 
%%DIF < We compare the errors for $\hat{\beta}$ and $\hat{\bb{v}}$ computed via the mixed-precision inner products to the errors computed while everything was done in half-precision. 
%%DIF < 
%%DIF < To simplify error analyses even further, we now introduce the $\tilde{\gamma}$ notation in Equation \ref{eqn:tildegamma}, which is introduced in Section 19.3 of \cite{Higham2002}.
%%DIF < This notation allows us to keep track of only the leading order floating point operations. 
%%DIF < For example, in the HQR factorization routine, computation of dot products is the most costly subroutine, as the number of FLOPs linearly depends on the number of rows, $m$, in the original matrix.
%%DIF < It is sufficient 
%%DIF < Without mixed-precision, the errors would be bounded by
%%DIF < \begin{equation}
%%DIF < \tilde{\gamma}^{(k)} := \frac{cku}{1-cku},
%%DIF < \label{eqn:tildegamma}
%%DIF < \end{equation}
%%DIF < and $c$ is a small integer (c.f. Section 19.3 \cite{Higham2002}).
%%DIF < %This new $\tilde{\gamma}$ notation is introduced in \cite{Higham2002} to further simplify error analyses by caring only about leading-order floating point operations. 
%%DIF < Let us further assume that the storage precision ($u_{w}$) in the mixed-precision analysis is half-precision. 
%%DIF < In other words, we can let $u\equiv u_w$, and directly compare $\tilde{\gamma_w}^{(m)}$ and $\gamma_w^{(3d+3z+5)}$.
%%DIF < The integer $d$ depends on the length of the vector, $m$ and the precisions ($u_w$ and $u_s$), and likely is a small integer.
%%DIF < For example, if storage is done in half-precision, and summation within the inner product is done in single-precision, then $d :=\lfloor\frac{m-1}{8192}\rfloor$.
%%DIF < Since both $d$ and $z$ are usually small integers, the errors for $\hat{\beta}$ and $\hat{\bb{v}}$ with mixed-precision arithmetic can be approximated by $\gamma_w^{(3d+3z+5)} \approx \tilde{\gamma_w}^{(d+z+1)}$.
%%DIF < This is an improvement from $\tilde{\gamma_w}^{(m)}$ as \begin{displaymath}m \gg \lfloor\frac{m-1}{8192}\rfloor + z + 1.\end{displaymath}
%\DIFdelend \DIFaddbegin \DIFadd{remaining columns, $\bb{A}[i:m, i+1:n]$ with the computed HH vector and constant.
%This is the most crucial building block of the rounding error analysis for any variant of HQR because the $\bb{R}$ factor is formed by applying the HH transformations to $\bb{A}$ and the $\bb{Q}$ factor is formed by applying them in reverse order to the identity.
%Both of the blocked versions in \cref{sec:BQR} and \cref{sec:TSQR} also require slightly different but efficient implementations of this step.
%%DIF > , although they may be implemented slightly differently.
%For example, BQR in \cref{algo:blockHQR} uses level-3 BLAS operations to apply multiple HH transformations at once whereas the variant of HQR in \cref{algo:hhQR} can only use level-2 BLAS operations to apply HH transformations.}\par
%\DIFaddend 
%
%\DIFdelbegin \subsubsection{\DIFdel{Applying a Single Householder Transformation}}
%%DIFAUXCMD
%\addtocounter{subsubsection}{-1}%DIFAUXCMD
%\DIFdel{A Householder }\DIFdelend \DIFaddbegin \DIFadd{A HH }\DIFaddend transformation is applied through a series of inner and outer products, since \DIFdelbegin \DIFdel{Householder }\DIFdelend \DIFaddbegin \DIFadd{HH }\DIFaddend matrices are rank-1 updates of the identity. 
%\DIFdelbegin %DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{For some $\bb{P}_{\bb{v}}=I-\beta \bb{v}\bb{v}^{\top}$, we result in $\bb{P}_{\bb{v}} \bb{x} = (I-\beta \bb{v}\bb{v}^{\top})\bb{x} = \bb{x} - (\beta \bb{v}^{\top}\bb{x})\bb{v}$.
%}\paragraph{\DIFdel{Applying $\bb{P}_{\bb{v}}$ to zero out the target column of a matrix}}
%%DIFAUXCMD
%\addtocounter{paragraph}{-1}%DIFAUXCMD
%\DIFdel{Let $\bb{x}\in\R^{m}$ be the target column we wish to zero out beneath the first element.
%Recall that we chose a specific $\bb{v}$ such that $\bb{P}_{\bb{v}}\bb{x} = \sigma \hat{e}_1$. 
%As a result, the only error lies in the first element, $\sigma$, and that isshown in Equation \ref{eqn:sigma}.
%Note that the normalization choice of $\bb{v}$ does not impact the Householder transformation matrix ($\bb{P}_{\bb{v}}$) nor its action on $\bb{x}$, }\DIFdelend \DIFaddbegin \DIFadd{That is, computing  }\DIFaddend $\bb{P}_{\bb{v}}\bb{x}$ \DIFdelbegin \DIFdel{.
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\paragraph{\DIFdel{Applying $\bb{P}_{\bb{v}}$ to the remaining columns of the matrix}}
%%DIFAUXCMD
%\addtocounter{paragraph}{-1}%DIFAUXCMD
%\DIFdel{Now, let $\bb{x}$ and $\bb{v}$ have no special relationship, as $\bb{v}$ was constructed given some preceding column.
%Set $\bb{w}:= \beta \bb{v}^{\top}\bb{x}\bb{v}$.
%Note }\DIFdelend \DIFaddbegin \DIFadd{for any $\bb{x}\in\R^{m}$ is as simple as computing 
%}\begin{equation}
%\DIFadd{\bb{y}:=\bb{P}_{\bb{v}}\bb{x} = \bb{x} - (\beta \bb{v}^{\top}\bb{x})\bb{v}.\label{eqn:effH}
%}\end{equation}
%\DIFadd{Let us assume }\DIFaddend that $\bb{x}$ is \DIFdelbegin \DIFdel{exact, whereas }\DIFdelend \DIFaddbegin \DIFadd{an exact vector and there were errors incurred in forming }\DIFaddend $\bb{v}$ and $\beta$\DIFdelbegin \DIFdel{were still computed with floating point operations}\DIFdelend . 
%The errors incurred from computing $\bb{v}$ and $\beta$ need to be included in addition to the new rounding errors accumulating from the action of applying $\bb{P}_{\bb{v}}$ to a column.
%\DIFdelbegin %DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdelend \DIFaddbegin \DIFadd{In practice, $\bb{x}$ is any column in $\bb{A}^{(i-1)}[i+1:m, i+1:n]$, where the superscript $(i-1)$ indicates that this submatrix of $\bb{A}$ has already been transformed by $i-1$ HH transformations that zeroed out components below $\bb{A}[j,j]$ for $j = 1:i-1$.
%}\DIFaddend We show the error for forming \DIFdelbegin \DIFdel{$\fl\left(\bb{\hat{v}}^{\top}\bb{x}\right)$ first:
%}\DIFdelend \DIFaddbegin \DIFadd{$\hat{\bb{w}}$ where $\bb{w}:=\beta\bb{v}^{\top}\bb{x}\bb{v}$ and $\bb{v},\bb{x}\in\R^{m}$,
%}\DIFaddend \begin{equation*}
%\DIFaddbegin \hat{\bb{w}} \DIFadd{=}\DIFaddend \fl\DIFdelbegin %DIFDELCMD < \left(%%%
%\DIFdel{\bb{\hat{v}}}\DIFdelend \DIFaddbegin \DIFadd{(}\hat{\beta}\DIFadd{\;\fl(}\hat{\bb{v}}\DIFaddend ^{\top}\bb{x}\DIFdelbegin %DIFDELCMD < \right) %%%
%\DIFdelend \DIFaddbegin \DIFadd{)}\hat{v}\DIFadd{)}\DIFaddend =(1+\tth\DIFdelbegin \DIFdel{_w^{(d+z)}}\DIFdelend \DIFaddbegin \DIFadd{_{m}}\DIFaddend )(\DIFaddbegin \DIFadd{1+\dd)(1+\dd')(\beta+\Delta\beta)(}\DIFaddend \bb{v}+\Delta \bb{v})^{\top}\bb{x}\DIFdelbegin \DIFdel{.
%}%DIFDELCMD < \end{equation*}
%%DIFDELCMD < %%%
%\DIFdel{Where $\tth_w^{(d+z)}$ is incurred from the action of a dot product }\DIFdelend \DIFaddbegin \DIFadd{(\bb{v}+\Delta \bb{v})}\DIFaddend ,
%\DIFdelbegin \begin{align*}
%\DIFdel{\fl\left(\bb{\hat{v}}^{\top}\bb{x}\right)}&\DIFdel{= (1+\tth_w^{(d+z)})(1+\tth_w^{(1+2(d+z+2))})\bb{v}^{\top}\bb{x},}\\
%&\DIFdel{= (1+\tth_w^{(3d+3z+5)})\bb{v}^{\top}\bb{x}.
%}\end{align*}
%%DIFAUXCMD
%\DIFdel{Now we can form $\fl(\bb{w})$,
%}\begin{displaymath}
%\DIFdel{\bb{\hat{w}} =(1+\tth_w^{(2)})(\beta+\Delta\beta)(1+\tth_w^{(3d+3z+5)})\bb{v}^{\top}\bb{x}\bb{w}.
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdel{Here, $\tth_w^{(2)}$ results from multiplying $\hat{\beta}$ }\DIFdelend \DIFaddbegin }
%\DIFadd{where $\tth_{m}$ is from computing the inner product $\hat{\bb{v}}^{\top}\bb{x}$, }\DIFaddend and \DIFdelbegin \DIFdel{$\bb{v}^{\top}\bb{x}$ to $\bb{w}$, }\begin{align*}
%\DIFdel{\bb{\hat{w}} }&\DIFdel{= (1+\tth_w^{(2)})(1+\tth_w^{(3d+3z+5)})\beta(1+\tth_w^{(3d+3z+5)})\bb{v}^{\top}\bb{x}\bb{w},}\\
%&\DIFdel{= (1+\tth_w^{(6d+6z+12)})\bb{w}.
%}\end{align*}
%%DIFAUXCMD
%%DIF <  TODO - save some space in the previous 2 equations.
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{Finally, we can add in the vector subtraction operation and complete the rounding error analysis of applying a Householder transformation to any vector:
%}\begin{align*}
%\DIFdel{\fl(\bb{P}_{\bb{v}}\bb{x}) }& \DIFdel{= \fl(\bb{x}-\bb{\hat{w}}) = (1+\dd_w)(1+\tth_w^{6d+6z+12)})\bb{w}, }\\
%&\DIFdel{= (1+\tth_w^{(6d+6z+13)})\bb{P}_{\bb{v}}\bb{x},}\\
%&\DIFdel{= (\bb{P_v} +\bb{\Delta P_v})\bb{x},\qquad \|\bb{\Delta P_v}\|_F \leq \gamma_w^{(6d+6z+13)}. %DIFDELCMD < \label{eqn:applyP}%%%
%}\end{align*}
%%DIFAUXCMD
%\DIFdel{Details behind the matrix norm errorbound in Equation~\ref{eqn:applyP} are shown in \ref{Appendix:HQR}.
%Constructing both $\bb{Q}$ }\DIFdelend \DIFaddbegin \DIFadd{$\dd$ }\DIFaddend and \DIFdelbegin \DIFdel{$\bb{R}$ relies on applying Householder }\DIFdelend \DIFaddbegin \DIFadd{$\dd'$ are from multiplying $\beta$, $\fl(\hat{\bb{v}}^{\top}\bb{x})$, and $\bb{\hat{v}}$.
%The forward error is
%%DIF > \begin{equation*}
%$\hat{\bb{w}} = \bb{w} + \Delta \bb{w}$, where $|\Delta \bb{w}| \leq \tilde{\gamma}_m|\beta||\bb{v}|^{\top}|\bb{x}||\bb{v}|.$
%%DIF > \end{equation*}
%%DIF > We show the error for forming $\fl\left(\bb{\hat{v}}^{\top}\bb{x}\right)$ where we let $\bb{v},\bb{x}\in\R^{m}$:
%%DIF > \begin{equation*}
%%DIF > \fl\left(\bb{\hat{v}}^{\top}\bb{x}\right) = (1+\tth_{m})(\bb{v}+\Delta \bb{v})^{\top}\bb{x}.
%%DIF > \end{equation*}
%%DIF > Set $\bb{w}:=\beta\bb{v}^{\top}\bb{x}\bb{v}$.
%%DIF > Then,
%%DIF > \begin{equation*}
%%DIF > \hat{\bb{w}} =(1+\tth_{m})(1+\dd)(1+\tilde{\dd})(\beta+\Delta\beta)(\bb{v}+\Delta \bb{v})^{\top}\bb{x}(\bb{v}+\Delta \bb{v}),
%%DIF > \end{equation*}
%%DIF > where $\tth_{m}$ is from computing the inner product $\hat{\bb{v}}^{\top}\bb{x}$, and $\dd$ and $\tilde{\dd}$ are from multiplying $\beta$, $\fl(\hat{\bb{v}}^{\top}\bb{x})$, and $\bb{\hat{v}}$ together. 
%%DIF > We can write 
%%DIF > \begin{equation*}
%%DIF > 	\hat{\bb{w}} = \bb{w} + \Delta \bb{w},\;\; |\Delta \bb{w}| \leq \tilde{\gamma}_m|\beta||\bb{v}|^{\top}|\bb{x}||\bb{v}|.
%%DIF > \end{equation*}
%Subtracting $\hat{\bb{w}}$ from $\bb{x}$ yields the HH transformation with forward error, 
%%DIF > we can add in the vector subtraction operation and complete the rounding error analysis of applying a HH transformation to any vector:
%}\begin{equation}
%\DIFadd{\fl(\hat{\bb{P_v}}\bb{x}) = \fl(\bb{x}-\bb{\hat{w}}) = (1+\dd)(\bb{x}-\bb{w}-\Delta \bb{w}) = \bb{y} + \Delta \bb{y} = (\bb{P_v} + \Delta \bb{P_v})\bb{x},\label{eqn:applyP}
%}\end{equation}
%\DIFadd{where $|\Delta \bb{y}| \leq u|\bb{x}| + \tilde{\gamma}_{m} |\beta||\bb{v}||\bb{v}|^{\top}|\bb{x}|$.
%Using $\sqrt{2/\beta} = \|\bb{v}\|_2$, we form a normwise bound,
%}\begin{equation}
%\DIFadd{\|\Delta \bb{y}\|_2 \leq \tilde{\gamma}_{m}\|\bb{x}\|_2. \label{eqn:19.2c}
%}\end{equation} 
%%DIF > Next, we convert this to a backward error for $\bb{P_v}$.
%%DIF > Since $\Delta \bb{P_v}$ is exactly $\frac{1}{\bb{x}^{\top}\bb{x}}\Delta \bb{y}\bb{x}^{\top}$, we can compute its Frobenius norm by using $\Delta \bb{P_v}[i,j] = \frac{1}{\|\bb{x}\|_2^2}\Delta \bb{y}[i]\bb{x}[j]$,
%\DIFadd{Since $\Delta \bb{P_v}[i,j] = \frac{1}{\|\bb{x}\|_2^2}\Delta \bb{y}[i]\bb{x}[j]$, we can compute its Frobenius norm,
%}\begin{equation}
%\DIFadd{\|\Delta \bb{P_v}\|_F
%= \left(\sum_{i=1}^m\sum_{j=1}^m\left(\frac{1}{\|\bb{x}\|_2^2}\Delta \bb{y}[i]\bb{x}[j]\right)^2\right)^{1/2}
%=  \frac{\|\Delta \bb{y}\|_2}{\|\bb{x}\|_2} \leq \tilde{\gamma}_{m}\label{eqn:outer},
%}\end{equation}
%\DIFadd{where the last inequality is a direct application of \cref{eqn:19.2c}.
%%DIF > We summarize these results in \cref{lem:19.2}.
%%DIF > \begin{lemma}
%%DIF > 	\label{lem:19.2}
%%DIF > 	Let $\bb{x}\in\R^m$ and consider the computation of $\hat{\bb{y}}=\fl(\bb{P_vx})$ via 
%%DIF > 	\begin{equation*}
%%DIF > 		\bb{y}+\Delta \bb{y} = (\bb{P_v} + \Delta \bb{P_v})\bb{x} =\fl(\bb{P_v}\bb{x}) = \fl(\bb{x}-\hat{\beta}\hat{\bb{v}}\hat{\bb{v}}^{\top}\bb{x})
%%DIF > 	\end{equation*}
%%DIF > 	and rounding errors incurred in forming $\hat{\bb{v}}$ and $\hat{\beta}$ are expressed componentwise via $\hat{\bb{v}} = \bb{v}+\Delta \bb{v}$ and $\hat{\beta} = \beta + \Delta \beta$ where the relative componentwise errors for both are bounded by quantity $\gamma_y$:  
%%DIF > 	\begin{equation}
%%DIF > 		|\Delta \bb{v}| \leq \gamma_y|\bb{v}|,\qquad |\Delta \beta| \leq\gamma_y |\beta|.
%%DIF > 	\end{equation}
%%DIF > 	Then, the normwise forward and backward errors are $\|\Delta \bb{y}\|_2 \leq \gamma_y\|\bb{y}\|_2$ and $ \|\Delta \bb{P_v}\|_F \leq \gamma_y.$
%%DIF > \end{lemma}
%%DIF > Note that in a uniform precision setting this bound is represented as $\gamma_y = \tilde{\gamma}_{m}$. 
%}\paragraph{\DIFadd{Applying many successive HH transformations}}
%\DIFadd{Consider applying a sequence of }\DIFaddend transformations in the \DIFdelbegin \DIFdel{above two ways: 1) to zero out below the diagonal of a target column and 2) to update the bottom right submatrix.
%We now have the tools to formulate the forward error bound on $\hat{\bb{Q}}$ }\DIFdelend \DIFaddbegin \DIFadd{set $\{\bb{P}_{i}\}_{i=1}^r\subset\R^{m\times m}$ to $\bb{x}\in\R^m$, where $\bb{P}_{i}$'s are all HH transformations computed with $\tilde{\bb{v}}_i$'s and $\hat{\beta_i}$'s.
%This is directly applicable to HQR as $\bb{Q}=\bb{P}_{1}\cdots\bb{P}_{n}\bb{I}$ }\DIFaddend and \DIFdelbegin \DIFdel{$\hat{\bb{R}}$ calculated from the HQR factorization.
%}\subsubsection{\DIFdel{HQR Factorization Forward Error Analysis}}
%%DIFAUXCMD
%\addtocounter{subsubsection}{-1}%DIFAUXCMD
%%DIF < %% \begin{wrapfigure}{L}{0.5\textwidth}
%%DIF < %% 	\begin{center}
%%DIF < %%		%\centering
%%DIF < %% 		\includegraphics[width=0.5\textwidth]{./figures/figure2.pdf}
%%DIF < %% 	\end{center}
%%DIF < %% 	\caption{\label{fig:QRerr} Grayscale representation of distribution of rounding errors bounds for the HQR algorithm.}% Elements $\hat{\bb{R}}_{ij}=(1+\tth_w^{(r_{ij})})\bb{R}_{ij}$ and $\hat{\bb{Q}}_{ij}=(1+\tth_w^{(q_{ij})})\bb{Q}_{ij}$, where $r_{ij}$ and $q_{ij}$ are represented by grayscale.}	
%%DIF < %% 	%\end{figure}
%%DIF < %% \end{wrapfigure}
%\DIFdelend \DIFaddbegin \DIFadd{$\bb{R} = \bb{Q}^{\top}\bb{A} = \bb{P}_{n}\cdots\bb{P}_{1}\bb{A}$.
%\Cref{lem:3.7} is very useful for any sequence of transformations, where each transformation has a known bound.
%We will invoke this lemma to prove \cref{lem:19.3}, and use it in future sections for other consecutive transformations.
%}\begin{lemma}\label{lem:3.7}
%	\DIFadd{If $\bb{X}_{j} + \Delta \bb{X}_{j} \in\R^{m\times m}$ satisfies $\|\Delta \bb{X}_{j}\|_F\leq \dd_j \|\bb{X}_{j}\|_2$ for all $j$, then }$$\DIFadd{\left|\left|\prod_{j=1}^n (\bb{X}_{j} + \Delta \bb{X}_{j})- \prod_{j=1}^n\bb{X}_{j} \right|\right|_F\leq\left(-1+\prod_{j=1}^n(1+\dd_j)\right)\prod_{j=1}^n\|\bb{X}_{j}\|_2.}$$
%\end{lemma}
%\DIFaddend 
%
%\DIFdelbegin \DIFdel{Consider a thin QR factorization where $\bb{A}\in\R^{m\times n}$ for $m\geq n$, we have $\bb{Q}\in\R^{m\times n}$ }\DIFdelend %DIF > Let us define $$\bb{Q}+\Delta \bb{Q'}\equiv \prod_{i=1}^{r}\left(\bb{P}_i +\Delta \bb{P}_i\right)$$ in the context of applying this matrix to a vector, $\bb{x}\in\R^m$ , where $\Delta \bb{Q'}^{\top}$ represents the backward error of forming $\bb{R}$, instead of the forward error of the $\bb{Q}$ factor. 
%%DIF > The forward error for $\bb{Q}$ is denoted as $\Delta \bb{Q} \equiv \fl(\bb{Q})-\bb{Q}$ where $\fl(\bb{Q})$ is formed via HQR.
%%DIF > That is, if $\bb{y} = \bb{Q}^{\top}\bb{x}$, then $\fl(\bb{y}) = \bb{y}+ \Delta \bb{y} = (\bb{Q}+\Delta \bb{Q'})^{\top}\bb{x}$.
%%DIF > Even though an efficient implementation would use that $\bb{P}_{i}$'s are applied to successively shorter vectors ($\bb{P}_{i}$ is left multiplied to $\bb{A}[i:m,i+1:n]$, which is equivalent to $n-i$ vectors of length $m-i+1$), we assume $\\bb{P}_{i}\}_{i=1}^r\subset\R^{m\times m}$ to allow for a simpler analysis while forming a looser bound. 
%%DIF > We will now use Lemma 3.7 from \cite{Higham2002} to bound $\Delta \bb{Q}'$ with the Frobenius norm.
%%DIF > \begin{align}
%%DIF > \|\Delta \bb{Q}'^{\top}\|_F &= \left|\left| \prod_{r=1}^{1}\left(\bb{P}_i +\Delta \bb{P}_i\right) - \prod_{i=r}^{1}\bb{P}_i \right|\right|_F,\label{eqn:rPs}\\
%%DIF > &\leq \left(\prod_{i=1}^r(1+\tilde{\gamma}_m)-1\right)\prod_{i=r}^1\|\bb{P}_i\|_2 = (1+\tilde{\gamma}_m)^r-1. \label{eqn:rPs-uniform}
%%DIF > \end{align}
%%DIF > While we omit the details here, we can show that $(1+\tilde{\gamma}_m)^r-1 \leq r\tilde{\gamma}_m$ using the argument for \cref{lem:gamma} if $r\tilde{\gamma}_m \leq 1/2$ .
%%DIF > \Cref{lem:19.3}
%\DIFaddbegin \begin{lemma}\label{lem:19.3}
%	\DIFadd{Consider applying a sequence of transformations $\bb{Q}=\bb{P}_{r}\cdots\bb{P}_{2}\bb{P}_1$ onto vector $\bb{x}\in\R^m$ to form $\hat{\bb{y}} =\fl(\hat{\bb{P}}_{r}\cdots\hat{\bb{P}}_{2}\hat{\bb{P}}_{1}\bb{x}),$
%	%DIF > $\bb{x}_{k+1} = \bb{P}_k\bb{x}_k$ for $k=1,\cdots,r$, where $\bb{x}[1] = \bb{x}\in\R^{m}$
%	where $\hat{\bb{P}}_{k}$'s are HH transformations constructed from $\hat{\beta}_k$ }\DIFaddend and \DIFdelbegin \DIFdel{$\bb{R}\in\R^{n\times n}$.
%The pseudo-algorithm in Section \ref{sec:HQRf} shows that each succeeding Householder transformation is applied to a smaller lower right submatrix each time.
%}%DIFDELCMD < \par
%%DIFDELCMD < %%%
%%DIF < For the $\bb{R}$ factor, everything beneath the diagonal is set to zero and therefore is exact, but all other elements incur rounding errors.
%%DIF < These elements ($\hat{\bb{R}_{ii}}$ for $i\leq j$) go through $i-1$ Householder transformations designed to zero out $A^{(0)}[1:m, 1], A^{(1)}[2:m, 2], \cdots, A^{(i-2)}[i-1:m, i-1]$ that correspond to vectors of length $m, \cdots, m-(i-1)$.
%%DIF < In addition, diagonal elements ($\hat{\bb{R}_{ii}}$) are then assigned $\hat{\sigma}$ from the process of zeroing out $A^{(i-1)}[i:m, i]$.
%%DIF < Rounding errors for the $\bb{Q}$ factor can be formulated similarly. 
%%DIF < Since the $i^{th}$ Householder transformation in building $\bb{Q}$ is performed on the $[i:m, i:n]$ lower-right submatrix, elements in $Q[i:m,i]$ and $Q[i,i:n]$ go through $i$ Householder transformations corresponding to vectors of sizes $m-(n-1), \cdots, m-(n-i)$ for $i = 1, \cdots, n$.
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%%DIF < Consequently, rounding error bounds for each element of $\bb{R}$ and $\bb{Q}$ can be specifically computed by its location within the matrices, as is displayed in Figure~\ref{fig:QRerr} for a $10$-by-$6$ example.
%\DIFdel{Instead of continuing with a componentwise analysis of how accumulated }\DIFdelend \DIFaddbegin \DIFadd{$\hat{\bb{v}}_{k}$.
%	These HH vectors and constants are computed via \cref{algo:hh_v2} and the }\DIFaddend rounding errors are \DIFdelbegin \DIFdel{distributed by HQR, we transition into normwise error analyses.
%	To do this, we use the analysis from }\DIFdelend \DIFaddbegin \DIFadd{bounded by \cref{eqn:beta3,eqn:vbound}.
%	If each transformation is computed via \cref{eqn:effH}, then
%	}\begin{align}
%	\DIFadd{\hat{\bb{y}} }&\DIFadd{= \bb{Q} (\bb{x} +\Delta \bb{x}) = (\bb{Q} + \Delta \bb{Q})\bb{x} = \hat{\bb{Q}}\bb{x},}\\
%	\DIFadd{\|\Delta \bb{y}\|_2 }&\DIFadd{\leq r\tilde{\gamma}_m\|\bb{x}\|_2,\;\; \|\Delta \bb{Q}\|_F\leq r\tilde{\gamma}_m .\label{eqn:19.3}
%	}\end{align}
%\end{lemma}
%\begin{proof}
%	\DIFadd{Applying \cref{lem:3.7} directly to $\bb{Q}$ yields
%	}\[\DIFadd{\footnotesize
%	\|\Delta\bb{Q}\|_F = \left|\left|\prod_{j=1}^r (\bb{P}_{j} + \Delta \bb{P}_{j})- \prod_{j=1}^r\bb{P}_{j} \right|\right|_F\leq\left(-1+\prod_{j-1}^r(1+\tilde{\gamma}_{m-j+1})^r\right)\prod_{j=1}^n\|\bb{P}_{j}\|_2 \leq -1+(1+\tilde{\gamma}_m)^r,}\]
%	\DIFadd{since $\bb{P}_{j}$'s are orthogonal and have 2-norm, 1, and $m-j+1 \leq m$.
%	While we omit the details here, we can show that $(1+\tilde{\gamma}_m)^r-1 \leq r\tilde{\gamma}_m$ using the argument from \cref{lem:gamma} if $r\tilde{\gamma}_m \leq 1/2$.
%}\end{proof}
%\DIFadd{In this error analysis, the prevailing bound for errors at various stages of forming and applying a HH transformation is $\tilde{\gamma}_{m}$ where $m$ corresponds to }\DIFaddend the \DIFdelbegin \DIFdel{preceding section (summarized in Equation~\ref{eqn:applyP}) to implicitly form the matrix norm error of the Householder transformation matrix, $\bb{P_v}$. 
%Then, we use the result of Lemma 3.7 in \cite{Higham2002} to get a normwise bound on the perturbation effect of multiple matrixmultiplications. 
%This result is summarized in Theorem~\ref{thm:feHQR}, and the proof is detailed extensively in \ref{Appendix:HQR}.
%%DIF < Then, we use the number of Householder transformations a column of $\bb{A}$ goes through to be transformed into a column of $\bb{R}$, and implicitly find the matrix norm error of the $\bb{Q}^{\top}$ that is formed in the process.
%%DIF < Note that columnwise norms are easily converted to matrix norms (c.f. Lemma 6.6 in\cite{Higham2002}). 
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdelend \DIFaddbegin \DIFadd{dimension of the transformed vectors.
%In \cref{lem:19.3}, a factor of $r$ is introduced for applying $r$ HH transformations to form the term $r\tilde{\gamma}_m \approx rmu$. 
%Therefore, we can expect that the columnwise norm error for a thin QR factorization should be $\cO(mnu)$ for a full rank matrix. 
%%DIF > To easily generalize this section for mixed precision analysis, we benefit from alternatively denoting this quantity as $\tilde{\gamma}_{\bb{P}}$ with the understanding that $\tilde{\gamma}_{\bb{P}}$ will be some combination of $\tilde{\gamma}$'s of differing precisions. 
%%DIF > The bound in \Cref{eqn:19.3} would then be  replaced by $r\tilde{\gamma}_{\bb{P}}$.
%%DIF > \begin{equation}
%%DIF > 	(1+\tilde{\gamma}_{\bb{P}})^r-1 \leq r\tilde{\gamma}_{\bb{P}}. \label{eqn:algebra3}
%%DIF > \end{equation}
%In \cref{thm:feHQR}, we formalize this by applying \cref{lem:19.3} directly and also show a conversion of columnwise bounds to a matrix norm bound,
%%DIF > Applying \cref{lem:19.3} directly to columns of $\bb{A}$ and $\bb{I}$ allows us to formulate 2-norm forward bounds for columns of $\bb{R}$ and $\bb{Q}$.
%%DIF > We show how to convert these columnwise bounds into matrix norms for the $\bb{R}$ factor.
%%DIF > Next, we apply \cref{eqn:19.3} to the $i^{th}$ columns of $\bb{Q},\bb{R}$ and set $r=n$ for a full rank matrix, $\bb{A}$.
%%DIF > Then,
%%DIF > \begin{align*}
%%DIF > 	\|\Delta \bb{R}[:,i]\|_2 &= \|\Delta \bb{Q'}^{\top}\bb{A}[:,i]\|_2 \leq \|\Delta \bb{Q'}\|_2 \|\bb{A}[:,i]\|_2  \leq n\tilde{\gamma}_{m} \|\bb{A}[:,i]\|_2, \\ 
%%DIF > 	\|\Delta \bb{Q}[:,i]\|_2 &= \|\Delta \bb{Q'}\bb{I}[:,i]\|_2 \leq \|\Delta \bb{Q'}\|_2\leq n\tilde{\gamma}_{m}.
%%DIF > \end{align*}
%%DIF > These columnwise bounds can now be transformed into matrix norms as follows: 
%}\begin{equation*}
%\DIFadd{\|\Delta \bb{R} \|_F = \left(\sum_{i=1}^n \|\Delta \bb{R}[:,i]\|_2^2\right)^{1/2} \leq \left(\sum_{i=1}^n n^2\tilde{\gamma}_{m}^2 \|\bb{A}[:,i]\|_2^2\right)^{1/2} = n\tilde{\gamma}_{m} \|\bb{A}\|_F.
%%DIF > \|\Delta \bb{Q} \|_F &= \left(\sum_{i=1}^n \|\Delta \bb{Q}[:,i]\|_2^2\right)^{1/2} \leq \left(\sum_{i=1}^n n^2\tilde{\gamma}_{m}^2 \right)^{1/2} = n^{3/2}\tilde{\gamma}_{m}.
%}\end{equation*}
%\DIFadd{We gather these results into \cref{thm:feHQR}.
%}\DIFaddend \begin{theorem}
%	\label{thm:feHQR}
%	Let $\bb{A}\in\R^{m\times n}$ with $m\geq n$ have full rank, $n$. 
%	Let $\hat{\bb{Q}}\in\R^{m\times n}$ and $\hat{\bb{R}}\in\R^{n\times n}$ be the thin QR factors of $\bb{A}$ obtained via \DIFdelbegin \DIFdel{the HQR algorithm with a mixed-precision scheme as is outlined in Assumption~\ref{assump:mp}. 
%Let $d=\lfloor\frac{(m-1) u_s}{u_w}\rfloor$, and $z=1$ or $z=2$.
%Then we have normwise forward error bounds
%	}\begin{align*}
%	\DIFdel{\hat{\bb{R}} }&\DIFdel{= \bb{R} + \bb{\Delta R} = \hat{\bb{P}}_n\cdots\hat{\bb{P}}_1 \bb{A},}\\
%	\DIFdel{\hat{\bb{Q}} }&\DIFdel{= \bb{Q} + \bb{\Delta Q} = \hat{\bb{P}}_1\cdots\hat{\bb{P}}_n \bb{I},
%	}\end{align*}
%	%DIFAUXCMD
%\DIFdel{where }\begin{displaymath}
%	  \DIFdel{\|\bb{\Delta Q}\|_F \leq n^{3/2} \tilde{\gamma}_w^{(6d+6z+13)},
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{and for column $j$ in $\{1, \cdots, n\}$, }\begin{displaymath}
%	\DIFdel{\|\bb{\Delta R}[:,j]\|_2 \leq j\tilde{\gamma}_w^{(6d+6z+13)}\|\bb{A}[:,j]\|_2.
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{We also form a backward error. Let $\bb{A}+\bb{\Delta A} = \hat{\bb{Q}}\hat{\bb{R}}$, where $\hat{\bb{Q}}$ and $\hat{\bb{R}}$ are obtained via Algorithm~\ref{algo:hhQR}.
%Then, }\begin{displaymath}
%	\DIFdel{\|\bb{\Delta A}\|_F \leq n^{3/2}\tilde{\gamma}_w^{(6d+6z+13)}\|\bb{A}\|_F.
%	}\end{displaymath}
%%DIFAUXCMD
%\DIFdelend \DIFaddbegin \DIFadd{\cref{algo:hhQR}.
%	Then,
%	}\begin{align*}
%	\DIFadd{\hat{\bb{R}} }&\DIFadd{= \bb{R} + \Delta \bb{R} = \fl(\hat{\bb{P}}_n\cdots\hat{\bb{P}}_1 \bb{A}),\;\; \|\Delta \bb{R}[:,j]\|_2\leq n\tilde{\gamma}_{m} \|\bb{A}[:,j]\|_2,\;\; \|\Delta \bb{R}\|_F\leq n\tilde{\gamma}_{m} \|\bb{A}\|_F}\\
%	\DIFadd{\hat{\bb{Q}} }&\DIFadd{= \bb{Q} + \Delta \bb{Q} = \fl(\hat{\bb{P}}_1\cdots\hat{\bb{P}}_n \bb{I}),\;\; \|\Delta \bb{Q}[:,j]\|_2\leq n\tilde{\gamma}_{m},\;\; \|\Delta \bb{Q}\|_F \leq n^{3/2} \tilde{\gamma}_{m}.
%	}\end{align*}
%	%DIF > 	Let $\bb{A}+\Delta \bb{A} = \hat{\bb{Q}}\hat{\bb{R}}$, where $\hat{\bb{Q}}$ and $\hat{\bb{R}}$ are obtained via Algorithm~\ref{algo:hhQR}.
%	%DIF > 	Then the backward error is
%	%DIF > 	\begin{equation*}
%	%DIF > 	\|\hat{\bb{Q}}\hat{\bb{R}}-\bb{A}\|_F \leq n^{3/2}\tilde{\gamma}_{m}\|\bb{A}\|_F.
%	%DIF > 	\end{equation*}
%\DIFaddend \end{theorem}
%%DIF > Note that the last backward error result follows from the columnwise forward errors for $\hat{\bb{R}}$ and $\hat{\bb{Q}}$.
%%DIF > Out of all of these different ways of measuring the error from computing a QR factorization (forward/backward errors for column/matrix norms), we will focus on $\|\hat{\bb{Q}}-\bb{Q}\|_F$ , a measure of orthogonality of the $\bb{Q}$ factor for the remainder of \cref{sec:algo} and for \cref{sec:mpanalysis}. 
%\DIFaddbegin \DIFadd{In future sections, we show the forward error columnwise bounds for each factor which can be easily converted to matrix norm bounds.
%The numerical experiments in \cref{sec:NE} measure backward errors with $\|\hat{\bb{Q}}\hat{\bb{R}}-\bb{A}\|_F$ and the orthogonality of the $\bb{Q}$ factor with $\|\hat{\bb{Q}}^{\top}\hat{\bb{Q}}-\bb{I}\|_2$.
%%DIF >  since it can actually be computed.
%%DIF >  we compare various QR factorizations with respect to $\|\hat{\bb{Q}}-\bb{Q}\|_F$, a comprehensive measure of orthogonality of the $\bb{Q}$ factor.
%}\DIFaddend 
%
%\DIFdelbegin \subsubsection{\DIFdel{HQR Comparison to Uniform Precision Analysis}}
%%DIFAUXCMD
%\addtocounter{subsubsection}{-1}%DIFAUXCMD
%%DIFDELCMD < \label{sec:mpupHQRcomparison}
%%DIFDELCMD < %%%
%%DIF < Contributions from the mixed-precision inner product scheme on the results from Theorem~\ref{thm:feHQR} are shown directly at the level of a single Householder transformation, as shown in Equation~\ref{eqn:applyP}.
%\DIFdel{The mixed-precision segments of the analysis behind Theorem~\ref{thm:feHQR} derive from the mixed-precision inner product scheme outlined in Assumption~\ref{assump:mp} and are propagated to form the error bounds for a single Householder transformation as }\DIFdelend \DIFaddbegin \DIFadd{The content of this section shows the standard rounding error analysis in \cite{Higham2002} where some important stages are summarized in \cref{eqn:beta3,eqn:vbound,eqn:19.3}, which we will modify to different mixed precision settings in \cref{sec:mpanalysis}.
%%DIF > by employing quantities denoted via $\Delta \beta$, $\Delta \bb{v}$, $\tilde{\gamma}_y$, and $\tilde{\gamma}_{\bb{P}}$. 
%These quantities account for various forward and backward errors formed in computing essential components of HQR, namely the HH constant and vector, as well as normwise errors of the action of applying HH transformations.
%In the next sections, we present blocked variants of HQR that use \cref{algo:hhQR}.
%}
%
%%DIF > \input{algo-BQR}
%\subsection{\DIFadd{Block HQR with partitioned columns (BQR)}}\label{sec:BQR}
%\DIFadd{We refer to the blocked variant of HQR where the columns are partitioned as BQR. 
%Note that this section relies on the WY representation described in \cite{Bischof1987} instead of the storage-efficient version of \cite{Schreiber1989}, even though both are known to be just as numerically stable as HQR.
%%DIF > , which is widely implemented.
%}\subsubsection{\DIFadd{The WY Representation}}
%\DIFadd{A convenient matrix representation that accumulates $r$ HH reflectors is known as the WY representation (see \cite{Bischof1987,golub2013matrix}).
%\Cref{lem:WY} shows how to update a rank-$j$ update of the identity, $\bb{Q}^{(j)}$, with a HH transformation, $\bb{P}$, to produce a rank-$(j+1)$ update of the identity, $\bb{Q}^{(j+1)}$. 
%With the correct initialization of $\bb{W}$ and $\bb{Y}$, we can build the WY representation of successive HH transformations as shown in \Cref{algo:buildWY}. 
%This algorithm assumes that the HH vectors, $\bb{V}$, and constants,$\bm{\beta}$, have already been computed.
%Since the $\bb{Y}$ factor is exactly $\bb{V}$, we only need to compute the $\bb{W}$ factor.
%}\begin{lemma}\label{lem:WY}
%	\DIFadd{Suppose $\bb{X}^{(j)}=\bb{I}-\bb{W}^{(j)}\bb{Y}^{(j)\top}\in\R^{m\times m}$ is an orthogonal matrix with $\bb{W}^{(j)},\bb{Y}^{(j)}\in\R^{m\times j}$.
%	Let us define $\bb{P}=\bb{I}-\beta\bb{vv}^{\top}$ for some $\bb{v}\in\R^m$ and let $\bb{z}^{(j+1)}=\beta\bb{X}^{(j)}\bb{v}$.
%	Then, }\[\DIFadd{\bb{X}^{(j+1)} = \bb{X}^{(j)}\bb{P} = \bb{I} - \bb{W}^{(j+1)}\bb{Y}^{(j+1)\top}, }\]\DIFadd{where $ \bb{W}^{(j+1)} =[\bb{W}^{(j)}|\bb{z}]$ and $ \bb{Y}^{(j+1)}=[\bb{Y}^{(j)}|\bb{v}]$ are each $m$-by-$(j+1)$. 
%}\end{lemma}
%%DIF > 
%%DIF > Let us now show the proof for \cref{lem:WY}.
%%DIF > \begin{proof}
%%DIF > 	A direct right multiplication of $\bb{P}:=\bb{I}_m - \beta\bb{v}\bb{v}^{\top}$ onto $\bb{Q}$ can be written as
%%DIF > 	\begin{equation*}
%%DIF > 	\bb{QP}=\bb{Q}-\beta\bb{Q}\bb{v}\bb{v}^{\top}.
%%DIF > 	\end{equation*}
%%DIF > 	Let us use the WY representation of $\bb{Q}$.
%%DIF > 	\begin{equation*}
%%DIF > 	\bb{QP}= \bb{I}_m - \bb{WY}^{\top} -\beta\bb{Q}\bb{v}\bb{v}^{\top} = \bb{I}_m - \bb{WY}^{\top} - \bb{z}\bb{v}^{\top}
%%DIF > 	\end{equation*}
%%DIF > 	Now note that the two subtracted terms are exactly the updated WY factors:
%%DIF > 	\[ \bb{W}_+\bb{Y}_+^{\top} = [\bb{W} \quad \bb{z}]\begin{bmatrix}
%%DIF > 	\bb{Y}^{\top}\\ 
%%DIF > 	\bb{v}^{\top}
%%DIF > 	\end{bmatrix} = \bb{WY}^{\top} + \bb{z}\bb{v}^{\top}.\]
%%DIF > \end{proof}
%
%\begin{algorithm2e}
%	\DontPrintSemicolon %DIF >  Some LaTeX compilers require you to use \dontprintsemicolon instead
%	\KwIn{$\bb{V}\in\R^{m \times r}$, $\bm{\beta}\in\R^{r}$ where $m > r$.}	
%	\KwOut{$\bb{W}$} 
%	\DIFadd{Initialize: $\bb{W}:=\bm{\beta}_1\bb{V}[:,1]$.}\tcc*{$\bb{Y}$ is $\bb{V}$.}
%	\For{$j=2:r$}{
%		$\bb{z}\gets \bm{\beta}_j \left[\bb{V}[:,j] - \bb{W}\left(\bb{V}[:,1:j-1]^{\top}\bb{V}[:,j]\right)\right]$\\
%		$\bb{W} \gets [\bb{W}\quad \bb{z}]$ \tcc*{Update $\bb{W}$ to an $m$-by-$j$ matrix.}
%		%$\bb{Y} \gets [\bb{Y}\quad \bb{V}[:,j]]$ \tcc*{Update $\bb{Y}$.}
%	}
%	\Return \DIFadd{$\bb{W}$
%	}\caption{\DIFadd{$\bb{W},\bb{Y}\gets {\tt buildWY}(V, \bm{\beta})$: Given a set of householder vectors $\{\bb{V}[:,i]\}_{i=1}^r$ and their corresponding constants $\{\bm{\beta}_i\}_{i=1}^r$, form the final $\bb{W}$ and $\bb{Y}$ factors of the WY representation of $\bb{P}_1\cdots \bb{P}_r$, where $\bb{P}_i := \bb{I}_m - \bm{\beta}_i\bb{v}_i\bb{v}_i^{\top}$}}
%	\label{algo:buildWY}
%\end{algorithm2e}
%
%\DIFadd{In HQR, $\bb{A}$ is transformed into an upper triangular matrix $\bb{R}$ by identifying a HH transformation that zeros out a column below the diagonal, then applying that HH transformation to the bottom right partition. 
%For example, the $k^{th}$ HH transformation finds an $m-k+1$ sized HH transformation that zeros out column $k$ below the diagonal and then applies it to the $(m-k+1)$-by-$(n-k)$ partition of the matrix, $\bb{A}[k:m,k+1:n]$.
%Since the $k+1^{st}$ column is transformed by the $k^{th}$ HH transformation, this algorithm must be executed serially as shown in \cref{algo:hhQR}.
%The highest computational burden at each iteration falls on \cref{algo:hhQR} line 6, which requires Level-2 BLAS operations when computed efficiently. }\par
%
%\DIFadd{In contrast, BQR replaces this step with Level-3 BLAS operations by partitioning $\bb{A}$ into blocks of columns.
%Let $\bb{A} = [\bb{C}_1 \cdots  \bb{C}_N]$ where $\bb{C}_1,\cdots,\bb{C}_{N-1}$ are each $m$-by-$r$, and $\bb{C}_N$ holds the remaining columns.
%The $k^{th}$ block, $\bb{C}_k$, is transformed with HQR (\cref{algo:hhQR}), and the WY representation of these $r$ successive HH transformations is constructed as in \cref{algo:buildWY}.
%We write the WY update as
%}\begin{equation}
%\DIFadd{\bb{X}_k = \bb{I}_m -\bb{W}_{k}\bb{Y}_{k}^{\top} = \bb{P}_k^{(1)}\cdots\bb{P}_{k}^{(r)}.
%}\end{equation}
%\DIFadd{Thus far, \cref{algo:hhQR,algo:buildWY} are rich in Level-2 BLAS operations.
%Next, $\bb{I} -\bb{Y}_{k}\bb{W}_{k}^{\top}$ is applied to $[\bb{C}_2 \cdots  \bb{C}_N]$ with two Level-3 BLAS operations as shown in line 5 of \cref{algo:blockHQR}.
%BQR performs approximately $1-\cO(1/N)$ fraction of its FLOPs in Level-3 BLAS operations (see section 5.2.3 of \cite{golub2013matrix}), and can reap the benefits from the accelerated block FMA feature of TensorCore. 
%Note that BQR does require strictly more FLOPs when compared to HQR, but these additional FLOPs are negligble in standard precision and does not impact the numerical stability.
%%DIF > TODO: some statement about experiment about this in low/mixed precision
%A pseudoalgorithm for BQR }\DIFaddend is shown in \DIFdelbegin \DIFdel{Equation~\ref{eqn:applyP}.
%All steps to form the error bounds in Theorem~\ref{thm:feHQR} from the error bound for a single Householder transformation (Equation~\ref{eqn:applyP})directly follow the analyses in Section 19.3 of\cite{Higham2002}}\DIFdelend \DIFaddbegin \DIFadd{\cref{algo:blockHQR} where we assume that $n=Nr$ to make our error analysis in \cref{sec:BQRerr} simple.
%In practice, an efficient implementation might require $r$ to be a power of two or a product of small prime factors and result a thinner $N^{th}$ block compared to the rest. 
%This discrepancy is easily fixed by padding the matrix with zeros, a standard procedure for standard algorithms like the Fast Fourier Transform (FFT)}\DIFaddend .
%\DIFdelbegin \DIFdel{In these steps, we generalize the single Householder transformation error bound,
%}\DIFdelend %DIF > TODO: cite?
%\DIFaddbegin \DIFadd{For any variable $x$ in $\{\bb{X},\bb{W}, \bb{Y}, \bb{z}, \beta, \bb{v}, \bb{P}\}$,  $x_k^{(j)}$ corresponds to the $j^{th}$ update for the $k^{th}$ block.
%%DIF > Note that the subscripts on $\bb{W}_{k},\bb{Y}_k$ indicate the WY representation for the HH transformations on the $k^{th}$ block of $\bb{A}$, $\bb{C}_{k}$, whereas the superscripts on $\bb{W}_k^{(j)}$ in \cref{lem:WY} refers to the $j^{th}$ update within building a WY representation. 
%}\begin{algorithm2e}
%	\DontPrintSemicolon %DIF >  Some LaTeX compilers require you to use \dontprintsemicolon instead
%	\KwIn{$\bb{A}\in\R^{m \times n}$, $r\in\R$ where $r < n$.}
%	\KwOut{$\bb{Q},\bb{R}$}
%	\DIFadd{$N=\frac{n}{r}$}\\
%	\tcp{Let $\bb{A} = [\bb{C}_{1} \cdots  \bb{C}_{N}]$ where all blocks except $\bb{C}_{N}$ are $m$-by-$r$ sized.}
%	%DIF > \tcp{Let $n_i=ri$ for $i=1:N-1$ and $n_N=n$.} 
%	\For{$i=1:N$}{
%		$\bb{V}_{i},\bm{\beta}_i,\bb{C}_{i}\gets$ {\tt hhQR}($\bb{C}_{i}$)\tcc*{\Cref{algo:hhQR}}
%		%$\bb{V}_i,\bm{\beta}_i,\bb{A}_{n_{i-1}+1:m,n_{i-1}+1:n_i}\gets$ {\tt hhQR}$(\bb{A}_{n_{i-1}:m,n_{i-1}+1:n_i})$\tcc*{\Cref{algo:hhQR}}
%		$\bb{W}_{i}\gets $ {\tt buildWY}$(\bb{V}_{i},\bm{\beta}_i)$ \tcc*{\Cref{algo:buildWY}}
%		$[\bb{C}_{i+1}\cdots\bb{C}_{N}]$ -= $\bb{V}_{i} \left(\bb{W}_{i}^{\top}[\bb{C}_{i+1}\cdots\bb{C}_{N}]\right) $ \tcc*{update the rest: BLAS-3}
%	}
%	%DIF > 	\tcp{Upper-triangularize the last set of columns and get the last WY factors.}
%	%DIF > 	$\bb{V}_N,\bm{\beta}_N,\bb{A}_{n_{}:m,n_{N-1}+1:n}\gets$ {\tt hhQR}$(\bb{A}_{m_N+1:m,n_{N-1}+1:n})$\tcc*{\Cref{algo:hhQR}}
%	%DIF > 	$\bb{W}_N,\bb{Y}_N \gets $ {\tt buildWY}$(\bb{V}_N,\bm{\beta}_N)$ \tcc*{\Cref{algo:buildWY}}
%	\tcp{$\bb{A}$ has been transformed into $\bb{R}=\bb{Q}^{\top}\bb{A}$.}
%	\tcp{Now build $\bb{Q}$ using level-3 BLAS operations.} 
%	\DIFadd{$\bb{Q}\gets \bb{I}$}\tcc*{$\bb{I}_m$ if full QR, and $\bb{I}_{m\times n}$ if thin QR.}
%	\For{$i=N:-1:1$}{
%		$\bb{Q}[(i-1)r+1:m,(i-1)r+1:n]$-= $\bb{W}_i \left(\bb{V}_i^{\top}\bb{Q}[(i-1)r+1:m,(i-1)r+1:n]\right)$
%	}
%	\Return \DIFadd{$\bb{Q},\bb{A}$
%	}\caption{\label{algo:blockHQR} \DIFadd{$\bb{Q},\bb{R}\gets {\tt blockHQR}(\bb{A}, r)$: Perform HH QR factorization of matrix $\bb{A}$ with column partitions of size $r$.}}
%\end{algorithm2e}
%\subsubsection{\DIFadd{BQR: Rounding Error Analysis}}\label{sec:BQRerr}
%\DIFadd{We now present the basic structure for the rounding error analysis for \cref{algo:blockHQR}, which consist of: 1)HQR, 2)building the W factor, and 3) updating the remaining blocks with the WY representation.
%We have adapted the analysis from \cite{Higham2002} to fit this exact variant, and denote $\hat{\bb{Q}}_{BQR},\hat{\bb{R}}_{BQR}$ to be the outputs from \cref{algo:blockHQR}.
%First, we analyze the error accumulated from updating $\bb{X}_k^{(j-1)}$ to $\bb{X}_k^{(j)}$, which applies a rank-1 update via the subtraction of the outer product $\hat{\bb{z}}_{k}^{(j)}\hat{\bb{v}}_{k}^{(j)\top}$.
%Since $\bb{z}_{k}^{(j)} = \beta_k^{(j)}\bb{X}_{k}^{(j-1)}\bb{v}_{k}^{(j)}$, this update requires a single HH transformation on the right side in the same efficient implementation that is discussed in \cref{eqn:effH},
%}\DIFaddend \begin{equation}
%\DIFaddbegin \hat{\bb{X}_k^{(j)}} \DIFadd{= 
%%DIF > \bb{I}-\begin{bmatrix}
%%DIF > \hat{\bb{W}}_k^{(j-1)}& \fl(\bb{z}_k^{(j)})
%%DIF > \end{bmatrix}\begin{bmatrix}
%%DIF > \hat{\bb{Y}}_k^{(j-1)\top} \\
%%DIF > \hat{\bb{v}}_k^{(j)\top}
%%DIF > \end{bmatrix} = 
%}\hat{\bb{X}}\DIFadd{_k^{(j-1)} - }\DIFaddend \fl(\DIFdelbegin \DIFdel{\bb{P}_{\bb{v}}\bb{x}}\DIFdelend \DIFaddbegin \hat{\beta}\DIFadd{_k^{(j-1)}}\hat{\bb{X}}\DIFadd{_k^{(j-1)}}\hat{\bb{v}}\DIFadd{_k^{(j-1)}}\DIFaddend )\DIFaddbegin \hat{\bb{v}}\DIFadd{_k^{(j)\top} }\DIFaddend = \DIFaddbegin \hat{\bb{X}}\DIFadd{_k^{(j-1)}}\DIFaddend (\bb{P}\DIFdelbegin \DIFdel{_{\bb{v}} }\DIFdelend \DIFaddbegin \DIFadd{_k^{(j)}}\DIFaddend +\DIFdelbegin \DIFdel{\bb{\Delta P_{v}}}\DIFdelend \DIFaddbegin \DIFadd{\Delta \bb{P}_k^{(j)}}\DIFaddend )\DIFdelbegin \DIFdel{\bb{x}}\DIFdelend , \DIFdelbegin %DIFDELCMD < \qquad %%%
%\DIFdel{\|\bb{\Delta P_v}\|_F \leq \epsilon,}%DIFDELCMD < \label{eqn:applyPgen}
%%DIFDELCMD < %%%
%\DIFdelend \DIFaddbegin \label{eqn:Xupdate}
%\DIFaddend \end{equation}
%\DIFdelbegin \DIFdel{for some small quantity $0<\epsilon\ll 1$, }\DIFdelend \DIFaddbegin \DIFadd{where $\|\Delta \bb{P}_k^{(j)}\|_F \leq \tilde{\gamma}_{m-(k-1)r}$.
%Since $\hat{\bb{X}}_k^{(1)} = \bb{I} - \hat{\beta}_k^{(1)}\hat{\bb{v}}_k^{(1)}\hat{\bb{v}}_k^{(1)\top} = \bb{P}_k^{(1)} + \Delta \bb{P}_k^{(1)}$, we can travel up the recursion relation in \cref{eqn:Xupdate} and use \cref{lem:3.7} to form
%}\begin{equation}
%\DIFadd{\|\Delta \bb{X}_k^{(j)} \|_F \leq j\tilde{\gamma}_{m-(k-1)r}. \label{eqn:deltX}
%}\end{equation}
%
%\paragraph{\DIFadd{HQR within each block: line 3 of \cref{algo:blockHQR}}}
%\DIFadd{We apply \Cref{algo:hhQR} to the $k^{th}$ block, $\hat{\bb{X}}_{k-1}\cdots\hat{\bb{X}}_1\bb{C}_k$, which applies $r$ more HH transformations to columns that had been transformed by $(k-1)$ WY transformations in prior iterations.
%The upper trapezoidal factor that results from applying HQR to $\bb{C}_{k}^{((k-1)r)}$ corresponds to the $(k-1)r+1^{st}$ to $kr^{th}$ columns of $\hat{\bb{R}}_{BQR}$, and applying \cref{lem:3.7,lem:19.3} yields
%}\begin{equation*}
%\DIFadd{\|\hat{\bb{R}}_{BQR}[:,j]-\bb{R}[:,j]\|_2 \leq r\tilde{\gamma}_{m}\|\hat{\bb{X}}_{k-1}\cdots\hat{\bb{X}}_1^{\top}\bb{C}_k[:,j]\|_2,\;\; j=(k-1)r+1:kr.%DIF > \leq kr\tilde{\gamma}_{m}\|\bb{A}[:,j]\|_2,\;\; j=(k-1)r+1:kr.
%}\end{equation*}
%%DIF > where $\bb{A}^{((k-1)r)}[:,j]$ has been transformed with $k-1$ WY updates already.
%%DIF > Let $\bb{R}_{k}$ be the $\bb{R}$ factor acquired from applying HQR on $\bb{C}_k$ : $ \hat{\bb{R}}_{k} = \bb{Q}^{\top} (\bb{C}_{k} + \Delta \bb{C}_{k}), $ where $\Delta \bb{C}_{k}$ represent the backward error incurred in this step. 
%%DIF > Then, applying \cref{lem:19.3} to the $j^{th}$ column of $\bb{C}_{k}$ yields 
%%DIF > \begin{equation*}
%%DIF > 	\|\Delta\bb{C}_k[:,j]\|_2 = \leq j\tilde{\gamma}_{m-(k-1)r} \|\bb{ C}_k[:,j]\|_2.
%%DIF > \end{equation*}
%%DIF > We can similarly apply \cref{thm:feHQR} for block-wise norm errors. 
%%DIF > Adjusting \cref{eqn:deltQ} for $k<N$ yields
%%DIF > \begin{equation}
%%DIF > \|\Delta \bb{Q'}\|_2\leq \|\Delta \bb{Q'}\|_F = \|\Delta \bb{Q'}^{\top}\|_F\leq r \tilde{\gamma}_{m-(i-1)k} \label{eqn:deltQBQR}
%%DIF > \end{equation}
%%DIF > \begin{align}
%%DIF > \|\Delta \bb{Q'}_k^{\top}\|_F &= \left|\left| \prod_{j=r}^{1}\left(\bb{P}_{(k-1)r+j} +\Delta\bb{P}_{(k-1)r+j}\right) - \prod_{j=r}^{1}\bb{P}_{(k-1)r+j} \right|\right|_F,\label{eqn:rPs-BQR}\\
%%DIF > &\leq \left(\prod_{i=1}^r(1+\tilde{\gamma}_{m-(r-1)k})-1\right)\prod_{i=r}^1\|\bb{P}_i\|_2 = (1+\tilde{\gamma}_{m-(r-1)k})^r-1. \label{eqn:rPs-BQR-uniform}
%%DIF > \end{align}
%%DIF > Since the $j^{th}$ HH vector for $\bb{C}_{k}$ zeros out the $r(k-1)+j^{th}$ column of $\bb{A}$ below the diagonal, it has length  $m -(k-1)r-j+1$. 
%%DIF > This corresponds to setting $i=r(k-1)+j$ in \cref{eqn:vbound,eqn:beta3}
%\paragraph{\DIFadd{Build WY at each block: line 4 of \cref{algo:blockHQR}}}
%%DIF > The WY representation of $r$ HH transformations in full precision is exactly that, just another representation. 
%\DIFadd{We now calculate the rounding errors incurred from building the WY representation when given a set of HH vectors and constants as shown in \cref{algo:buildWY}.
%%DIF > Consider the $j^{th}$ update of the WY representation for block $\bb{C}_{k}$.
%%DIF > Our goal is to analyze the error accumulated from updating the WY representation from the $j-1^{st}$ step to the $j^{th}$ for block $\bb{C}_{k}$.
%%DIF > Let us represent the $j^{th}$ HH constant and vector of the $k^{th}$ block computed with FLOPs as with $\hat{\beta}_k^{(j)}$ and $\hat{\bb{v}}_{k}^{(j)}$ and the $j^{th}$ update to the WY representation as $$\bb{X}_{k}^{(j)} = \bb{I} - \hat{\bb{W}}_{k}^{(j)}\hat{\bb{Y}}_{k}^{(j)\top}.$$ %we write $\bb{V}_k[:,1:j]=:\bb{Y}_k^{(j)}$ to make a clear connection to \cref{lem:WY}, and also denote
%%DIF > The update in \cref{lem:WY} applies a rank-1 update via the subtraction of the outer product $\hat{\bb{z}}_{k}^{(j)}\hat{\bb{v}}_{k}^{(j)\top}$ to apply $\hat{\bb{P}}_{\bb{(k-1)r+j}}\equiv \hat{\bb{P}}_k^{(j)}$ on the right. 
%%DIF > Since $\bb{z}_{k}^{(j)} = \beta_k^{(j)}\bb{X}_{k}^{(j-1)}\bb{v}_{k}^{(j)}$, this update requires a single HH transformation in the same efficient implementation that is discussed in \cref{eqn:effH}, but on the right side:
%%DIF > \begin{align*}
%%DIF > 	\bb{X}_{k}^{(j)} &=\bb{X}_{k}^{(j-1)} - \bb{z}_k^{(j)}\bb{v}_k^{(j)\top}\\ %&=\bb{X}_{k}^{(j-1)}-\beta_k^{j}\bb{X}_{k}^{(j-1)}\bb{v}_{k}^{(j)}\bb{v}_{k}^{(j-1)\top} \\
%%DIF > 	&= \bb{X}_{k}^{(j-1)} (\bb{I}-\beta_k^{(j)}\bb{v}_{k}^{(j)}\bb{v}_{k}^{(j)\top})=  \bb{X}_{k}^{(j-1)}\bb{P}_{k}^{(j)}.
%%DIF > \end{align*}
%%DIF > The analysis for this update is slightly different as it is applied to a matrix, but the componentwise error remains the same
%%DIF > Traveling up this recursion relation to $j=1$, we find that $\hat{\bb{X}}_{k}^{(j)}$ is built from applying a sequence of HH transformations $\{\hat{\bb{P}}_{k}^{(2)},\cdots \hat{\bb{P}}_k^{(j)}\}$ to $\bb{P}_k^{(1)}$.
%%DIF > Therefore, we can apply \cref{eqn:19.3} (set $\bb{Q}=\bb{X}_{k}^{(j-1)}$, $\bb{x}=\fl(\hat{\beta}_k^{(j)}\hat{\bb{v}}_{k}^{(j)})$, and $\bb{y}=\bb{z}_{k}^{(j)}$) to form \cref{eqn:BQR-z}.
%%DIF >  directly for the construction of $\bb{z}_{k}^{(j)}$
%Since the columns of $\hat{\bb{Y}}_k$ are simply $\{\hat{\bb{v}}_k^{(j)}\}$ built in \cref{algo:hhQR} the errors for forming these are shown in \cref{eqn:vbound} where $m$ should be replaced by $m-(k-1)r$.
%The HH constants, $\hat{\beta}_k^{(j)}$ are bounded by \cref{eqn:beta3} modified similarly. 
%Thus, $\bb{z}_k^{(j)}$ is the only newly computed quantity. 
%Using \cref{eqn:deltX,eqn:vbound,eqn:beta3}, we find
%}\begin{align*}
%\DIFadd{\|\Delta \bb{z}_k^{(j)}\|_2 }&\DIFadd{= \|\Delta\bb{X}_k^{(j-1)}\hat{\beta}_k^{(j)}\hat{\bb{v}}_k^{(j)} \|_2 \leq \|\Delta\bb{X}_k^{(j-1)}\|_2 \|\hat{\beta}_k^{(j)}\hat{\bb{v}}_k^{(j)}\|_2  \leq \|\Delta\bb{X}_k^{(j)-1}\|_F\|\hat{\beta}_k^{(j)}\hat{\bb{v}}_k^{(j)}\|_2 }\\
%& \DIFadd{\leq \left((1+(j-1)\tilde{\gamma}_{m-(k-1)r})(1 + \tilde{\gamma}_{m-(k-1)r})-1\right) \| \beta_k^{(j)}\bb{v}_k^{(j)}\|_2 \leq j\tilde{\gamma}_{m-(k-1)r}\|\bb{z}_k^{(j)}\|_2.
%%DIF > 	\|\Delta \hat{\bb{z}}_k^{(j)}\|_2 &= \|\bb{X}_k^{(j)-1}(\hat{\beta}_k^{(j)}\hat{\bb{v}}_k^{(j)}-\beta_k^{(j)}\bb{v}_k^{(j)}) + \Delta \bb{X}_k^{(j-1)}\hat{\beta}_k^{(j)}\hat{\bb{v}}_k^{(j)} \|_2 \\
%%DIF > 	&\leq \|\bb{X}_k^{(j)-1}(\hat{\beta}_k^{(j)}\hat{\bb{v}}_k^{(j)}-\beta_k^{(j)}\bb{v}_k^{(j)})\|_2 + \|\Delta \bb{X}_k^{(j-1)}\hat{\beta}_k^{(j)}\hat{\bb{v}}_k^{(j)}\|_2 \\
%%DIF > &\leq \tilde{\gamma}_{m-(k-1)r}\|\bb{X}_k^{(j-1)}\|_2\|\beta_k^{(j)}\bb{v}_k^{(j)}\|_2 + \|\Delta \bb{X}_k^{(j-1)}\|_F\|\hat{\beta}_k^{(j)}\hat{\bb{v}}_k^{(j)}\|_2 \\
%}\end{align*}
%\DIFadd{Componentwise bounds follow immediately, and are summarized in \cref{lem:BQR-build}.
%}\begin{lemma}\label{lem:BQR-build}
%	\DIFadd{Consider the construction of the WY representation for the $k^{th}$ partition of matrix $\bb{A}\in\R^{m\times n}$ given a set of HH constants and vectors, $\{\beta_k^{(j)}\}_{j=1}^r$ }\DIFaddend and \DIFdelbegin \DIFdel{propagate it through the for-loop in Algorithm~\ref{algo:hhQR}.
%This process then results in forward error bound coefficients $n\epsilon$ or $n^{3/2}\epsilon$.
%Since this $\epsilon$ value remains constant, the rounding error analysis for both mixed-precision and uniform-precision schemes are essentially the same with different values for $\epsilon$.
%	The uniform precision equivalent of Equation~\ref{eqn:applyP} is shown in Equation~\ref{eqn:applyPup}, }\begin{displaymath}
%\DIFdel{\fl(\bb{P}_{\bb{v}}\bb{x})= (\bb{P}_{\bb{v}} + \bb{\Delta P_{v}})\bb{x},\qquad \|\bb{\Delta P_v}\|_F \leq \tilde{\gamma}^{(m)},
%%DIFDELCMD < \label{eqn:applyPup}%%%
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdel{which is derived in detail in \cite{Higham2002}.
%Therefore, we }\DIFdelend \DIFaddbegin \DIFadd{$\{\bb{v}_{k}^{(j)}\}$ via \cref{algo:buildWY}.
%	Then, 
%	}\begin{equation}
%	\DIFadd{\hat{\bb{z}}_{k}^{(j)} = \bb{z}_{k}^{(j)} + \Delta \bb{z}_{k}^{(j)},\;\; |\Delta \bb{z}_{k}^{(j)}| \leq j\tilde{\gamma}_{m-(k-1)r} |\bb{z}_{k}^{(j)}|,\;\; \|\Delta \bb{z}_k^{(j)}\|_2 \leq j\tilde{\gamma}_{m-(k-1)r}\|\bb{z}_k^{(j)}\|_2.\label{eqn:BQR-z}
%	%DIF > \\
%	%DIF > 		\hat{\bb{v}}_{k}^{(j)}& = \bb{v}_{k}^{(j)} + \Delta \bb{v}_{k}^{(j)},\;\; |\Delta \bb{v}_{k}^{(j)}| \leq \tilde{\gamma}_{m-(k-1)r} |\bb{v}_{k}^{(j)}|\label{eqn:BQR-v},
%	}\end{equation}
%	%DIF > where the second bound is derived from \cref{eqn:vbound}.
%\end{lemma}
%\DIFadd{Most importantly, this shows that constructing the WY update is just as numerically stable as applying successive HH transformations (see Section 19.5 of \cite{Higham2002}).
%}
%
%\paragraph{\DIFadd{Update blocks to the right: line 5 of \cref{algo:blockHQR}}}
%\DIFadd{We now consider applying $\bb{X}_{k}:=\bb{I}-\bb{W}_k\bb{Y}_k^{\top}$ to some matrix, $\bb{B}$.
%In practice, $\bb{B}$ is the bottom right submatrix, $[\bb{C}_{k+1}\cdots \bb{C}_{N}][(k-1)r+1:m,:]$.
%%DIF > In practice, this step is performed with a level-3 BLAS operation. 
%%DIF > Regardless, 
%We can apply \cref{eqn:deltX} directly to the columns of $\bb{B}$, 
%}\begin{align}
%\DIFadd{\|\fl(\hat{\bb{X}}_k \bb{B}[:,j])\|_2 = \|\fl(\hat{\bb{X}}_k^{(r)} \bb{B}[:,j])\|_2 \leq r\tilde{\gamma}_{m-(k-1)r} \|\bb{B}[:,j]\|_2
%}\end{align}
%\DIFadd{A normwise bound for employing a general matrix-matrix multiplication operation is stated in section 19.5 of \cite{Higham2002}.
%%DIF > \begin{align*}
%%DIF > %\fl(\hat{\bb{Y}}_k^{\top}\bb{b}) &= (\hat{\bb{Y}}_k + \tilde{\Delta} \bb{Y}_k)^{\top}\bb{b},\\
%%DIF > \hat{\bb{y}}_k&=\fl(\hat{\bb{X}}_{k}\bb{b})=\fl(\bb{b} - \fl(\hat{\bb{W}}_{k}\fl(\hat{\bb{Y}}_{k}^{\top}\bb{b}))) \\
%%DIF > &=(1+\dd)(\bb{b} - (\hat{\bb{W}}_{k} + \tilde{\Delta} \bb{W}_{k}) (\hat{\bb{Y}}_{k} + \tilde{\Delta} \bb{Y}_{k})^{\top}\bb{b}),\\
%%DIF > &=\bb{X}_k\bb{b}+\Delta\bb{y}_k= (\bb{X}_{k} + \Delta \bb{X}_{k})\bb{b},
%%DIF > \end{align*}
%%DIF > where $\tilde{\Delta} \bb{W}_{k}$ and $\tilde{\Delta} \bb{Y}_{k}$ each represent the backward error for a matrix-vector multiply with inner products of lengths $m-(k-1)r$ and $r$ and $\hat{\bb{W}}_{k},\hat{\bb{Y}}_k$ include the errors from forming the WY representation (see \cref{lem:BQR-build}).
%%DIF > %= \bb{W}_{k}+\Delta \bb{W}_{k}$, $\hat{\bb{Y}}_{k} = \bb{Y}_{k}+\Delta \bb{Y}_{k}$ where the columns of $\Delta \bb{W}_{k}$,$\Delta \bb{Y}_{k}$ are bounded by \cref{eqn:BQR-z,eqn:BQR-v}.
%%DIF > Since $|\tilde{\Delta} \bb{W}_{k}| \leq \gamma_{m-(k-1)r}|\hat{\bb{W}}_{k}|$ and $|\tilde{\Delta} \bb{Y}_{k}| \leq \gamma_{r}|\hat{\bb{Y}}_{k}|$, they are small compared to the errors from forming $\hat{\bb{W}}_k$,$\hat{\bb{Y}}_k$, and we result in
%%DIF > \begin{equation*}
%%DIF > \|\Delta \bb{y}_k\|_2= \|\fl(\hat{\bb{X}}_{k}\bb{b}) - \bb{X}_k\bb{b}\|_2 \leq  r\tilde{\gamma}_{m-(k-1)r}\|\bb{b}\|_2.
%%DIF > %\left(|\bb{b}_j|+|\hat{\bb{W}}_k||\hat{\bb{Y}}_k|^{\top}|\bb{b}_j|\right),
%%DIF > \end{equation*}
%%DIF > where $\tilde{\gamma}_{\bb{X}_{k}}$ accounts for the errors from perturbations $\tilde{\Delta}\bb{W}_k+\Delta \bb{W}_k$, $\tilde{\Delta}\bb{Y}_k + \Delta \bb{Y}_k$, and $\dd$.
%%DIF > In uniform precision, this is largely derived from 
%%DIF > \begin{align*}
%%DIF > 	\fl(\hat{\bb{W}}_k\fl(\hat{\bb{Y}}_k^{\top}\bb{b}_j)) &= (1+\tth_{m-(k-1)r})\hat{\bb{W}}_k\left[(1+\tth_{r}) \hat{\bb{Y}}_k^{\top}\bb{b}_j)\right] \\
%%DIF > 	&= (1+\tth_{m-(k-2)r})\hat{\bb{W}}_k\hat{\bb{Y}}_k^{\top}\bb{b}_j,\\
%%DIF > 	\|\hat{\bb{W}}_k\hat{\bb{Y}}_k^{\top}\bb{b}_j - \bb{W}_k\bb{Y}_k\bb{b}_j\|_2 &= \left(\sum_{j=1}^r \left(\bb{v}_k^{(j)}+\Delta \bb{v}_k^{(j)}\right)^{\top}\bb{b}_j\left(\bb{z}_k^{(j)}+\Delta \bb{z}_k^{(j)}\right)\right)-\left(\sum_{j=1}^r\bb{v}_k^{(j)\top}\bb{b}_j\bb{z}_k^{(j)}\right)
%%DIF > \end{align*}
%%DIF > \begin{align*}
%%DIF > |(\bb{W}_{k} +\Delta \bb{W}_{k} &+ \tilde{\Delta} \bb{W}_{k}) (\bb{Y}_{k} + \Delta \bb{Y}_{k} + \tilde{\Delta} \bb{Y}_{k})^{\top}\bb{b}_j - \bb{W}_{k} \bb{Y}_{k}^{\top}\bb{b}_j| \\
%%DIF > &\leq \left[(1+r\tilde{\gamma}_{m-(k-1)r}+ \gamma_{r})(1 + \tilde{\gamma}_{m-(k-1)r}+ \gamma_{m-(k-1)r})-1\right]|\bb{W}_k||\bb{Y}_k|^{\top}|\bb{b}_j| \\ 
%%DIF > &\leq r\tilde{\gamma}_{m-(k-1)r}|\bb{W}_k||\bb{Y}_k|^{\top}|\bb{b}_j|,\;\; \gamma_{\bb{X}_{k}}:= r\tilde{\gamma}_{m-(k-1)r},
%%DIF > \end{align*}
%%DIF > since the subtraction step only adds a single rounding error. 
%%DIF >  I have used that $\bb{W}_{k}$ accrues additional rounding errors compared to $\bb{Y}_{k}$.
%%DIF > Note that we implicitly covered the same step of applying an WY update in the construction of $\bb{z}_{k}^{(j)}$, but used \cref{lem:19.3} instead since we were concerned with the error occurred at a single update. 
%%DIF > Now, we leave this error in a general form to allow for different mixed precision settings in \cref{sec:mpanalysis}.
%%DIF > As a result of introducing generality, 
%%DIF > Since $\Delta \bb{X}_k = \Delta\bb{y}_k\bb{b}^{\top}/\|\bb{b}\|_2^2$, we conclude with a backward matrix norm bound, 
%%DIF > \begin{equation}
%%DIF > \fl(\hat{\bb{X}}_{k}\bb{b}) = (\bb{X}_{k} + \Delta \bb{X}_{k})\bb{b}, \;\; \|\Delta \bb{X}_{k}\|_F \leq r\tilde{\gamma}_{m-(k-1)r}.\label{eqn:Xkbound}
%%DIF > \end{equation}
%%DIF > We showed earlier in this section that HQR performed on $\bb{C}_{k}$ accrues error of order $\tilde{\gamma}_{m-(k-1)r}$ by applying \cref{thm:feHQR}, and the building of the W factor, $r\tilde{\gamma}_{m-(k-1)r}$ order error.
%%DIF > These combined with the error in \cref{eqn:BQR} still amount to $cO(r\tilde{\gamma}_{m-(k-1)r})$, so we conclude that.
%%DIF >  is stated in \cref{lem:WYnorm}.
%%DIF > \begin{lemma}\label{lem:WYnorm}
%%DIF > 	Consider applying the WY transformation of \cref{lem:WY} to a matrix $\bb{B}\in\R^{m\times n-r}$ as would be done at the first iteration of line  in \cref{algo:blockHQR}.	%The WY factors $\hat{\bb{W}},\hat{\bb{Y}}\in\R^{m\times r}$ formed via \cref{algo:buildWY} are slight perturbations of their exact arithmetic variants.	%	Suppose that 
%%DIF > 	\begin{equation*}	
%%DIF > 	\hat{\bb{Q}}:=\fl(\bb{I}-\hat{\bb{W}}\hat{\bb{Y}}^{\top}) = \bb{U} + \Delta\bb{U},\quad
%%DIF > 		\hat{\bb{W}} = \bb{W} + \Delta\bb{W},\quad
%%DIF > 		\hat{\bb{W}} = \bb{Y} + \Delta\bb{Y},
%%DIF > 	\end{equation*}
%%DIF > 	where $\bb{U}$ is orthogonal and 
%%DIF > 	\begin{equation*}
%%DIF > 		\|\Delta\bb{U}\|_2 \leq ud_1, \quad
%%DIF > 		\|\Delta\bb{W}\|_2  \leq d_2, \quad
%%DIF > 		\|\Delta\bb{Y}\|_2 \leq d_3.
%%DIF > 	\end{equation*}
%%DIF > 	Finally, we denote the rounding errors incurred from a level-3 BLAS matrix-matrix multiplication of matrices of sizes $p$-by-$t$ and $t$-by-$q$ to be represented by a function, $c(p,t,q)$.
%%DIF > 	Then, 
%%DIF > 	\begin{align}
%%DIF > 		\|\fl(\hat{\bb{Q}}\bb{B}) - \bb{QB}\|_2 \leq u\left[1+d_1+d_2d_3\left(1+c(m,r,n-r)+c(r,m,n-r)\right)\right] \|\bb{B}\|_2 + \cO(u^2).
%%DIF > 	\end{align}
%%DIF > \end{lemma}
%}\paragraph{\DIFadd{Multiple WY updates: line 8-9 of \cref{algo:blockHQR}}}
%\DIFadd{All that remains is to consider the application of successive WY updates to form the QR factorization computed with BQR denoted as $\bb{Q}_{BQR}$ and $\bb{R}_{BQR}$. 
%We can apply \cref{lem:3.7} directly by setting $\bb{X}_{k}:= \bb{I}-\bb{W}_{k}\bb{Y}_{k}^{\top}$ and consider the backward errors for applying the sequence to a vector, $\bb{x}\in\R^{m}$, as we did for \cref{lem:19.3}. 
%Since $\bb{X}_{k}=\bb{P}_{(k-1)r+1}\cdots\bb{P}_{kr}$, is simply a sequence of HH transformations, it is orthogonal, i.e. $\|\bb{X}_{k}\|_2=1$.
%We }\DIFaddend only need to \DIFdelbegin \DIFdel{compare $\gamma^{(6d+6z+13)}$ against $\gamma^{(cm)}$, where $c$ is a small integer. 
%Although $d$ relies on both $m$ and the precisions $w$ and $s$, we can generally assume that $cm\gg (6d+6z+13)$ in most mixed-precision settings.
%%DIF < , where $c$ is a small integer constant. 
%Therefore, the new bounds in Theorem~\ref{thm:feHQR} are much tighter than the existing ones and more accurately describe the kind of rounding error accumulated in mixed-precision computational settings }\DIFdelend \DIFaddbegin \DIFadd{replace with $\bb{x}$ with $\bb{A}[:,i]$'s to form the columnwise bounds for $\bb{R}_{BQR}$, and apply the transpose to $\hat{\bb{e}}_i$'s to form the bounds for $\bb{Q}_{BQR}$}\DIFaddend . 
%%DIF < up to the level of a single Householder transformation as is shown in Equation~\ref{eqn:applyP}.
%%DIF < 
%%DIF < All steps to form the error bounds in Theorem~\ref{thm:feHQR} from the error bound for a single Householder transformation (c.f. Equation~\ref{eqn:applyP}) result in coefficients of $\tilde{\gamma}_w^{(6d+6z+13)}$, and these steps directly follow the analyses in Section 19.3 of \cite{Higham2002}.
%%DIF < In these steps, we generalize the single Householder transformation error bound, 
%%DIF > We define the backward error as $\Delta \bb{Q'}_{BQR}) \bb{b}$.
%%DIF > Then, this backward error is quantified via
%\DIFaddbegin \DIFadd{Then, 
%}\begin{align}
%\DIFadd{\left|\left|\prod_{k=1}^N (\bb{X}_{k} + \Delta \bb{X}_{k})- \prod_{k=1}^N\bb{X}_{k} \right|\right|_F }&\DIFadd{\leq\left(-1+\sum_{k=1}^N (1+r\tilde{\gamma}_{m-(k-1)r})\right) \leq rN\tilde{\gamma}_m \equiv n\tilde{\gamma}_m ,\label{eqn:BQR-mp}}\\%DIF > &\leq \sum_{k=1}^N (1+ r\tilde{\gamma}_{m-(k-1)r})-1 \leq rN\tilde{\gamma}_m \equiv n\tilde{\gamma}_m ,\label{eqn:BQR-mp}\\
%\DIFadd{\|\hat{\bb{Q}}_{BQR}-\bb{Q}\|_F}&\DIFadd{\leq n^{3/2}\tilde{\gamma}_m. \label{eqn:BQR}
%}\end{align}
%%DIF > \begin{align}
%%DIF > \Delta \bb{Q'}_{BQR} &:= \prod_{k=N}^{1}\left(\bb{Q}_{k} +\Delta \bb{Q}_{k}\right) - \prod_{k=N}^{1}\bb{Q}_{k} ,\\
%%DIF > %\|\Delta\bb{Q}'^{\top}\|_F &= \left|\left| \prod_{k=1}^{N}\left(\bb{Q}_k +\Delta\bb{P}_i\right) - \prod_{i=r}^{1}\bb{P}_i \right|\right|_F,\\
%%DIF > \|\Delta \bb{Q'}_{BQR}\| & \leq \sum_{k=N}^1 (1+\tilde{\gamma}_{\bb{Q}_{k})  -1 \label{eqn:BQR-mp}\\
%%DIF > &\leq \sum_{k=N}^1 (1+ r\tilde{\gamma}_{m-(k-1)r})-1 \leq rN\tilde{\gamma}_m \equiv n\tilde{\gamma}_m \label{eqn:BQR}.
%%DIF > \end{align}
%\DIFadd{We can also form the normwise bound for the $j'^{\ th}$ column of $\hat{\bb{Q}}_{BQR},\hat{\bb{R}}_{BQR}$. 
%If we let $k' = \lceil j'/r\rceil^{th}$, then the $j'^{\ th}$ column is the result of applying $k'-1$ WY updates and an additional HQR. 
%Applying \cref{lem:3.7} yields 
%}\begin{align}
%\DIFadd{\|\Delta \bb{R}_{BQR}[:,j']\|_2 \leq rk'\tilde{\gamma}_{m} \|\bb{A}[:,j']\|_2,}&\DIFadd{\;\; \|\Delta \bb{R}_{BQR}\|_F \leq n\tilde{\gamma}_{m} \|\bb{A}\|_F}\\
%\DIFadd{\|\Delta \bb{Q}_{BQR}[:,j']\|_2 \leq rk'\tilde{\gamma}_{m},}&\DIFadd{\;\;\|\Delta \bb{Q}_{BQR}\|_F = r\tilde{\gamma}_{m}\sum_{j=1}^n \lceil j/r\rceil = n^{3/2}\tilde{\gamma}_{m}.\label{eqn:BQRmat}
%}\end{align}
%\DIFadd{and near orthogonality of the $\bb{Q}$ factor is still achieved.
%}\DIFaddend %\begin{equation}
%%DIF < \fl(\bb{P}_{\bb{v}}\bb{x})= (\bb{P}_{\bb{v}} + \bb{\Delta P_{v}})\bb{x},\qquad \|\bb{\Delta P_v}\|_F \leq \epsilon,\label{eqn:applyPgen}
%%DIF > 	\|\Delta \bb{Q}_{BQR}\|_F = r\tilde{\gamma}_{m}\sum_{j=1}^n \lceil j/r\rceil = n^{3/2}\tilde{\gamma}_{m}.\label{eqn:BQRmat}
%%\end{equation}
%%DIF < for some small quantity $0<\epsilon\ll 1$, and propagate it through the for-loop in Algorithm~\ref{algo:hhQR}. 
%%DIF < Since this $\epsilon$ value remains constant, the rounding error analysis for both mixed-precision and uniform-precision schemes are essentially the same with different values for $\epsilon$.
%%DIF < Therefore, we only need to compare $\gamma^{(6d+6z+13)}$ against $\tilde{\gamma}^{(m)}$. 
%%DIF < Although $d$ relies on both $m$ and the precisions $w$ and $s$, we can generally assume that $m\gg (6d+6z+13)$ in most mixed-precision settings. 
%%DIF < Therefore, new bounds in Theorem~\ref{thm:feHQR} are much tighter than the existing ones, and more accurately describe the kind of rounding error accumulated in mixed-precision computational settings.
%\DIFaddbegin \paragraph{\DIFadd{BQR and HQR error bound comparison}}
%\DIFadd{BQR under exact arithmetic is equivalent to HQR, and it is often referred to as the level-3 BLAS version of HQR. 
%Furthermore, the error analysis of this section shows that BQR is as numerically stable as HQR despite requiring more FLOPs.
%In fact, many linear algebra libraries such as LAPACK use a variant of BQR as the QR factorization algorithm (see }{\tt \DIFadd{dgeqrf}} \DIFadd{of \cite{LAPACK}).
%}\DIFaddend 
%
%\DIFdelbegin \section{\DIFdel{Tall-and-Skinny QR}}
%%DIFAUXCMD
%\addtocounter{section}{-1}%DIFAUXCMD
%\DIFdelend \DIFaddbegin \DIFadd{The primary goal of the analysis presented in this section is to provide the basic skeleton for the standard BQR rounding error analysis to make the generalization to mixed precision settings in \cref{sec:mpanalysis} easier.
%Readers should refer to \cite{golub2013matrix,Higham2002} for full details.
%%DIF > \input{algo-TSQR}
%}\subsection{\DIFadd{Block HQR with partitioned rows : Tall-and-Skinny QR (TSQR)}}\DIFaddend \label{sec:TSQR}
%Some important problems that require QR factorizations of overdetermined systems include least squares problems, eigenvalue problems, low rank approximations, as well as other matrix decompositions.
%Although Tall-and-Skinny QR (TSQR) broadly refers to \DIFdelbegin \DIFdel{row-block }\DIFdelend \DIFaddbegin \DIFadd{block }\DIFaddend QR factorization methods \DIFaddbegin \DIFadd{with row partitions}\DIFaddend , we will discuss a specific variant of TSQR which is also known as the AllReduce algorithm \cite{Mori2012}.
%In this paper, the TSQR/AllReduce algorithm refers to the most parallel variant of \DIFdelbegin \DIFdel{all row-block }\DIFdelend \DIFaddbegin \DIFadd{the block }\DIFaddend QR factorization algorithms discussed in \cite{Demmel2012}.
%A detailed description and rounding error analysis of this algorithm can be found in \cite{Mori2012}, and we present a pseudocode for the algorithm in \DIFdelbegin \DIFdel{Algorithm~\ref{algo:par_tsqr}}\DIFdelend \DIFaddbegin \DIFadd{\cref{algo:par_tsqr}}\DIFaddend .
%Our initial interest in this algorithm came from its parallelizable nature, which is particularly suitable to implementation on GPUs. 
%Additionally, our numerical simulations (discussed in \DIFdelbegin \DIFdel{Section~\ref{sec:NE}}\DIFdelend \DIFaddbegin \DIFadd{\cref{sec:NE}}\DIFaddend ) show that TSQR can not only increase the speed but also outperform the traditional HQR factorization in low precisions.
%\DIFdelbegin \subsection{\DIFdel{TSQR/AllReduce Algorithm}}
%		%DIFAUXCMD
%\addtocounter{subsection}{-1}%DIFAUXCMD
%\DIFdel{Algorithm~\ref{algo:par_tsqr} takes }\DIFdelend \DIFaddbegin \subsubsection{\DIFadd{TSQR/AllReduce Algorithm}}
%\DIFadd{\Cref{algo:par_tsqr} partitions the rows of }\DIFaddend a tall-and-skinny matrix, $\bb{A}$\DIFdelbegin \DIFdel{, and organizes it into row-blocks. 
%HQR factorization }\DIFdelend \DIFaddbegin \DIFadd{. 
%HQR }\DIFaddend is performed on each of those blocks \DIFdelbegin \DIFdel{, }\DIFdelend and pairs of $\bb{R}$ factors are combined to form the next set of $\bb{A}$ matrices to be QR factorized. 
%This process is repeated until only a single $\bb{R}$ factor remains, and the $\bb{Q}$ factor is built from all of the \DIFdelbegin \DIFdel{Householder }\DIFdelend \DIFaddbegin \DIFadd{HH }\DIFaddend constants and vectors stored at each level.
%The most gains from parallelization can be made in the initial level where the maximum number of independent HQR factorizations occur. 
%Although more than one configuration of this algorithm may be available for a given tall-and-skinny matrix, the number of nodes available and the shape of the matrix eliminate some of those choices. 
%For example, a 1600-by-100 matrix can be partitioned into 2, 4, 8, or 16 initial row-blocks but may be restricted by a machine with only 4 nodes, and a 1600-by-700 matrix can only be partitioned into 2 initial blocks.
%%DIF < The choice in the initial partition determine the recursion depth which we call level.
%		Our numerical experiments show that the choice in the initial partition, which directly relates to the recursion depth of TSQR, has an impact in the accuracy of the QR factorization. \par
%%DIF < Our numerical experiments provide some insight into how to make this decision in an optimal way.  \par
%
%We refer to \emph{level} as the number of recursions in a particular TSQR implementation. 
%An $L$-level TSQR algorithm partitions the original matrix into \DIFdelbegin \DIFdel{$2^L$ }\DIFdelend \DIFaddbegin \DIFadd{$2^{(l)}$ }\DIFaddend submatrices in the initial or $0^{th}$ level of the algorithm, and $2^{L-i}$ QR factorizations are performed in level $i$ for $i = 1 , \cdots, L$. 
%The set of matrices that are QR factorized at each level $i$ are called $\bb{A}_j^{(i)}$ for $j = 1, \cdots, 2^{L-i}$, where superscript $(i)$ corresponds to the level and the subscript $j$ indexes the row-blocks within level $i$.
%%DIF < Note that each $\bb{A}_i^{(j)}$ is created from combining two $\bb{R}$ factors in the previous level, $i-1$.
%		%DIF < ,  $\bb{A}$ matrices that are QR factorized.
%		%DIF < The initial row-blocks that partition the original matrix is the initial or $0^{th}$ level of the algorithm, and each successive set of $\bb{A}_i^{(j)}$ matrices (created from combining $\bb{R}$ factors of the previous level) are referred to as first level, second level, and so forth.
%		%DIF < The subscript, $i$ corresponds to the level, and the superscript $(j)$ indexes the row-blocks within level $i$.
%		In the following sections, \DIFdelbegin \DIFdel{Algorithm~\ref{algo:par_tsqr} }\DIFdelend \DIFaddbegin \DIFadd{\cref{algo:par_tsqr} }\DIFaddend ({\tt tsqr}) will find a TSQR factorization of a matrix $A\in\R^{m\times n}$ where $m \gg n$. 
%The inline function {\tt qr} refers to \DIFdelbegin \DIFdel{Algorithm~\ref{algo:hhQR}, }%DIFDELCMD < {\tt %%%
%\DIFdel{hh\_mult}%DIFDELCMD < } %%%
%\DIFdel{is Algorithm~\ref{algo:hh_mult}, }\DIFdelend \DIFaddbegin \DIFadd{\cref{algo:hhQR} }\DIFaddend and we use \DIFdelbegin \DIFdel{Algorithm ~\ref{algo:hh_v2} }\DIFdelend \DIFaddbegin \DIFadd{\cref{algo:hh_v2} }\DIFaddend as a subroutine of {\tt qr}.
%
%%DIF < 		performs a HQR factorization and returns $\bb{V} \in \R^{m\times n}$, $\bm{\beta}\in\R^{n}$, and $\bb{R} \in R^{n\times n}$.
%%DIF < 		For $i=1,\cdots,n$, the $i^{th}$ column of $\bb{V}$ and $\bm{\phi}_i$ are the Householder vector and constant that defines the $i^{th}$ Householder transformation matrix, $\bb{P}_i$ for the QR decomposition of the input matrix. 
%%DIF < 		The columns of $\bb{V}$ are the Householder vectors (first component normalized to $1$) that can form the matrix $\bb{Q}_{\text{thin}} = \bb{P}_1 \cdots \bb{P}_nI_{m\times n}$.
%%DIF < 		Note that a full $\bb{Q}$ can be constructed via $\bb{Q}_{\text{full}}=\bb{P}_1\cdots \bb{P}_n$. 
%%DIF < 		
%%DIF < 		
%%DIF < 		Algorithm~\ref{algo:hh_mult} is the implementation of multiplying  $\bb{Q}:= \bb{P}_1 \cdots \bb{P}_n$ to another matrix or vector, when only the householder vectors to construct $\bb{P}_i$'s are given. This takes advantage of the special property of householder matrices-- $\bb{P}_i$'s are rank-one updates of the identity. let $\bb{B}\in\R^{m\times d}$. The straightforward mod of  computing $\bb{Q}\bb{B}$ costs $\mathcal{O}(m^2d)$ where the costs of constructing $\bb{Q}$ itself is ignored. However,  Algorithm ~\ref{algo:hh_mult} describes a method that is only $\mathcal{O}(mnd)$. 
%\DIFdelbegin %DIFDELCMD < 
%
%%DIFDELCMD < 		%%%
%\subsubsection{\DIFdel{TSQR Notation}}
%		%DIFAUXCMD
%\addtocounter{subsubsection}{-1}%DIFAUXCMD
%\DIFdel{We will }\DIFdelend \DIFaddbegin \paragraph{\DIFadd{TSQR Notation}}
%\DIFadd{We }\DIFaddend introduce new notation due to the multi-level nature of the TSQR algorithm.
%In the final task of constructing $\bb{Q}$, $\bb{Q}_j^{(i)}$ factors are aggregated from each block at each level.
%Each $\bb{Q}_j^{(i)}$ factor from level $i$ is partitioned such that two corresponding $\bb{Q}^{(i-1)}$ factors from level $i-1$ can be applied to them. 
%The partition (approximately) splits $\bb{Q}_{j}^{(i)}$ into two halves, $[\tilde{\bb{Q}}_{j, 1}^{(i)\top} \tilde{\bb{Q}}_{j, 2}^{(i)\top}]^{\top}$.
%%DIF <  \(\bb{Q}_{j}^{(i)} = \begin{bmatrix}
%		%DIF < \tilde{\bb{Q}}_{j, 1}^{(i)}\\ 
%		%DIF < \tilde{\bb{Q}}_{j, 2}^{(i)} 
%		%DIF < \end{bmatrix},\)
%		The functions $\alpha(j)$ and $\phi(j)$ are defined such that $\bb{Q}_j^{(i)}$ is applied to \DIFaddbegin \DIFadd{the correct blocks from the level below: }\DIFaddend $\tilde{\bb{Q}}_{\alpha(j), \phi(j)}^{(i+1)}$.
%For $j = 1 , \cdots, 2^{L-i}$ at level $i$, we need $j = 2(\alpha(j)-1) + \phi(j)$, where $\alpha(j) = \lceil \frac{j}{2}\rceil$ and \DIFdelbegin \DIFdel{$\phi(j) = 2 + j - 2\alpha(j)$.
%%DIF < 		\begin{itemize}
%%DIF < 			\item $\alpha(j) = \lceil \frac{j}{2}\rceil $ and
%%DIF < 			\item $\phi(j) = 2 + j - 2\alpha(j)$.
%%DIF < 		\end{itemize} 
%		Section~\ref{Qdetails} }\DIFdelend \DIFaddbegin \DIFadd{$\phi(j) = 2 + j - 2\alpha(j) \in\{1,2\}$.
%\cref{Qdetails} }\DIFaddend shows full linear algebra details for a single-level ($L=1$, $2$ initial blocks) example.
%The reconstruction of $\bb{Q}$ can be implemented more efficiently (see \cite{BDGJNS2014}), but the reconstruction method in \DIFdelbegin \DIFdel{Algorithm~\ref{algo:par_tsqr} }\DIFdelend \DIFaddbegin \DIFadd{\cref{algo:par_tsqr} }\DIFaddend is presented for a clear, straightforward explanation.
%
%\DIFaddbegin \input{TSQRALGO}
%\DIFaddend \subsubsection{Single-level Example}
%\label{Qdetails}
%In the single-level version of this algorithm, we first bisect $\bb{A}$  into $\bb{A}_1^{(0)}$ and $\bb{A}_2^{(0)}$ and compute the QR factorization of each of those submatrices.
%We combine the resulting upper-triangular matrices \DIFdelbegin \DIFdel{, i.e.,  %
%\mbox{%DIFAUXCMD
%\(\bb{A}_{1}^{(1)} =\begin{bmatrix}
%		\bb{R}_{1}^{(0)}\\ 
%		\bb{R}_{2}^{(0)} 
%		\end{bmatrix},\)
%}%DIFAUXCMD
%}\DIFdelend \DIFaddbegin \DIFadd{(see below)
%%DIF >  , i.e.,  \(\bb{A}_{1}^{(1)} =\begin{bmatrix}
%%DIF > \bb{R}_{1}^{(0)}\\ 
%%DIF > \bb{R}_{2}^{(0)} 
%%DIF > \end{bmatrix},\)   
%}\DIFaddend which is QR factorized, and the process is repeated:
%\[
%\bb{A} = \begin{bmatrix}
%\bb{A}_1^{(0)}\\
%\bb{A}_2^{(0)}
%\end{bmatrix} = \begin{bmatrix}
%\bb{Q}_1^{(0)}\bb{R}_1^{(0)}\\
%\bb{Q}_2^{(0)}\bb{R}_2^{(0)}
%\end{bmatrix} = \begin{bmatrix}
%\bb{Q}_1^{(0)} & \bb{0}\\
%\bb{0} & \bb{Q}_2^{(0)}
%\end{bmatrix} \begin{bmatrix}
%\bb{R}_1^{(0)} \\
%\bb{R}_2^{(0)}
%\end{bmatrix} =\begin{bmatrix}
%\bb{Q}_1^{(0)} & \bb{0}\\
%\bb{0} & \bb{Q}_2^{(0)}
%\end{bmatrix} \bb{A}_1^{(1)} =\begin{bmatrix}
%\bb{Q}_1^{(0)} & \bb{0}\\
%\bb{0} & \bb{Q}_2^{(0)}
%\end{bmatrix} \bb{Q}_1^{(1)}\bb{R}.
%%DIF < _1^{(1)}
%		\] 
%The $\bb{R}$ factor of $\bb{A}_1^{(1)}$ is the final $\bb{R}$ factor of the QR factorization of the original matrix, $\bb{A}$. 
%However, the final $\bb{Q}$ still needs to be constructed.
%Bisecting  $\bb{Q}_1^{(1)}$ into two submatrices, i.e. $\tilde{\bb{Q}}_{1,1}^{(1)}$ and $\tilde{\bb{Q}}_{1,2}^{(1)}$, allows us to write and compute the product more compactly,  \[
%\bb{Q}:=\begin{bmatrix}
%\bb{Q}_1^{(0)} & \bb{0}\\
%\bb{0} & \bb{Q}_2^{(0)}
%\end{bmatrix} \bb{Q}_1^{(1)} =    \begin{bmatrix}
%\bb{Q}_1^{(0)} & \bb{0}\\
%\bb{0} & \bb{Q}_2^{(0)}
%\end{bmatrix} \begin{bmatrix}
%\tilde{\bb{Q}}_{1,1}^{(1)}\\
%\tilde{\bb{Q}}_{1,2}^{(1)}
%\end{bmatrix}= \begin{bmatrix}
%\bb{Q}_1^{(0)}\tilde{\bb{Q}}_{1,1}^{(1)} \\ 
%\bb{Q}_2^{(0)}\tilde{\bb{Q}}_{1,2}^{(1)}
%\end{bmatrix}. \]
%More generally, \DIFdelbegin \DIFdel{Algorithm~\ref{algo:par_tsqr} }\DIFdelend \DIFaddbegin \DIFadd{\cref{algo:par_tsqr} }\DIFaddend takes a tall-and-skinny matrix $\bb{A}$ and level $L$ and finds a QR factorization by initially partitioning $\bb{A}$ into \DIFdelbegin \DIFdel{$2^L$ }\DIFdelend \DIFaddbegin \DIFadd{$2^{(l)}$ }\DIFaddend row-blocks and includes the building of $\bb{Q}$.
%\DIFdelbegin %DIFDELCMD < 
%
%%DIFDELCMD < 		\begin{algorithm2e}[H]
%%DIFDELCMD < 			\DontPrintSemicolon %%%
%%DIF <  Some LaTeX compilers require you to use \dontprintsemicolon instead
%			%DIFDELCMD < \KwIn{$\bb{A}\in\R^{m \times n}$ where $m \gg n$, $L\leq\lfloor\log_2\left(\frac{m}{n}\right)\rfloor$, and $2^L$ is the initial number of blocks. }
%%DIFDELCMD < 			
%
%%DIFDELCMD < 			\KwOut{$\bb{Q}\in\R^{m \times n}$, $\bb{R} \in\R^{n\times n}$ such that 	$\bb{Q}\bb{R} = \bb{A}$.}
%%DIFDELCMD < 			%%%
%\DIFdel{$h \gets \lfloor \frac{m}{2^L} \rfloor$ }%DIFDELCMD < \tcp*{Number of rows for all but the last block.}
%%DIFDELCMD < 			%%%
%\DIFdel{$r \gets m - (2^L-1)h$ }%DIFDELCMD < \tcp*{Number of rows for the last block ($h\leq r <2h$).}
%%DIFDELCMD < 			\tcc{Split $\bb{A}$ into $2^L$ blocks. Note that level $(i)$ has $ 2^{L-i}$ blocks.}
%%DIFDELCMD < 			\For %%%
%\DIFdelend \DIFaddbegin \DIFadd{For simplicity, we assume that $m$ is exactly $h2^{(l)}$ so that the initial partition yields $2^{(l)}$ blocks of equal sizes, $h$-by-$n$. 
%Also, note that }\DIFaddend {\DIFdelbegin \DIFdel{$j = 1 : 2^L-1$}\DIFdelend \DIFaddbegin \tt \DIFadd{hh\_mult}\DIFaddend } \DIFdelbegin %DIFDELCMD < {
%%DIFDELCMD < 				%%%
%\DIFdel{$\bb{A}_j^{(0)} \gets \bb{A}[(j-1)h+1: jh, :]$ %DIF < \bb{I}_{(j-1)h, jh}^{\top}\bb{A}$
%			}%DIFDELCMD < }
%%DIFDELCMD < 			%%%
%\DIFdel{$\bb{A}_{2^L}^{(0)} \gets \bb{A}[(2^L-1)h:m, :]$ }%DIFDELCMD < \tcp*{Last block may have more rows.} %%%
%%DIF < \bb{I}_{(2^L-1)h, m}^{\top}\bb{A}
%			%DIFDELCMD < \tcc{Store Householder vectors as columns of matrix $\bb{V}_j^{(i)}$, Householder constants as components of vector $\bm{\beta}_j^{(i)}$, and set up the next level.}
%%DIFDELCMD < 			\For{$i = 0 : L-1$}{
%%DIFDELCMD < 				\tcc{The inner loop can be parallelized.}
%%DIFDELCMD < 				\For {$j = 1 : 2^{L-i}$ }{
%%DIFDELCMD < 					$\bb{V}_{2j-1}^{(i)}$, $\bm{\beta}_{2j-1}^{(i)}$, $\bb{R}_{2j-1}^{(i)} \gets{\tt qr}(\bb{A}_{2j-1}^{(i)})$ \;
%%DIFDELCMD < 					$\bb{V}_{2j}^{(i)}$, $\bm{\beta}_{2j}^{(i)}$, $\bb{R}_{2j}^{(i)} \gets{\tt qr}(\bb{A}_{2j}^{(i)})$\;
%%DIFDELCMD < 					% \tcp*{$\bb{V}_j^{(i)} \in \R^{2n\times n}$ for $i > 0$ and $\bb{R}_j^{(i)} \in \R^{n\times n}$ always.} 
%%DIFDELCMD < 					%
%\mbox{%DIFAUXCMD
%\(\bb{A}_{j}^{(i+1)} \gets \begin{bmatrix}
%%DIFDELCMD < 					\bb{R}_{2j-1}^{(i)}\\
%%DIFDELCMD < 					\bb{R}_{2j}^{(i)}
%%DIFDELCMD < 					\end{bmatrix}\)
%}%DIFAUXCMD
%%DIFDELCMD < 				}
%%DIFDELCMD < 			}
%%DIFDELCMD < 			\tcc{At the bottom-most level, get the final $\bb{R}$ factor.}
%%DIFDELCMD < 			%%%
%\DIFdel{$\bb{V}_{1}^{(L)}$, $\bm{\beta}_1^{(L)}$, $\bb{R}  \gets{\tt qr}(\bb{A}_{1}^{(L)})$ \;
%			$\bb{Q}_{1}^{(L)} \gets {\tt hh\_mult}(\bb{V}_{1}^{(L)}, I_{2n\times n})$\;
%			}%DIFDELCMD < \tcc{Compute $\bb{Q}^{(i)}$ factors by applying $\bb{V}^{(i)}$ to $\bb{Q}^{(i+1)}$ factors.}
%%DIFDELCMD < 			%%%
%%DIF < \tcc{Combine $\bb{Q}$ factors from bottom-up-- look at Notation (4).}
%			%DIFDELCMD < \For {%%%
%\DIFdel{$i = L-1 : -1 : 1$}%DIFDELCMD < }{
%%DIFDELCMD < 				\For {%%%
%\DIFdel{$j = 1 : 2^{L-i}$}%DIFDELCMD < }{
%%DIFDELCMD < 					%%%
%\DIFdel{%
%\mbox{%DIFAUXCMD
%\(\bb{Q}_{j}^{(i)} \gets {\tt hh\_mult}\left(\bb{V}_{j}^{(i)}, \begin{bmatrix}
%					\tilde{\bb{Q}}_{\alpha(j), \phi(j)}^{(i+1)}\\
%					\bb{0}_{n,n}
%					\end{bmatrix}\right)\)
%}%DIFAUXCMD
%%DIF < \bb{Q}_{j}^{(i)} \gets $ {\tt hh\_mult} $(\bb{V}_{j}^{(i)}, [\tilde{\bb{Q}}_{\alpha(j), \phi(j)}^{(i+1)}; O_{n,n}])$
%				}%DIFDELCMD < }
%%DIFDELCMD < 			}
%%DIFDELCMD < 			\tcc{At the top-most level, construct the final $\bb{Q}$ factor.}%%%
%%DIF <  from $\bb{Q}^{0}$ factors.}
%			\DIFdel{$\bb{Q} \gets [];$\;
%			}%DIFDELCMD < \For{$ j = 1 : 2^L $}{
%%DIFDELCMD < 				%
%\mbox{%DIFAUXCMD
%\(\bb{Q} \gets \begin{bmatrix}
%%DIFDELCMD < 				\bb{Q} \\
%%DIFDELCMD < 				{\tt hh\_mult}\left(\bb{V}_{j}^{(0)} , \begin{bmatrix}
%%DIFDELCMD < 				\tilde{\bb{Q}}_{\alpha(j), \phi(j)}^{(1)}\\
%%DIFDELCMD < 				O_{\tilde{h},n}
%%DIFDELCMD < 				\end{bmatrix} \right)
%%DIFDELCMD < 				
%
%%DIFDELCMD < 				\end{bmatrix}\)
%}%DIFAUXCMD
%%DIFDELCMD < 			}
%%DIFDELCMD < 			\Return{$\bb{Q}$, $\bb{R}$}
%%DIFDELCMD < 			%%%
%%DIFDELCMD < \caption{%
%{%DIFAUXCMD
%\DIFdel{$\bb{Q},\bb{R}={\tt tsqr}(\bb{A}, L)$.  Finds a QR factorization of a tall, skinny matrix, $\bb{A}$. }}
%			%DIFAUXCMD
%%DIFDELCMD < \label{algo:par_tsqr}
%%DIFDELCMD < 		\end{algorithm2e}
%%DIFDELCMD < 	%%%
%\subsection{\DIFdel{TSQR Rounding Error Analysis}}
%	%DIFAUXCMD
%\addtocounter{subsection}{-1}%DIFAUXCMD
%\DIFdelend \DIFaddbegin \DIFadd{refers to the action of applying multiple HH transformations given a set of HH vectors and constants, which can be performed by iterating line 6 of \cref{algo:hhQR}.
%This step can be done in a level-3 BLAS operation via a WY update if \cref{algo:par_tsqr} was modified to store the WY representation at the QR factorization of each block of each level, $\bb{A}_j^{(i)}$. 
%}
%
%\subsubsection{\DIFadd{TSQR: Rounding Error Analysis}}
%\DIFaddend \label{sec:TSQRre}
%The TSQR algorithm presented in \DIFdelbegin \DIFdel{Algorithm~\ref{algo:par_tsqr} }\DIFdelend \DIFaddbegin \DIFadd{\cref{algo:par_tsqr} }\DIFaddend is a divide-and-conquer strategy for the QR factorization that uses the HQR within the subproblems. 
%Divide-and-conquer methods can naturally be implemented in parallel and accumulate less rounding errors.
%For example, the single-level TSQR decomposition of a tall-and-skinny matrix, $\bb{A}$ requires 3 total HQRs of matrices of sizes $\lfloor\log_{2}(\frac{m}{n})\rfloor$-by-$n$, $\lceil\log_{2}(\frac{m}{n})\rceil$-by-$n$, and $2n$-by-$n$.
%The single-level TSQR strictly uses more FLOPs, but the dot product subroutines may accumulate smaller rounding errors (and certainly have smaller upper bounds) since they are performed on shorter vectors, and lead to a more accurate solution overall.
%These concepts are elucidated in \cite{Mori2012} \DIFdelbegin \DIFdel{, where the rounding error analysis of TSQR is shown in detail in \cite{Mori2012}.
%We }\DIFdelend \DIFaddbegin \DIFadd{and we }\DIFaddend summarize the main results  \DIFdelbegin \DIFdel{from \cite{Mori2012} in Theorem~\ref{thm:moriTSQR}}\DIFdelend \DIFaddbegin \DIFadd{in \cref{thm:moriTSQR}}\DIFaddend .
%
%%DIF < 		\begin{figure}[ht]
%%DIF < 			\centering
%%DIF < 			\includegraphics{./figures/TSQR.pdf}
%%DIF < 			\caption{\label{fig:TSQR} Visualization of the TSQR factorization (AllReduce) algorithm.}	
%%DIF < 		\end{figure}
%		\DIFdelbegin %DIFDELCMD < 
%
%%DIFDELCMD < 		%%%
%%DIF < \subsubsection{Variants of TSQR}
%			%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%%DIF < Since the subroutine with leading order FLOPs of an HQR implementation is the dot product, and we will use it to approximate the computational complexity of the TSQR algorithm.
%%DIF < For a tall-and-skinny matrix $\bb{A}\in\R^{m\times n}$ where $m\gg n$, the deepest possible TSQR scheme has $\lfloor\log_{2}(\frac{m}{n})\rfloor$ levels of recursion. 
%%DIF < The general idea is that deeper levels of recursion lead to shorter dot products, but more FLOPs over all, and both of these factors contribute to the accumulation of rounding errors and its effect on the accuracy of the QR factorization.
%\DIFdelend \begin{theorem}
%	\label{thm:moriTSQR}
%	Let $\bb{A}\in\R^{m\times n}$ with $m\geq n$ have full rank, $n$, and \DIFdelbegin \DIFdel{$\hat{\bb{Q}}\in\R^{m\times n}$ and $\hat{\bb{R}}\in\R^{n\times n}$ }\DIFdelend \DIFaddbegin \DIFadd{$\hat{\bb{Q}}_{TSQR}\in\R^{m\times n}$ and $\hat{\bb{R}}_{TSQR}\in\R^{n\times n}$ }\DIFaddend be the thin QR factors of $\bb{A}$ obtained via \DIFdelbegin \DIFdel{Algorithm~\ref{algo:par_tsqr}. 
%	Then we have normwise forward error bounds
%	}\begin{align*}
%	\DIFdel{\hat{\bb{A}} = \bb{A} +\bb{\Delta A} }&\DIFdel{=  \bb{Q}(\bb{R} + \bb{\Delta R}),}\\
%	\DIFdel{\hat{\bb{Q}} }&\DIFdel{= \bb{Q} + \bb{\Delta Q},
%	}\end{align*}
%	%DIFAUXCMD
%\DIFdel{where
%	}\begin{align*}
%	\DIFdel{\|\bb{\Delta R}\|_F, \|\bb{\Delta A}\|_F }&\DIFdel{\leq \left[n\tilde{\gamma}_{ \frac{m}{2^L}}+(1+n\tilde{\gamma}_{ \frac{m}{2^L}})\left\{(1+n\tilde{\gamma}_{ 2n})^L-1\right\}\right]\|\bb{A}\|_F, \text{ and} %DIFDELCMD < \label{eqn:tsqrRA}%%%
%}\\
%	\DIFdel{\|\bb{\Delta Q}\|_F }&\DIFdel{\leq \sqrt{n}\left[(1+n\tilde{\gamma}_{ \frac{m}{2^L}})(1+n\tilde{\gamma}_{ 2n})^L -1\right].%DIFDELCMD < \label{eqn:tsqrQ}%%%
%	%DIF < & \leq \left[n\tilde{\gamma}_{ \frac{m}{2^L}}+(1+n\tilde{\gamma}_{ \frac{m}{2^L}})\left\{(1+n\tilde{\gamma}_{ 2n})^L-1\right\}\right]\|\bb{A}\|_F,\label{eqn:tsqrA}
%	}\end{align*}
%	%DIFAUXCMD
%\DIFdel{Furthermore, if we assume $n\tilde{\gamma}_{ \frac{m}{2^L}}, n\tilde{\gamma}_{ 2n} \ll 1$, the coefficient for $\|\bb{A}\|_F$ in Equations~\ref{eqn:tsqrRA} can be approximated as
%	}\begin{displaymath}
%	\DIFdel{\left[n\tilde{\gamma}_{ \frac{m}{2^L}}+(1+n\tilde{\gamma}_{ \frac{m}{2^L}})\left\{(1+n\tilde{\gamma}_{ 2n})^L-1\right\}\right] \simeq n\tilde{\gamma}_{ \frac{m}{2^L}} + Ln\tilde{\gamma}_{ 2n}, %DIF < (46) in Mori
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{and the right hand side of Equation~\ref{eqn:tsqrQ} can be approximated as
%	}\begin{displaymath}
%	 \DIFdel{\sqrt{n}\left[(1+n\tilde{\gamma}_{ \frac{m}{2^L}})(1+n\tilde{\gamma}_{ 2n})^L -1\right]\simeq \sqrt{n}\left(n\tilde{\gamma}_{ \frac{m}{2^L}} + Ln\tilde{\gamma}_{ 2n}\right). %DIF < (67) in Mori
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{We can also form a backward error, where $\bb{A}+\bb{\Delta \bb{A}_{\text{TSQR}}} = \hat{\bb{Q}}\hat{\bb{R}}$, and both $\hat{\bb{Q}}$ and $\hat{\bb{R}}$ are obtained via Algorithm~\ref{algo:par_tsqr}}\DIFdelend \DIFaddbegin \DIFadd{\cref{algo:par_tsqr} with $L$ levels. 
%	Let us further assume that $m$ is divisible by $2^L$ and $n\tilde{\gamma}_{m2^{-L}}, n\tilde{\gamma}_{ 2n} \ll 1$}\DIFaddend .
%	Then, \DIFdelbegin \begin{displaymath}
%	\DIFdel{\|\bb{\Delta \bb{A}_{\text{TSQR}}}\|_F =\|\bb{Q \Delta R} + \bb{\Delta Q}\hat{\bb{R}}\|_F \simeq \sqrt{n}\left(n\tilde{\gamma}_{ \frac{m}{2^L}} + Ln\tilde{\gamma}_{ 2n}\right)\|\bb{A}\|_F.
%	}\end{displaymath}
%%DIFAUXCMD
%%DIFDELCMD < \end{theorem}
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{In Section~\ref{sec:mpupHQRcomparison}, the steps of the HQR algorithm resulted in an error bound of  $\mathcal{O}(\epsilon)$, where the constant is some function with respect to $n$ and where $\epsilon$ is the }\DIFdelend \DIFaddbegin \DIFadd{2-norm error bound for the $j^{th}$ column ($j=1:n$) of $\hat{\bb{R}}_{TSQR}$ and the Frobenius norm }\DIFaddend error bound for \DIFdelbegin \DIFdel{a single Householder transformation, described in Equation~\ref{eqn:applyPgen} .
%%DIF <  MATH QUESTION: Can n\epsilon and n^{3/2}\epsilon both be considered as O(\epsilon)?
%Similarly, the analysis behind Theorem~\ref{thm:moriTSQR} can be generalized via defining $\epsilon_1$ to be the error bound for a single Householder transformation corresponding to the vector length at the initial level $0$, $\frac{m}{2^L}$, and defining $\epsilon_2$ to be the error bound for a Householder transformation corresponding to vector length in all deeper levels ,  $2n$.
%This generalization leads to the error bound coefficients
%}\begin{align*}
% \DIFdel{n\epsilon_1 + Ln\epsilon_2}& \DIFdel{\qquad\text{for}\quad  \|\bb{\Delta Q}\|_F, \|\bb{\Delta \bb{A}_{\text{TSQR}}}\|_F,}\\
% \DIFdel{\sqrt{n}(n\epsilon_1+Ln\epsilon_2)}& \DIFdel{\qquad\text{for}\quad \|\bb{\Delta R}\|_F, \|\bb{\Delta A}\|_F.
%}\end{align*}
%%DIFAUXCMD
%\DIFdelend \DIFaddbegin \DIFadd{$\hat{\bb{Q}}_{TSQR}$ are
%	}\begin{align}
%	\DIFadd{\|\hat{\bb{R}}_{TSQR}[:,j]-\bb{R}[:,j]\|_2 }&\DIFadd{\leq n(\tilde{\gamma}_{m2^{-L}} + L\tilde{\gamma}_{ 2n})\|\bb{A}[:,j]\|_2,  \label{eqn:tsqrRA}}\\
%	\DIFadd{\|\hat{\bb{Q}}_{TSQR}-\bb{Q}\|_F }&\DIFadd{\leq n^{3/2}(\tilde{\gamma}_{m2^{-L}} + L\tilde{\gamma}_{ 2n}).\label{eqn:tsqrQ}
%	%DIF > \|\hat{\bb{Q}}_{TSQR}[:,j]-\bb{Q}[:,]\|_2 &\leq n(\tilde{\gamma}_{m2^{-L}} + L\tilde{\gamma}_{ 2n}).\label{eqn:tsqrQ}
%	}\end{align}
%\end{theorem}
%\DIFaddend 
%
%\DIFdelbegin \DIFdel{In a uniform-precision setting, these correspond to
%}\begin{displaymath}
%\DIFdel{\epsilon_1 = \tilde{\gamma}^{(\frac{m}{2^L})}\quad \text{and}\quad \epsilon_2 = \tilde{\gamma}^{(2n)},
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdel{and in the mixed-precision setting outlined in Assumption~\ref{assump:mp}, they correspond to
%}\begin{displaymath}
%\DIFdel{\epsilon_1 = \gamma_w^{(6d_1+6z+13)}, \quad \text{and } \epsilon_2 = \gamma_w^{(6d_2+6z+13)},
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdel{where $d_1 := \lfloor{(\frac{m}{2^L}-1)\frac{u_s}{u_w}\rfloor}$ }\DIFdelend \DIFaddbegin \DIFadd{Note that the $n\tilde{\gamma}_{m2^{-L}}$ }\DIFaddend and \DIFdelbegin \DIFdel{$d_2 :=\lfloor \frac{(2n-1)u_s}{u_w}\rfloor$ respectively.
%In both settings, we see that increasing }\DIFdelend \DIFaddbegin \DIFadd{$n\tilde{\gamma}_{ 2n}$ terms correspond to errors from applying HQR to the blocks in the initial partition and to the blocks in levels 1 through }\DIFaddend $L$ \DIFdelbegin \DIFdel{may decrease $\epsilon_1$, but may still increase the overall bounds; the larger $L$ still could have an adverse effect on the coefficients in Theorem~\ref{thm:moriTSQR}.
%This trade-off is precisely the balance between the sizes of initial blocks and the number of levels in the TSQR algorithm, and an optimal TSQR scheme would ideally minimize $\epsilon_1$ and $\epsilon_2$ with the choice of $L$.
%These error bounds are studied in detail in the following section. %DIF < Section ~\ref{sec:HTSQR}.
%%DIF < , resulting in some sort of a trade-off balance between , 
%%DIF < While the original problem required forming the QR decomposition of a $m$-by-$n$ matrix, an $L$-level TSQR solves $2^L$ QR factorizations of $\lfloor\frac{m}{2^L}\rfloor$-by-$n$ matrices in Level $0$, followed by $2^{L-1}, \cdots, 2^{0}$ QR factorizations of $2n$-by-$n$ matrices in levels $1$ to $L$, which sums to $2^{L}-1$ QR factorizations of $2n$-by-$n$ matrices. 
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\subsubsection{\DIFdel{HQR and TSQR error bound comparison}}
%%DIFAUXCMD
%\addtocounter{subsubsection}{-1}%DIFAUXCMD
%%DIFDELCMD < \label{sec:HTSQR}
%%DIFDELCMD < %%%
%\DIFdelend \DIFaddbegin \DIFadd{respectively.
%We can easily replace these with analogous mixed precision terms and keep the analysis accurate.
%Both level-2 and level-3 BLAS implementations will be considered in \cref{sec:mpanalysis}.
%}\paragraph{\DIFadd{TSQR and HQR error bound comparison}}
%%DIF > \subsubsection{HQR and TSQR error bound comparison}
%%DIF > \label{sec:HTSQR}
%\DIFaddend We compare the error bounds for HQR and TSQR\DIFdelbegin \DIFdel{algorithms. 
%}\paragraph{\DIFdel{Uniform precision comparison}}%DIFAUXCMD
%\addtocounter{paragraph}{-1}%DIFAUXCMD
%\DIFdel{Consider the larger error bounds in the uniform precision equivalents of Theorems~\ref{thm:feHQR} and \ref{thm:moriTSQR}, which are the bounds of $\bb{\Delta Q}$ and $\bb{\Delta A}$.
%In order for the a meaningful TSQR error bound to outperform the bound for the HQR algorithm, we need }\DIFdelend \DIFaddbegin \DIFadd{. 
%%DIF > \paragraph{Uniform precision comparison}
%Consider the bounds for $\|\hat{\bb{Q}}-\bb{Q}\|_F$ in \cref{thm:feHQR,thm:moriTSQR}.
%%DIF > In order for the a meaningful TSQR error bound to outperform the bound for the HQR algorithm, 
%TSQR has a lower worst-case error bound than HQR when }\DIFaddend integers $m, n > 0$, and $L\geq0$ \DIFdelbegin \DIFdel{such that,
%}\DIFdelend \DIFaddbegin \DIFadd{satisfy
%}\DIFaddend \begin{equation*}
%1\gg n^{3/2}\gamma^{(m)} \gg n^{3/2}(\gamma^{(\frac{m}{2^L})}+L\gamma^{(2n)}).
%\end{equation*}
%\DIFdelbegin \DIFdel{If we assume }\DIFdelend %DIF > for TSQR to have a better worst-case error bound than HQR.
%\DIFaddbegin \DIFadd{Let us consider as an example the case when }\DIFaddend $\frac{m}{2^L}=2n$\DIFaddbegin \DIFadd{.
%Then}\DIFaddend , the HQR bound is \DIFdelbegin \DIFdel{$\frac{L+1}{2^L}$ }\DIFdelend \DIFaddbegin \DIFadd{$2^L/(L+1)$ }\DIFaddend larger than the bound for TSQR with $L$ levels. 
%%DIF > TODO: comment reviewer
%For example, in single precision, a HQR of a $2^{15}$-by-$2^6$ matrix results in an upper bound relative backward error ($\|\bb{A}-\hat{\bb{Q}}\hat{\bb{R}}\|_F/\|\bb{A}\|_F$) of $\approx${\tt1.002}, but a TSQR with $L=8$ is bounded by $\approx${\tt 3.516e-02}. 
%This case exemplifies a situation in which stability is not guaranteed in HQR, but the method is stable when using TSQR, even in the worst-case. 
%Now consider some $2^{20}$-by-$2^{12}$ matrix and QR factorizations performed with double precision.
%The error bound for HQR is {\tt 1.686e-7}, whereas the error bound for TSQR with 12 levels is {\tt 5.351e-10}.
%In general, we can conjecture that values of $L$ that can make $m2^{-L}$ and $2Ln$ much smaller than $m$, should produce a TSQR that outperforms HQR in worst-case scenarios, at least in uniform precision settings.
%However, the range of matrix sizes that TSQR can accommodate decreases as $L$ grows larger.
%%, and the range is only half of that of HQR even for a single-level TSQR. 
%Figure~\ref{fig:paramspace} shows the matrix sizes HQR, 2-level TSQR, and 4-level TSQR can accommodate as well as their respective error bounds.\par
%\begin{wrapfigure}{l}{.45\textwidth}
%	\centering
%	%\vspace{-15pt}
%	\includegraphics[width=.45\textwidth]{./figures/paramspace.png}
%	\caption{\label{fig:paramspace} Non-white space indicates allowable matrix sizes for each scheme, and color map represents error bounds for $\|\bb{\Delta Q}\|_F$ for uniform precision error analysis when using double precision arithmetic.}
%	\vspace{-10pt}	
%\end{wrapfigure}
%\DIFdelbegin \paragraph{\DIFdel{Mixed precision comparison}}%DIFAUXCMD
%\addtocounter{paragraph}{-1}%DIFAUXCMD
%\DIFdel{Consider a mixed-precision setting such as in Assumption~\ref{assump:mp}, 
%}\DIFdelend \DIFaddbegin 
%
%
%\section{\DIFadd{Mixed precision error analysis}}\label{sec:mpanalysis}
%%DIF > \input{mpanalysis}
%\DIFadd{In this section, we consider three different mixed precision settings for the QR factorization, all of which take in a matrix $\bb{A}$ stored in low precision and return $\bb{Q},\bb{R}$ both represented in low precision. 
%First, we consider a trivial mixed precision setting where HQR, BQR, and TSQR are computed in high precision after casting up the input matrix at the beginning, and casting down the resulting high precision factors to low precision. 
%Then in \cref{sec:mp-3}, we modify BQR and TSQR to utilize level-3 BLAS operations and TensorCore bFMAs for the matrix product subroutines. 
%Finally, we impose \cref{assump:mp} in \cref{sec:mp-2} to see how a mixed precision inner product impacts HQR, BQR, and TSQR when applied in level-2 BLAS operations.
%}
%
%\paragraph{\DIFadd{Backward error of casting down vectors}} \DIFadd{First, consider 
%%DIF > casting down a scalar $x\in\F_h$ to $\F_l$.
%%DIF > Without overflow or underflow, this results in \[\text{\tt castdown}_{l}(x) = x(1+\dd^{(l)}),\;\; |\dd^{(l)}| < u^{(l)},\]
%%DIF > and accrues a single rounding error in the lower precision.
%%DIF > Now, let us consider 
%casting down a vector  $\bb{x}\in\F_h^{(m)}$.
%The componentwise forward error is, }\[\DIFadd{\text{\tt castdown}_{l}(\bb{x}) = \bb{x} + \Delta }{\DIFadd{\bb{x}}}\DIFadd{,\;\; |\Delta\bb{x}| < u^{(l)}|\bb{x}|.}\]
%\DIFadd{We use this to represent the backward error of a casting down a vector with a linear transformation, $\bb{I}^{(l)}:=\bb{I} +\bb{E}\in\R^{m\times m}$, a diagonal perturbation of the identity.
%%DIF > That is, 
%%DIF > For some vector $\bb{x}\in\F_h$, the cast down operation yields
%We write,
%}\begin{equation}
%\DIFadd{\bb{x}^{(l)} := \text{\tt castdown}(\bb{x}^{(h)}) = \bb{I}^{(l)}\bb{x}^{(h)} = (\bb{I}+\bb{E})\bb{x}^{(h)} = \bb{x}^{(h)}+\Delta \bb{x},
%}\end{equation}
%\DIFadd{where $|\Delta \bb{x}| \leq u^{(l)} |\bb{x}^{(h)}|$ }\DIFaddend and  \DIFdelbegin \DIFdel{we assume $u_p=u_w$, so that $z=2$.
%In order for the a meaningful TSQR error bound to outperform the bound for the HQR algorithm}\DIFdelend \DIFaddbegin \DIFadd{$\|\Delta \bb{x}\|_2 \leq u^{(l)} \|\bb{x}^{(h)}\|_2$.
%Thus, $\bb{E} = \Delta \bb{x x}^{\top}/\|\bb{x}\|_2^2$ and we can use the same argument as in \cref{eqn:outer} to form a backward matrix norm bound, 
%}\begin{equation}
%\DIFadd{\|\bb{E}\|_F\leq u^{(l)}. \label{eqn:castdown}
%}\end{equation}
%
%\paragraph{\DIFadd{Casting down after HQR in high precision}} \DIFadd{Let us consider the trivial case of carrying out HQR in high precision and casting down at the very end.
%This is useful for the analysis of mixed precision
%%DIF > applications that require economical storage but have enough memory to carry out HQR in higher precision, or in 
%block algorithms as will be shown in \cref{sec:mp-3}.
%If the two floating point types $\F_{l}$ and $\F_{h}$ satisfy $\F_{l}\subseteq \F_{h}$
%%DIF >  and for all $x,y\in\F_{l}$, the exact product $xy$ can be represented in $\F_{h}$.
%%DIF > Some example pairs of $\{\F_{l}, \F_{h}\}$ include $\{\text{fp16}, \text{fp32}\}$, $\{\text{fp32}, \text{fp64}\}$, and $\{\text{fp16}, \text{fp64}\}$.
%%DIF > Suppose that 
%and the matrix to be factorized is stored with low precision numbers, $\bb{A}\in\F_{l}^{m\times n}$, then casting up adds no rounding errors.
%Therefore, we can directly apply the analysis that culminated in \cref{thm:feHQR}, and we only consider the columnwise forward error in the $\bb{Q}$ factor.
%Then, the $j^{th}$ column of $\hat{\bb{Q}}_{HQR} = \bb{Q} + \Delta \bb{Q}_{HQR}$ is bounded normwise via $\|\Delta \bb{Q}_{HQR}[:,j]\|_2 \leq n\tilde{\gamma}_{m}^{h},$ and incurs an extra rounding error when $\hat{\bb{Q}}_{HQR}\in\F_{h}^{m\times n}$ is cast down to $\F_{l}^{m\times n}$.
%%DIF > Casting down $x\in\F_h$ to $\F_l$ without overflow or underflow results in \[\text{\tt castdown}(x) = x(1+\dd^{(l)}),\;\; |\dd^{(l)}| < u^{(l)},\]
%%DIF > and accrues a single rounding error in the lower precision.
%%DIF > Extending this result, we represent the backward error of a casting down a vector in $\F_h^{(m)}$ with a linear transformation, $\bb{I}^{(l)}\in\R^{m\times m}$.
%%DIF > This transformation is a diagonal perturbation of the identity, $\bb{I}_m$.
%%DIF > For some vector $\bb{x}\in\F_h$, the cast down operation yields
%%DIF > \begin{equation}
%%DIF > 	\bb{x}^{(l)} := \text{\tt castdown}(\bb{x}^{(h)}) = \bb{I}_{l}\bb{x}^{(h)} = (\bb{I}+\bb{E})\bb{x}^{(h)} = \bb{x}^{(h)}+\Delta \bb{x},
%%DIF > \end{equation}
%%DIF > where $|\Delta \bb{x}| \leq u^{(l)} |\bb{x}^{(h)}|$ and  $\|\Delta \bb{x}\|_2 \leq u^{(l)} \|\bb{x}^{(h)}\|_2$.
%%DIF > Then, $\bb{E} = \Delta \bb{x x}^{\top}/\|\bb{x}\|_2^2$ and we can use the same argument as in \cref{eqn:outer} to form a backward matrix norm bound, 
%%DIF > \begin{equation}
%%DIF > 	\|\bb{E}\|_F\leq u^{(l)}. \label{eqn:castdown}
%%DIF > \end{equation}
%Using this in \cref{lem:3.7} to analyze the forward norm error for the $j^{th}$ column of the $\bb{Q}$ factor computed with \cref{algo:hhQR} yields
%}\begin{equation}
%\DIFadd{\|(\text{\tt castdown}(\hat{\bb{Q}}_{HQR})- \bb{Q})}[\DIFadd{:,j}]\DIFadd{\|_2 = \|(\bb{I}^{(l)}\hat{\bb{P}}_{1}\cdots\hat{\bb{P}}_{n}-\bb{P}_{1}\cdots\bb{P}_{n})\hat{\bb{e}}_j\|_2 \leq u^{(l)}+n\tilde{\gamma}_m^{(h)} + nu^{(l)}\tilde{\gamma}_m^{(h)}.\label{eqn:HQR-mp}
%}\end{equation}
%\DIFadd{The final castdown operation increases the upper bound by $u^{(l)}$ and the size of $\bb{A}$ has no impact on this extra rounding error. 
%%DIF > To convert this bound to the lower precision, we define function $d$,
%%DIF > \begin{equation}
%%DIF > d(m,u^{(h)}, q,u^{(l)}) := \lceil (qu^{(l)}+mu^{(h)})/u^{(l)}\rceil = \cO(q+mu^{(h)}/u^{(l)}),\label{eqn:d}
%%DIF > \end{equation} 
%%DIF > so that if $\|\hat{\bb{x}}-\bb{x}\|_2 \leq \gamma_m^{(h)}$, then $\|\text{\tt castdown}(\hat{\bb{x}})-\bb{x}\|_2 \leq \gamma_{d(m,u^{(h)}, q,u^{(l)})}^{(l)}$.
%%DIF > This is a looser bound but it allows us to easily compare the errors to the uniform, low precision implementation of forming $\hat{\bb{x}}$.
%%DIF > A looser upper bound is given by $\tilde{\gamma}_d^{(l)}$ where $d = \lceil u^{(l)}_+mu^{(h)}/u^{(l)}\rceil$ and we benefit from being able to represent it in terms of the low precision . 
%%DIF > Additionally, we can use this to identify a function that allows us to formulate the new error bound after applying a castdown operation to vectors whose error bounds were known in high precision,
%%DIF > Let $d = \lceil nmu^{(h)}/u^{(l)}\rceil$, so that $nmu^{(h)}\leq du^{(l)}$, and there exists some $r\leq \lfloor u^{(h)}/u^{(l)} \rfloor$ so that $nmu^{(h)} = (d-1)u^{(l)}+ru^{(h)}$.
%%DIF > This $d$ value allows us to convert the error in terms of the higher precision to the lower precision while also adding in the extra rounding error that may be incurred in the casting down step. 
%%DIF > Then, the componentwise error is bounded by
%%DIF > \begin{equation}
%%DIF > 	|\text{{\tt castdown}}(\Delta \bb{Q}_{HQR}[i,j])| \leq \frac{cnmu^{(h)}}{1-cmu^{(h)}} \leq \frac{cdu^{(l)}}{1-cmu^{(h)}} \leq \tilde{\gamma}_{d}^{(l)},
%%DIF > \end{equation}
%%DIF > and the columnwise error is 
%%DIF > \begin{equation}
%%DIF > 	\|\text{{\tt castdown}}(\Delta \bb{Q}_{HQR}[:,j])\|_2 \leq \left(\sum_{i=1}^m |\tilde{\tth}_d^{(l)}|^2 \right)^{1/2} \leq \sqrt{m}\gamma_d^{(l)}
%%DIF > \end{equation}
%%DIF > Since $\hat{\bb{Q}}_{HQR}$ should be almost orthogonal with respect to the higher precision, we can expect all components to be within the dynamic range of $\F_{l}$.
%%DIF > In \ref{sec:mp-f}, we look at the rounding errors incurred from carrying out a QR factorization in a high precision, then cast down at the very end.
%%DIF > Since this requires only one cast down operation, this is very similar to the results from the standard uniform precision analysis.
%%DIF > Similarly, we can apply the operator $\bb{I}^{(l)}$ to cast down any quantity stored in the higher precision. 
%%DIF > We consider this trivial case for HQR since BQR and TSQR use it as a subroutine. 
%%DIF > If we consider the same trivial mixed precision setting for BQR and TSQR, then the corresponding forward matrix norm errors on the $\bb{Q}$ factor are
%%DIF > \begin{align*}
%%DIF > 	\|\hat{\bb{Q}}_{BQR}\|_F&\leq u^{(l)}+n\tilde{\gamma}_m^{(h)} +u^{(l)}n\tilde{\gamma}_m^{(h)},\\
%%DIF > 	%\leq \tilde{\gamma}_{d(nm,u^{(h)},u^{(l)})}^{(l)},\\
%%DIF > 	\|\hat{\bb{Q}}_{TSQR}\|_F&\leq u^{(l)}+n(L\tilde{\gamma}_{2n}^{(h)}+\tilde{\gamma}_{m2^{-L}}^{(h)}) +u^{(l)}n(L\tilde{\gamma}_{2n}^{(h)}+\tilde{\gamma}_{m2^{-L}}^{(h)}).
%%DIF > 	%\leq \tilde{\gamma}_{d(n(L2n+m2^{-L}),u^{(h)},1,u^{(l)})}^{(l)}.
%%DIF > \end{align*}
%Applying this trivial mixed precision setting to BQR and TSQR would simply increases the error bound by approximately $u^{(l)}$ all the while taking an even longer time than the high precision implementation due the extra cast down and cast up operations.
%Therefore, we do not analyze the rounding error analysis of this mixed precision variant of BQR and TSQR.
%However, we will use this mixed precision HQR as a subroutine of the mixed precision BQR and TSQR in the following section. 
%%DIF > We will modify BQR and TSQR so that matrix-matrix multiply and accumulate operations can be performed on TensorCore block FMAs which work on $4$-by-$4$ matrices, $\bb{A},\bb{B},\bb{C},$, and $\bb{D}$ that compute \[
%%DIF > \bb{D} = \fl( \bb{C} +\bb{A}\bb{B}),\]
%%DIF > where $\bb{A},\bb{B}\in \F_{\text{fp16}}^{4\times 4}$ and  $\bb{C},\bb{D}\in \F_{\text{fp16}}^{4\times 4}$ or $\bb{C},\bb{D}\in \F_{\text{fp32}}^{4\times 4}$.
%%DIF > The inner product step in forming $\bb{A}\bb{B}$ is similar to \cref{assump:mp} in that full precision (exact) products are accumulated in the higher precision, fp32.
%%DIF > One difference is that the cast down operation at the end of the inner product is optional.
%%DIF > Matrices larger than $4$-by-$4$'s can be multiplied and added using this optional cast down feature and by using block matrix multiplication with $4$-by-$4$ blocks.
%%DIF > In \cref{sec:mp-3}, we consider performing BQR and TSQR with high precision FLOPs within a block/level, but cast down to low precision in between blocks and at the very end.
%%DIF > Finally, in \cref{sec:mp-2}, we consider all 3 algorithms with the ad hoc mixed precision setting described in \cref{assump:mp} where inner products are performed in high precision before being cast down, and all other operations are computed in low precision.
%%DIF > \subsection{Round down at the end of the factorization}\label{sec:mp-f}
%%DIF > Results in this section are quite straight forward, but 
%%DIF > \subsubsection{HQR}
%}
%
%%DIF > \input{mp3}
%\subsection{\DIFadd{Round down at block-level: level-3 BLAS mixed precision setting}}\label{sec:mp-3}
%\DIFadd{The mixed precision setting in this section is designed to meet the below requirements.
%}\begin{enumerate}
%	\item \DIFadd{Modify \Cref{algo:blockHQR,algo:par_tsqr} to maximize level-3 BLAS operations and use TensorCore bFMAs. 
%	}\item \DIFadd{Apply \cref{eqn:HQR-mp} to all instances of HQR to the error analyses for BQR and TSQR in \cref{sec:algo}.
%	}\item \DIFadd{Cast down quantities at every block/level and the insertion of low precision errors $u^{(l)}$ should be somewhat correlated to the number of blocks and levels. 
%	}\item \DIFadd{Both input and output of the various QR factorization algorithms are given in the low precision. 
%}\end{enumerate}
%\DIFadd{TensorCore's bFMA computes 
%}\begin{equation}
%\DIFadd{\hat{\bb{D}} =\fl_{TC}(\bb{C} + \bb{A}\bb{B}),\qquad \bb{C},\bb{D}\in\F_{\text{fp16}}^{4\times 4}\text{ or }\F_{\text{fp32}}^{4\times 4},\text{ and } \bb{A},\bb{B}\in\F_{\text{fp16}}^{4\times 4},\label{eqn:bFMA}
%}\end{equation}
%\DIFadd{and employs \emph{full} precision products and fp32 summation accumulate.
%Here, the \emph{full} precision multiplication is exact as explained in \cref{sec:background}.
%In \cite{Blanchard2019}, the authors investigate all four possible matrix-matrix multiplication routines in TensorCore, which depend on whether $\bb{C}$ and $\bb{D}$ are computed in fp16 or fp32. 
%They also note that matrices larger than $4$-by-$4$ can still be computed using this block FMA by accumulating matrix sums with $\bb{C}\in\F_{\text{fp32}}^{4\times 4}$.
%Suppose that we aim to compute a fp16 matrix product of two fp16 matrices, $\bb{X}\in\F_{(fp16)}^{m\times p}$}\DIFaddend , \DIFdelbegin \DIFdel{we now need integers $m, n > 0$, }\DIFdelend \DIFaddbegin \DIFadd{$\bb{Y}\in\F_{(fp16)}^{p\times n}$, and $\bb{Z}=\bb{XY}\in\F_{\text{fp16}}^{m\times n}$.
%We pad $\bb{X},\bb{Y}$ with zeros so that all matrix dimensions are multiples of $4$ and the matrix product can be computed with the TensorCore block FMA.
%Let $\bb{Q}_{[i,j]}:= \bb{Q}[4(i-1)+1:4i,4(j-1)+1:4j]$ refer to the $(i,j)^{th}$ $4$-by-$4$ block for any $\bb{Q}\in\{\bb{X},\bb{Y},\bb{Z}\}$.
%Then, we compute $\bb{Z}_{[i,j]}$ via }\[
%\DIFadd{\bb{Z}_{[i,j]} = \sum_{k=1}^{\lceil p/4\rceil} \bb{X}_{[i,k]} \bb{Y}_{[k,j]},
%}\]
%\DIFadd{where we use \cref{eqn:bFMA} by initializing with $\bb{A}^{(1)}:= \bb{X}_{[i,1]}$, $\bb{B}^{(1)}:= \bb{Y}_{[1,j]}$, and $\bb{C}^{(1)}:= \bb{0}_{4\times 4}$ and setting $\bb{A}^{(k)}:= \bb{X}_{[i,k]}$, $\bb{B}^{(k)}:= \bb{Y}_{[k,j]}$, and $\bb{C}^{(k)}:= \bb{D}^{(k-1)}$ for $k=2:\lceil p/4\rceil$.
%By setting $\bb{C}^{(k)}, \bb{D}^{(k)}\in\F_{\text{fp32}}^{4\times 4}$ for $k>1$ and only casting down at the end via $\bb{Z}_{[i,j]} =$ fp16$(\bb{D}^{(\lceil p/4\rceil)})$, we mostly employ fp32 arithmetic for a mixed precision matrix product routine whose inputs and output are in fp16.
%For example, take $p=8$.
%Then,
%}\begin{align*}
%\DIFadd{\bb{D}^{(1)} }&\DIFadd{= \fl_{TC}(\bb{X}_{[i,1]} \bb{Y}_{[1,j]}),\quad\bb{D}^{(2)} = \fl_{TC}(\bb{X}_{[i,2]} \bb{Y}_{[2,j]} + \bb{D}^{(1)})\in\F_{\text{fp32}}^{4\times 4}}\\
%\DIFadd{\bb{Z}_{[i,j]} }&\DIFadd{= \text{\tt castdown}(\bb{D}^{(2)})\in\F_{\text{fp16}}^{4\times 4}.
%}\end{align*}
%\DIFadd{Adapting the rounding error analysis in \cite{Blanchard2019} into this specific mixed precision matrix product setting yields the componentwise forward bound 
%}\begin{equation}
%\DIFadd{|\bb{Z}-\fl(\bb{Z})| \leq \left(u^{(\text{fp16})}+ \gamma_{p/4}^{(\text{fp32})}+u^{(\text{fp16})} \gamma_{p/4}^{(\text{fp32})}\right)|\bb{X}||\bb{Y}|.\label{eqn:bFMAerr}
%}\end{equation}
%
%\DIFadd{We denote BQR and TSQR computed via TensorCore bFMA's with }{\tt \DIFadd{mpBQR3}} \DIFaddend and \DIFdelbegin \DIFdel{$L\geq 0$ such that
%}\begin{displaymath}
%\DIFdel{1\gg n^{3/2}\gamma_w^{(6d + 25)} \gg n^{3/2}(\gamma_w^{(6d_1+ 25)}+L\gamma_w^{(6d_2+ 25)}),
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdel{where $d=\lfloor\frac{(m-1) u_s}{u_w}\rfloor$, $d_1 = \lfloor{(\frac{m}{2^L}-1)\frac{u_s}{u_w}\rfloor}$, }\DIFdelend \DIFaddbegin {\tt \DIFadd{mpTSQR3}}\DIFadd{, where the }{\tt \DIFadd{3}} \DIFadd{represents the BLAS level-3 nature of this mixed precision setting.
%}\subsubsection{\DIFadd{BQR round down at block level: }{\tt \DIFadd{mpBQR3}}}\label{sec:mp-3b}
%%DIF > Let us consider a setting in which only $M$ blocks of width $r$ can be loaded onto memory.
%%DIF > Then, lines 2-6 of \cref{algo:blockHQR} can be modified via \cref{algo:mpBQR}.
%%DIF > \begin{algorithm2e}
%%DIF > 	$q = N/M$\tcc*{Note that $n=Nr=qMr$.}
%%DIF > 	\For{$q'=1:q$}{
%%DIF > 		\If {$q'>2$} {
%%DIF > 				Update $[\bb{C}_{(q'-1)M+1}\cdots\bb{C}_{qM}]$ with WY updates from blocks $1:(q'-1)M$.
%%DIF > 		}
%%DIF > 		\For{$k=1:M$}{
%%DIF > 		Apply HQR to $\bb{C}_{(q'-1)M+k}$\;
%%DIF > 		Form WY update for $\bb{C}_{(q'-1)M+k}$\;
%%DIF > 		WY update blocks to the right, $[\bb{C}_{(q'-1)M+k+1}\cdots \bb{C}_{q'M}]$.
%%DIF > 	}
%%DIF > }
%%DIF > \caption{\label{algo:mpBQR} A portion of a mixed precision BQR: modifying first for-loop in \cref{algo:blockHQR}.}
%%DIF > \end{algorithm2e}
%%DIF > We now impose a mixed-precision setting where the inner for-loop in \cref{algo:mpBQR} is performed in high precision, but the WY updates for the outer loop is stored in low precision and only $M$ blocks is updated at a time due to the memory constraint.
%%DIF > These low precision WY updates would be used to build the $\bb{Q}$ factor serially in groups of $M$.
%\DIFadd{Consider the input matrix, $\bb{A}\in\F_l^{m\times n}$, partitioned into $N$ blocks of $r$ columns, $\bb{A}=[\bb{C}_1 \cdots \bb{C}_N]$ as in \cref{sec:BQR}.
%%DIF > Since \cref{algo:blockHQR} uses level-3 BLAS operations on $1-\cO(1/N)$ fraction of all the FLOPs, we modify it so that those operations are performed with TensorCore bFMAs. 
%%DIF > %We assume that the returned factors should also be represented in the lower precision, $\F_l$, and modify \cref{algo:blockHQR} so that matrix-matrix multiply and accumulate operations are performed with TensorCore block FMAs.
%%DIF > The remaining FLOPs are a relatively small($\cO(1/N)$) fraction in comparison, and we assume that we can afford to compute these level-1 and 2 BLAS operations using high precision.
%\Cref{algo:mpBQR} shows a mixed precision variant of BQR that maximizes the use of bFMAs but uses high precision arithmetic for level-1 }\DIFaddend and \DIFdelbegin \DIFdel{$d_2 =\lfloor \frac{(2n-1)u_s}{u_w}\rfloor$.
%	}\DIFdelend \DIFaddbegin \DIFadd{2 BLAS operations which are only a $\cO(1/N)$ fraction of the total number of FLOPs. 
%Each block is casted up to compute a high precision HQR and to form the WY representation. 
%The WY representation is then casted down to low precision since the bFMAs require low precision inputs for matrix products, and the $\bb{R}$ factor from the high precision HQR can be casted down to return a low precision $\bb{R}$ factor at the very end. 
%Since the cast down operations for the $\bb{R}$ factor and the WY representations occur at every block, we can expect columnwise error bound for \cref{algo:mpBQR} to increase by approximately $Nu^{(l)}$ from the error bound for \cref{algo:blockHQR}.
%%DIF > The $\bb{R}$ factor from each call to HQR is stored in low precision, but the HH constants and vectors ($\bm{\beta}_k^{(j)}$,$\bb{v}_k^{(j)}$) are kept in high precision to build the WY representation.
%%DIF > We enforce a cast down at the end of \cref{algo:buildWY} since the bFMAs require low precision inputs.
%%DIF > Since the WY representations ($\bb{W}_k$, $\bb{V}_k$) should be stored in low precision, we enforce a cast down at the end of \cref{algo:buildWY}.
%%DIF > Finally, all but the last WY update for each block are stored in the higher precision, and the last WY update returned in low precision. 
%%DIF > This mixed precision BQR variant is rich in level-3 BLAS operations can be implemented with TensorCore block FMAs easily, and is formally introduced in \cref{algo:mpBQR}.
%}\begin{algorithm2e}
%	\DontPrintSemicolon %DIF >  Some LaTeX compilers require you to use \dontprintsemicolon instead
%	\KwIn{$\bb{A}$, $r$. \hfill\textbf{Output: }$\hat{\bb{Q}}_{mpBQR3}$,$\hat{\bb{R}}_{mpBQR3}$}
%	\DIFadd{$N=\frac{n}{r}$}\tcc*{Let $\bb{A} = [\bb{C}_{1} \cdots  \bb{C}_{N}]$.}
%	%DIF > \tcp{Let $n_i=ri$ for $i=1:N-1$ and $n_N=n$.} 
%	\For{$k=1:N-1$}{
%		%		\If{$k == 1$}{
%		$\bb{V}_{k},\bm{\beta}_k,\bb{C}_{k}\gets$ {\tt hhQR}({\tt castup}($\bb{C}_{k}$))\tcc*{\Cref{algo:hhQR} in high precision.}
%		%		}
%		%		\Else{
%		%		$\bb{V}_{k},\bm{\beta}_k,\bb{C}_{k}\gets$ {\tt hhQR}($\bb{C}_{k}$)\tcc*{\Cref{algo:hhQR} in high precision.}	
%		%	}
%		$\bb{C}_{k}\gets ${\tt castdown }($\bb{C}_{k}$)\tcc*{Builds $\bb{R}$ factor in low precision.}
%		%$\bb{V}_i,\bm{\beta}_i,\bb{A}_{n_{i-1}+1:m,n_{i-1}+1:n_i}\gets$ {\tt hhQR}$(\bb{A}_{n_{i-1}:m,n_{i-1}+1:n_i})$\tcc*{\Cref{algo:hhQR}}
%		$\bb{W}_{k}\gets $ {\tt buildWY}$(\bb{V}_{k},\bm{\beta}_k)$ \tcc*{\Cref{algo:buildWY} in high precision}
%		$[\bb{V}_{k},\bb{W}_{k}]\gets ${\tt castdown}($[\bb{V}_{k},\bb{W}_{k}]$)\;
%		$[\bb{C}_{k+1}\cdots\bb{C}_{N}]$ -= $\bb{V}_{k} \left(\bb{W}_{k}^{\top}[\bb{C}_{k+1}\cdots\bb{C}_{N}]\right) $ \tcc*{returned in low precision}
%	}
%	%DIF > \tcp{Now build $\bb{Q}$ using level-3 BLAS operations.} 
%	\DIFadd{$\bb{Q}\gets \bb{I}$}\tcc*{Build $\bb{Q}$: $\bb{I}_m$ if full QR, and $\bb{I}_{m\times n}$ if thin QR.}
%	\For{$k=N:-1:1$}{
%		\tcp{All updates are returned in low precision.}
%		$\bb{Q}[(k-1)r+1:m,(k-1)r+1:n]$-= $\bb{W}_k \left(\bb{V}_k^{\top}\bb{Q}[(k-1)r+1:m,(k-1)r+1:n]\right)$
%	}
%	%DIF > \tcp{The last update is returned in low precision.}
%	%DIF > $\bb{Q}$-= $\bb{W}_1 \left(\bb{V}_1^{\top}\bb{Q}\right)$\;
%	\Return \DIFadd{$\bb{Q},\bb{A}$
%	}\caption{\label{algo:mpBQR} \DIFadd{$\hat{\bb{Q}}_{mpBQR3},\hat{\bb{R}}_{mpBQR3}\gets {\tt mpBQR3}(\bb{A}, r)$: Perform a mixed precision variant of BQR of low precision $\bb{A}$ with column partitions of size $r$. $\hat{\bb{Q}}_{mpBQR3}$,$\hat{\bb{R}}_{mpBQR3}$, are returned in low precision. Operations in lines 7 and 10 require low precision inputs.}}
%\end{algorithm2e}
%\DIFaddend 
%
%\DIFaddbegin \DIFadd{Since $\hat{\bb{W}}_{k},\hat{\bb{Y}}_k$'s are computed with \cref{algo:buildWY} in high precision then cast down, the new low precision WY update is $\hat{\bb{X}}_{k}^{(l)} = \bb{I}-\bb{I}^{(l)}\hat{\bb{W}}_k\bb{I}^{(l)}\hat{\bb{V}}_k^{(\top)}$.
%Consider applying $\hat{\bb{X}}_k^{(l)}$ to some matrix stored in low precision, $\bb{B}$ using the TensorCore bFMAs.
%We analyze a single column $\bb{b}_j:=\bb{B}[:,j] \in \F_l^{m-(k-1)r}$ even though this operation is done on $\bb{B}$ as a whole.
%Let $\bb{I}^{(l)}\hat{\bb{W}}_k = (\bb{I}+\bb{E}_W)\hat{\bb{W}}_k$ and $\bb{I}^{(l)}\hat{\bb{Y}}_k = (\bb{I}+\bb{E}_Y)\hat{\bb{Y}}_k$, where $\bb{E}_W,\bb{E}_Y$ are diagonal and bounded componentwise by $u^{(l)}$.  
%%DIF > Since \[\hat{\bb{X}}_{k}^{(l)}- \bb{X}_k = \hat{\bb{X}}_{k}^{(l)} -\hat{\bb{X}}_{k}+ \hat{\bb{X}}_{k}- \bb{X}_k= \hat{\bb{X}}_{k}^{(l)} -\hat{\bb{X}}_{k}+ \Delta \bb{X}_{k},\]
%%DIF > the errors derived in \cref{eqn:deltX} since the bFMAs perform block $4$-by-$4$ matrix products using \cref{eqn:bFMAerr}, then.
%%DIF > The rounding errors for forming $\bb{W}_k$ and $\bb{Y}_k$ remain the same since these are computed in high precision. 
%Then, 
%%DIF > we include errors introduced from casting down $\hat{\bb{W}}$ and $\hat{\bb{Y}}$ and compute 
%the Frobenius norm error of forming $\hat{\bb{X}}_{k}^{(l)}$ is,
%%DIF > to $\bb{b}_j$ with exact arithmetic.
%}\begin{align*}
%\DIFadd{\|\hat{\bb{X}}_{k}^{(l)}- \bb{X}_{k}\|_F  }&\DIFadd{= \|-\left(\bb{I}+\bb{E}_W+\bb{E}_Y+ \bb{E}_W\bb{E}_Y\right)\hat{\bb{W}}_k\hat{\bb{Y}}_k^{\top} + \bb{W}_k\bb{Y}_k^{\top}\|_F,}\\
%&\DIFadd{\leq \left((1+\gamma_2^{(l)}+(u^{(l)})^2)r\tilde{\gamma}_{m-(k-1)r}^{(h)}+\gamma_2^{(l)}+(u^{(l)})^2\right)\|\bb{X}_k\|_F}\\
%&\DIFadd{\leq \tilde{\gamma}_2^{(l)} +r\tilde{\gamma}_{m-(k-1)r}^{(h)} + r\tilde{\gamma}_2^{(l)}\tilde{\gamma}_{m-(k-1)r}^{(h)}.
%}\end{align*}
%\DIFadd{Now, we consider the backward error of applying $\hat{\bb{X}}_{k}^{(l)}$ to $\bb{b}_j$ with the bFMA matrix product error bound from \cref{eqn:bFMAerr}.
%The multiplication by $(\bb{I}^{(l)}\hat{\bb{Y}}_k)^{\top}$ yields backward error bounded by
%}\begin{equation*}
%\DIFadd{\fl_{TC}((\bb{I}^{(l)}\hat{\bb{Y}}_k)^{\top}\bb{b}_j) = (\hat{\bb{Y}}_k+\Delta_{TC}\hat{\bb{Y}}_k)\bb{b}_j,\;\;|\Delta_{TC}\hat{\bb{Y}}_k| \leq u^{(l)}+\gamma_{\frac{m-(k-1)}{4}}^{(h)}+u^{(l)}\gamma_{\frac{m-(k-1)}{4}}^{(h)}|\hat{\bb{Y}}_k||\bb{b}_j|,
%%DIF > 	\fl_{TC}(\bb{b}_j-\bb{W}_k\fl_{TC}((\bb{I}^{(l)}\hat{\bb{Y}}_k)^{\top}\bb{b}_j)) &= (\hat{\bb{X}}_k+\Delta_{TC}\hat{\bb{X}}_k)\bb{b}_j,
%}\end{equation*}
%\DIFadd{and the subsequent multiplication by $(\bb{I}^{(l)}\hat{\bb{W}}_k)$ and subtraction from $\bb{b}_j$ result in,
%%DIF >  $\cO(u^{(l)}+\gamma_{1+r/4}^{(h)}+u^{(l)}\gamma_{1+r/4}^{(h)})$ to the error bound.
%%DIF > Finally, this is summarized by
%}\begin{align*}
%\DIFadd{\fl_{TC}(\hat{\bb{X}}_{k}^{(l)}\bb{b}_j) }&\DIFadd{= (\hat{\bb{X}}_{k}^{(l)}+\Delta^{(l)}\bb{X}_k)\bb{b}_j,}\\
%\DIFadd{|\Delta^{(l)}\bb{X}_k| }&\DIFadd{\leq \left(\gamma_2^{(l)}+\gamma_{1+\frac{m-(k-2)r}{4}}^{(h)}+\gamma_2^{(l)}\gamma_{1+\frac{m-(k-2)r}{4}}^{(h)}\right)\left(|\bb{b}_j|+|\bb{I}^{(l)}\hat{\bb{W}}_k||\bb{I}^{(l)}\hat{\bb{Y}}_k|^{\top}|\bb{b}_j|\right).
%}\end{align*}
%\DIFadd{Converting to a normwise error bound using the same logic from \cref{eqn:applyP,eqn:19.2c}, we result in  
%%DIF > Recall that applying $\bb{X}_k$ in high precision  
%%DIF > \begin{align*}
%%DIF > 	\|(\hat{\bb{X}}_{k}^{(l)}- \hat{\bb{X}}_{k}+ \Delta \bb{X}_{k})\bb{b}_j \|_2 %&=\| (\bb{b}_j-(\hat{\bb{W}}_k + \bb{E}_W\hat{\bb{W}}_k)((\hat{\bb{Y}}_k + \bb{E}_Y\hat{\bb{Y}}_k)^{\top}\bb{b}_j)) - (\bb{b}_j-\hat{\bb{W}}_k(\hat{\bb{Y}}_k^{(\top)}\bb{b}_j) + \Delta \bb{X}_{k} \bb{b}_j\|_2\\
%%DIF > 	&= \| \left(-\left(\bb{E}_W+\bb{E}_Y+ \bb{E}_W\bb{E}_Y\right)\hat{\bb{W}}_k\hat{\bb{Y}}_k^{\top} + \Delta \bb{X}_{k} \right)\bb{b}_j\|_2,\\
%%DIF > 	&= (\gamma_2^{(l)}(1+r\tilde{\gamma}_{m-(k-1)r}^{(h)}) + r\tilde{\gamma}_{m-(k-1)r}^{(h)}) \|\bb{b}_j\|_2.
%%DIF > %	\text{\tt castdown}(\hat{\bb{P}}_{k'}^{(1)}\cdots\hat{\bb{P}}_{k'}^{(r)}) \\
%%DIF > %	\|\hat{\bb{X}}_{k'} - \bb{X}_{k'}\|_F &= \|\bb{I}^{(l)}\hat{\bb{P}}_{k'}^{(1)}\cdots\hat{\bb{P}}_{k'}^{(r)} -\bb{P}_{k'}^{(1)}\cdots\bb{P}_{k'}^{(r)} \|_F \leq (1+u^{(l)})(1+r\tilde{\gamma}_{m-(k'-1)r})-1,
%%DIF > \end{align*}
%}\begin{equation}
%\DIFadd{\|\fl_{TC}(\hat{\bb{X}}_{k}^{(l)}\bb{b}_j)-\bb{X}_k\bb{b}_j\|_2 \leq (\tilde{\gamma}_2^{(l)} +r\tilde{\gamma}_{m-(k-1)r}^{(h)} + r\gamma_2^{(l)}\tilde{\gamma}_{m-(k-1)r}^{(h)}) \|\bb{b}_j\|_2, 
%}\end{equation}
%\DIFadd{since the rounding errors from the bFMAs are small in comparison to the errors from casting down the WY representation built in high precision.
%The corresponding matrix error bound is
%}\begin{equation}
%\DIFadd{\|\fl_{TC}(\hat{\bb{X}}_{k}^{(l)})-\bb{X}_k\|_F \leq \tilde{\gamma}_2^{(l)} +r\tilde{\gamma}_{m-(k-1)r}^{(h)} + r\tilde{\gamma}_2^{(l)}\tilde{\gamma}_{m-(k-1)r}^{(h)}.\label{eqn:mpdeltX}
%}\end{equation}
%%DIF > where $\Delta^{(l)}\bb{X}_k = \fl_{TC}(\hat{\bb{X}}_{k}^{(l)})-\bb{X}_k$.
%
%\DIFadd{We can finally compute the forward errors from implementing \cref{algo:mpBQR}.
%Consider the $j^{th}$ column of the $\bb{Q}$ factor, which we denote with $\bb{q}_j:=\hat{\bb{Q}}_{mpBQR3}[:,j]$, and let $k = \lfloor j/r\rfloor$.
%Invoking \cref{lem:3.7} with error bounds for $\fl_{TC}(\hat{\bb{X}}_k^{(l)})$'s in \cref{eqn:mpdeltX} results in columnwise error,
%}\begin{align}
%\DIFadd{\|\Delta \bb{q}_j \|_2 }&\DIFadd{\leq -1 + \prod_{k'=1}^k (1+\tilde{\gamma}_2^{(l)})(1+r\tilde{\gamma}_{m-(k'-1)r}^{(h)})}\\ 
%&\DIFadd{\leq k\tilde{\gamma}_{2}^{(l)} + kr\tilde{\gamma}_m^{(h)} + k^2r\tilde{\gamma}_{2}^{(l)}\tilde{\gamma}_m^{(h)}, \label{eqn:mpBQRcol}
%}\end{align} 
%\DIFadd{where $\Delta \bb{q}_j = (\fl_{TC}(\hat{\bb{X}}_1^{(l)})\cdots\fl_{TC}(\hat{\bb{X}}_k^{(l)}) - \bb{X}_1\cdots\bb{X}_k )\hat{\bb{e}}_j.$
%Summing over the columns to find a matrix norm error bound yields
%}\begin{equation}
%\DIFadd{\|\hat{\bb{Q}}_{mpBQR}-\bb{Q}\|_F \leq n^{1/2}\left(\tilde{\gamma}_{N}^{(l)} + n\tilde{\gamma}_m^{(h)}\right),
%}\end{equation}
%\DIFadd{where the summation of the third term in \cref{eqn:mpBQRcol} is swept under the tilde notation in $n^{1/2} \tilde{\gamma}_{N}^{(l)}$.
%This bound shows that \cref{algo:mpBQR} only adds $n^{1/2}\tilde{\gamma}_{N}^{(l)}$ order errors to the bounds in \cref{eqn:BQRmat}.
%Using that $u^{(l)}=M_{l,h}u^{(h)}$, this increase corresponds to a multiplicative factor shown below,
%}\begin{equation}
%\DIFadd{n^{1/2}\tilde{\gamma}_{N}^{(l)} + n^{(3/2)}\tilde{\gamma}_m^{(h)} \approx \left(1+\frac{M_{l,h}}{rm}\right)n^{(3/2)}\tilde{\gamma}_m^{(h)}. \label{eqn:mpBQR3}
%}\end{equation}
%\DIFadd{Therefore, the loss in accuracy due to mixed precision computing is relatively small when the disparity in precision ($M_{l,h}$) is small in comparison to the block size, $mr$.
%However, as $r$ grows large, $N=n/r$ decreases which then reduces the portion of }{\tt \DIFadd{mpBQR3}} \DIFadd{performed using level-3 BLAS operations and increases the size of high precision HQR being performed at each block.
%Whether this loss in accuracy in the worst-case scenario is worth the speed-ups from using mixed precision hardware is an open question that can be tackled in future research.
%Our analysis shows that the block size $r$, the dimension of the input matrix $m,n$, and hardware specificities will be contributing factors. 
%}
%
%\subsubsection{\DIFadd{TSQR round down at block level: }{\tt \DIFadd{mpTSQR3}}}\label{sec:mp-3t}
%%DIF > Unlike BQR, which is rich in level-3 BLAS operations, TSQR d
%%DIF > Since the majority of FLOPs in BQR use level-3 BLAS operations, it was simple to adapt TensorCore block FMAs within the algorithm.
%%DIF > 
%%DIF > TSQR is best suited for tall-and-skinny matrices where $m\gg n$. 
%%DIF > The majority of the computational gains are from parallelizing the QR factorizations of the initial level blocks, which are performed by different node/tasks ideally.
%\DIFadd{Unlike BQR which is rich in level-3 BLAS operations, the variant of TSQR in \cref{algo:par_tsqr} uses none.
%Therefore, we modify \cref{algo:par_tsqr} by replacing all instances of }{\tt \DIFadd{hh\_mult}} \DIFadd{with level-3 BLAS operations.
%We omit presenting the exact algorithm for mixed precision variant of TSQR in this paper, but consider computing the HQR of each block in high precision and build and store the WY representation of the HH transformations in low precision as we did in lines (3-6) of \cref{algo:mpBQR}.
%The low precision WY representation is then applied with TensorCore bFMAs when building the $\bb{Q}$ factor (lines 11-16 of \cref{algo:par_tsqr}). 
%}
%
%%DIF > We consider applying $n$ HH transformations with level-3 BLAS operations by building the WY representation using high precision arithmetic, casting them down and then applying the update with TensorCore block FMAs as in lines 3-6 of \cref{algo:mpBQR}.
%%DIF > but for our mixed precision bFMA TSQR variant, the WY transformations are only used when building the $\bb{Q}$ factor.
%%DIF > Note that for all blocks in all levels, exactly $n$ HH transformations of lengths either $m2^{-L}$ or $2n$ are applied via {\tt hh\_mult}, and let $m' := \max\{m2^{-L},2n\}$ be the larger of the two. 
%%DIF > The multiplication by the $\bb{Y}$ factor requires at most $m'$-length inner products and a cast down operation and the multiplication by the $\bb{W}$ factor requires $n$-length inner products and another cast down operation.
%%DIF > The errors accumulated from these actions are bounded by $\cO(3u^{(l)}+\tilde{\gamma}_{m'n}^{(h)})$ componentwise.
%%DIF > 	%\item Now, consider applying the $n$ HH transformations by forming the operator explicitly then computing a single matrix-matrix product.
%%DIF > 	We form the operator using the same steps as forming the $\bb{Q}$ factor in HQR in high precision arithmetic, then cast the result down. 
%%DIF > 	The construction of the operator and the cast down result in error bounded by $\cO(u^{(l)}+\tilde{\gamma}_{m'n}^{(h)})$, and the matrix-matrix product requires $m'$-length inner products and another cast down operation. 
%%DIF > 	This option also accrues error bounded by $\cO(2u^{(l)}+\tilde{\gamma}_{m'n}^{(h)})$.
%%DIF > \end{enumerate}
%%DIF > This requires more FLOPs than in the standard algorithm implemented with level-2 BLAS operations, since the same number of level-2 BLAS operations are required to form the matrices required for the level-3 variants. 
%%DIF > The level-3 variants are built during the formation of the $\bb{R}$ factor and can be reused when forming the $\bb{Q}$ factor.
%%DIF > Notice that unlike BQR which is rich in level-3 BLAS operations by design, it is not clear whether implementing TSQR with level-3 BLAS operations has obvious benefits.
%%DIF > Regardless, we still perform a rounding error analysis of TSQR performed with mixed precision level-3 BLAS operations, i.e. TensorCore block FMAs.
%\paragraph{\DIFadd{Rounding Error analysis}} \DIFadd{The analysis in \cite{Mori2012} shows that each column of $\bb{Q}$ is transformed by $n$ HH transformations of length $2n$ from levels $L:-1:1$, and another set of $n$ HH transformations of length $m2^{-L}$ at level $0$.
%Let us represent the WY representation at the $j^{th}$ block of level $i$ and its bFMA counterpart as $\bb{X}_j^{(i)}$ and $\fl_{TC}(\hat{\bb{X}}_j^{(i)})$.
%Then, we can use \cref{eqn:mpdeltX} to form backward error  
%}\begin{equation}
%\DIFadd{\|\fl_{TC}(\hat{\bb{X}}_j^{(i)})-\bb{X}_j^{(i)})\|_F \leq \tilde{\gamma}_2^{(l)} +n\tilde{\gamma}_{m'}^{(h)} + n\tilde{\gamma}_2^{(l)}\tilde{\gamma}_{m'}^{(h)}, \;\; m' = \begin{cases}
%m2^{-L}, &i=0\\
%2n, & i = 1 : L
%\end{cases}.
%}\end{equation}
%%DIF >  and apply \cref{lem:3.7}$(1+2u^{(l)})$ to every set of $n$ HH transformations in each level.
%%DIF > There are two low precision rounding errors in each block per level since casting down the matrix operator formed with high precision is cast down to the low precision, and the matrix-matrix product used in applying this operator with TensorCore block FMAs incurs another low precision rounding error. 
%\DIFadd{We can now modify the analysis in \cite{Mori2012} by replacing $n\tilde{\gamma}_{m2^{-L}}$ and $n\tilde{\gamma}_{2n}$ with }\[\DIFadd{(1+\tilde{\gamma}_2^{(l)})(1+n\tilde{\gamma}_{m2^{-L}}^{(h)})-1,\quad\text{and}\quad (1+\tilde{\gamma}_2^{(l)})(1+n\tilde{\gamma}_{2n}^{(h)})-1,}\]
%\DIFadd{and apply \cref{lem:3.7}.
%Then, the factors formed by }{\tt \DIFadd{mpTSQR3}} \DIFadd{are denoted by $\hat{\bb{R}}_{mpTSQR3},\hat{\bb{Q}}_{mpTSQR3}$ and the error bounds for the $j^{th}$ column of the triangular factor and the orthogonal factor are
%}\begin{align*}
%\DIFadd{\|(\hat{\bb{R}}_{mpTSQR3}- \bb{R})}[\DIFadd{:,j}]\DIFadd{\|_2 }&\DIFadd{\leq \tilde{\gamma}_{L+1}^{(l)}+n\left(L\tilde{\gamma}_{2n}^{(h)}+\tilde{\gamma}_{m2^{-L}}^{(h)}\right)\|\bb{A}[:,j]\|_2\label{eqn:mpTSQR1},}\\
%%DIF > \end{align*}
%%DIF > Summing up the columns for a matrix norm error bound, we result in 
%%DIF > \begin{align*}
%\DIFadd{\|\hat{\bb{Q}}_{mpTSQR3} - \bb{Q}\|_F }&\DIFadd{\leq n^{1/2}\tilde{\gamma}_{L+1}^{(l)}+n^{3/2}\left(L\tilde{\gamma}_{2n}^{(h)}+\tilde{\gamma}_{m2^{-L}}^{(h)}\right).
%}\end{align*}
%\DIFadd{Converting the low precision rounding errors as a fraction of the TSQR error bound in \cref{eqn:tsqrQ} to quantify the impact of modifying \cref{algo:par_tsqr} to utilize bFMAs yields
%}\begin{equation}
%\DIFadd{n^{1/2}\tilde{\gamma}_{L+1}^{(l)}+n^{3/2}\left(L\tilde{\gamma}_{2n}^{(h)}+\tilde{\gamma}_{m2^{-L}}^{(h)}\right) = \left(1+ \frac{M_{l,h}(L+1)}{n(2nL+m2^{-L})}\right)n^{3/2}\left(L\tilde{\gamma}_{2n}^{(h)}+\tilde{\gamma}_{m2^{-L}}^{(h)}\right).\label{eqn:mpTSQR3}
%}\end{equation}
%\DIFadd{Like in \cref{eqn:mpBQR3}, the disparity in the two precisions, $M_{l,h}$ is compared against the original matrix size $m,n$ and the block size specifications derived from $L$.
%%DIF > TODO: conclusion maybe a parameter regime analysis for m, n, L, M?
%Let us consider the shallowest, middle, and the deepest levels of TSQR that are possible given some matrix in $\R^{m\times n}$.
%All three cases in \cref{table:mpTSQR3} show that }{\tt \DIFadd{mpTSQR3}} \DIFadd{on sufficiently large matrices may yield errors closer to the high precision implementation, and the optimal choice for $L$ depends on $m,n$. 
%}\begin{table}[H]
%	\center
%	\begin{tabular}{||c|c|c|c||} 
%		\hline
%		\DIFaddFL{Number of levels, $L$}& \DIFaddFL{$1$ }& \DIFaddFL{$\frac{1}{2}\log_2(m/n)$ }& \DIFaddFL{$-1+\log_2(m/n)$ }\\ \hline
%		\DIFaddFL{$\frac{(L+1)}{n(2nL+m2^{-L})}$}&  \DIFaddFL{$1/(n^2+m/4)$ }&\DIFaddFL{$1/\left(2n^2+\frac{m^{1/2}n^{3/2}}{\log_2(m/n)}\right)$ }& \DIFaddFL{$1/(2n^2)$}\\ \hline
%	\end{tabular}
%	\caption{\DIFaddFL{Error bounds for $\|\Delta \bb{Q}_{mpTSQR3}\|_F$ for varying $L$'s.}} 
%	\label{table:mpTSQR3}
%\end{table} 
%\vspace{-1cm}
%%DIF > Let us consider the example where $m2^{-L}=2n$ as we did in \cref{sec:TSQR}. 
%%DIF > Now, we compare {\tt mpTSQR3} against {\tt mpBQR3} with respect to their worst-case error bounds. 
%%DIF > TSQR has a lower worst-case error bound than HQR when integers $m, n > 0$, and $L\geq0$ satisfy
%%DIF > \begin{equation*}
%%DIF > 1\gg n^{3/2}\gamma^{(m)} \gg n^{3/2}(\gamma^{(\frac{m}{2^L})}+L\gamma^{(2n)}).
%%DIF > \end{equation*}
%%DIF > %for TSQR to have a better worst-case error bound than HQR.
%%DIF > Let us consider as an example the case when $\frac{m}{2^L}=2n$.
%%DIF > Then, the HQR bound is $2^L/(L+1)$ larger than the bound for TSQR with $L$ levels. 
%%DIF > Let us now consider a WY variant of TSQR, where all instances of {\tt qr} (lines 6,7,9 of \cref{algo:par_tsqr}) are followed by {\tt buildWY} (see \cref{algo:buildWY}), and all instances of {\tt hh\_mult} is replaced by a WY update (line 6 of \cref{algo:blockHQR}).
%%DIF > We additionally impose a mixed precision assumption similar to \cref{sec:mp-3b}, where we store all WY representations of HQR within the for-loop (lines 4-8) of \cref{algo:par_tsqr} in low precision, and consider the construction of the $\bb{Q}$ factor.
%%DIF > We can assume that each $2n$-by-$n$ and $m2^{-L}$-by-$n$ size matrices can fit into memory and only introduce one cast down for each $\bb{Q}_j^{(i)}$ block, where $i=1:L-1$ and $j=1:2^{i-1}$.
%%DIF > Let us compute lines 9-10 in the higher precision, which introduces an error of order $n\tilde{\gamma}_{2n}^{(h)}$.
%%DIF > In levels $L-1$ to $1$, each WY update adds error $u^{(l)}+n\tilde{\gamma}_{2n}^{(h)}$, and the final construction at the $0^{th}$ level (line 16), the WY update adds error $u^{(l)} + n\tilde{\gamma}_{m2^{-L}}^{(h)}$.
%
%%DIF > \input{mp2}
%\subsection{\DIFadd{Round down at inner product: level-2 BLAS mixed precision setting}}\label{sec:mp-2}
%\DIFadd{While the previous section discussed blocked variants of HQR that can be easily adapted for the mixed precision setting specific to TensorCore bFMA's, we want to provide a more general mixed precision environment in this section.
%Recall that HQR, BQR, and TSQR all rely on HH transformations in one way or another, and implementations of HH transformations are expressed by \cref{eqn:effH}.
%This implementation capitalizes on the rank-1 update structure of HH transformations where the predominant share of FLOPs is spent on an inner product, and computing the HH vector and constant also rely heavily on inner products.
%Therefore, nearly all of the computational tasks for \cref{algo:hhQR,algo:blockHQR,algo:par_tsqr} are attributed to the inner product, which is important in other linear algebra tools such as projections, matrix-vector, and matrix-matrix multiply.
%Consequently, we return to \cref{assump:mp}, where every inner product is cast down to the lower precision as shown in \cref{eqn:aftercd}. 
%We denote HQR, BQR, and TSQR computed with \cref{assump:mp} with }{\tt \DIFadd{mpHQR2}}\DIFadd{, }{\tt \DIFadd{mpBQR2}}\DIFadd{, and }{\tt \DIFadd{mpTSQR2}}\DIFadd{, where the }{\tt \DIFadd{2}} \DIFadd{represents the mixed precision procedure computed at a level-2 BLAS operation.
%%DIF > \cref{sec:background}, where every inner product is cast down to the lower precision as shown in \cref{eqn:aftercd}.
%}\subsubsection{\DIFadd{HQR round down at inner product: }{\tt \DIFadd{mpHQR2}}}
%\DIFadd{Consider forming a HH transformation that zeros out $\bb{x}\in\R^m$ below the the $i^{th}$ element. 
%We need to compute $\sigma$, $\beta$, $\tilde{\bb{v}}_1$, and $\bb{v}$ as defined in \cref{sec:HQR},
%}\begin{align}
%\DIFadd{\fl(\sigma) }&\DIFadd{= \fl(-\rm{sign}(\bb{x}[1])\|\bb{x}\|_2) = \sigma + \Delta \sigma,\;\;|\Delta\sigma| \leq \left(\gamma_{2}^{(l)}+\gamma_{m}^{(h)}+\gamma_{2}^{(l)}\gamma_{m}^{(h)}\right)|\sigma|,\label{eqn:mpsigma}}\\
%\DIFadd{\fl(\bb{v}'}[\DIFadd{1}]\DIFadd{)}& \DIFadd{=\bb{v}'}[\DIFadd{1}] \DIFadd{+ \Delta \bb{v}'}[\DIFadd{1}] \DIFadd{= (1+\dd^{(l)}) (\bb{x}[1]-\sigma-\Delta\sigma), \;\;|\Delta\bb{v}'}[\DIFadd{1}]\DIFadd{| \leq (\gamma_{3}^{(l)}+\tilde{\gamma}_{m}^{(h)})|\bb{v}'}[\DIFadd{1}]\DIFadd{| \label{eqn:mpv1}}\\
%\DIFadd{\fl(\beta) }&\DIFadd{= \beta +\Delta \beta= (1+\dd^{(l)})\left(-\bb{v}'}[\DIFadd{1}]\DIFadd{/\hat{\sigma}\right), \;\; |\Delta\beta| \leq (\gamma_{8}^{(l)}+\tilde{\gamma}_{m}^{(h)})|\beta|, \label{eqn:mpbeta}}\\
%%DIF > \end{align}
%%DIF > \begin{align}
%\DIFadd{\fl(\bb{v}[j])	}&\DIFadd{= \bb{v}[j] + \Delta \bb{v}[j]\text{ where }|\Delta \bb{v}[j]|\leq 
%%DIF > 	\begin{cases}
%%DIF > 	0,& j=1\\
%(\gamma_{7}^{(l)} + \tilde{\gamma}_{m}^{(h)})|\bb{v}_j|,j=2:m-i+1 \label{eqn:mpv}.
%%DIF > 	\end{cases}  
%}\end{align}
%\DIFadd{These bounds on $\Delta\sigma$, $\Delta \bb{v}'[1]$, $\Delta \beta$, and $\Delta \bb{v}[j]$ are computed by using the rules from \cref{lem:mp} on the analysis shown in \cref{sec:HQR}.
%Using these, we can formulate the mixed precision version of \cref{eqn:applyP} where $\hat{\bb{y}}=\fl(\bb{P_vx})\in\R^m$ is implemented via \cref{eqn:effH}.
%Note that the inner product $\hat{\bb{v}}^{\top}\bb{x}$ via \cref{assump:mp}, and all other operations are done in the lower precision.
%Then, the transformed vector is bounded by
%}\begin{equation}
%\DIFadd{\hat{\bb{y}} = \bb{y}+\Delta \bb{y},\;\; \|\Delta \bb{y}\|_2 \leq (\gamma_{25}^{(l)} + \tilde{\gamma}_{m}^{(h)})\|\bb{y}\|_2.\label{eqn:mpdelty}
%}\end{equation}
%\DIFadd{Thus, a backward error can be formed using $\Delta \bb{P_v} = \Delta \bb{y}\bb{x}^{
%	\top}/\|\bb{x}\|_2^2$,
%}\begin{equation}
%\DIFadd{\hat{\bb{y}} = (\bb{P_v} + \Delta \bb{P_v})\bb{x},\;\; \|\Delta \bb{P_v}\|_F\leq (\gamma_{25}^{(l)} + \tilde{\gamma}_{m}^{(h)}). \label{eqn:mpapplyP}
%}\end{equation}
%\DIFadd{Now, we form the error bounds for applying $n$ HH transformations to $\bb{x}$ using \cref{lem:3.7},
%}\begin{align}
%\DIFadd{\hat{\bb{z}} }&\DIFadd{= \fl(\bb{P}_1\cdots\bb{P}_n\bb{x})=\bb{Q} (\bb{x} +\Delta \bb{x}) = (\bb{Q} + \Delta \bb{Q})\bb{x},}\\
%\DIFadd{\|\Delta \bb{y}\|_2 }&\DIFadd{\leq (\tilde{\gamma}_n^{(l)}+n\tilde{\gamma}_m^{(h)})\|\bb{x}\|_2,\;\; \|\Delta \bb{Q}\|_F\leq (\tilde{\gamma}_n^{(l)}+n\tilde{\gamma}_m^{(h)}).\label{eqn:mp19.3}
%}\end{align} 
%\DIFadd{Note that we use the $\tilde{\gamma}^{(l)}$ notation, where the small integer $c$ is now required to be $\cO(25)$.
%The analogous mixed precision QR factorization error bounds are shown in \cref{thm:mpHQR}.
%}\begin{theorem}
%	\label{thm:mpHQR}
%	\DIFadd{Let $\bb{A}\in\R^{m\times n}$ with $m\geq n$ have full rank, $n$. 
%	Let $\hat{\bb{Q}}_{mpHQR2}\in\R^{m\times n}$ and $\hat{\bb{R}}\in\R^{n\times n}_{mpHQR2}$ be the thin QR factors of $\bb{A}$ obtained via \cref{algo:hhQR} with mixed precision FLOPs where inner products are computed in precision $h$ then cast down.
%	All other operations are carried out in precision $l$.
%	Then,
%	}\begin{align}
%	%DIF > 	\hat{\bb{R}} &= \bb{R} + \Delta \bb{R}__{mpHQR} = \fl(\hat{\bb{P}}_n\cdots\hat{\bb{P}}_1 \bb{A}),\\
%	%DIF > 	\hat{\bb{Q}} &= \bb{Q} + \Delta \bb{Q} = \fl(\hat{\bb{P}}_1\cdots\hat{\bb{P}}_n \bb{I}),\\
%	\DIFadd{\|\Delta \bb{R}_{mpHQR2}[:,j]\|_2}&\DIFadd{\leq (\tilde{\gamma}_n^{(l)}+n\tilde{\gamma}_m^{(h)}) \|\bb{A}[:,j]\|_2,\;\; \|\Delta \bb{R}_{mpHQR2}\|_F\leq (\tilde{\gamma}_n^{(l)}+n\tilde{\gamma}_m^{(h)}) \|\bb{A}\|_F \label{eqn:mpHQR2R}}\\
%	\DIFadd{\|\Delta \bb{Q}[:,j]_{mpHQR2}\|_2}&\DIFadd{\leq (\tilde{\gamma}_n^{(l)}+n\tilde{\gamma}_m^{(h)}),\;\; \|\Delta \bb{Q}_{mpHQR2}\|_F \leq n^{1/2} (\tilde{\gamma}_n^{(l)}+n\tilde{\gamma}_m^{(h)})\label{eqn:mpHQR2Q}.
%	}\end{align}
%	%DIF > 	Let $\bb{A}+\Delta \bb{A} = \hat{\bb{Q}}\hat{\bb{R}}$, where $\hat{\bb{Q}}$ and $\hat{\bb{R}}$ are obtained via Algorithm~\ref{algo:hhQR}.
%	%DIF > 	Then the backward e|rror is
%	%DIF > 	\begin{equation}
%	%DIF > 	\|\Delta \bb{A}\|_F \leq n^{3/2}\tilde{\gamma}_{m}\|\bb{A}\|_F.
%	%DIF > 	\end{equation}
%\end{theorem}
%\DIFadd{Unsurprisingly, the inner product mixed precision setting yields higher error bounds as it uses more low precision arithmetic than the settings described in \cref{sec:mp-3}. 
%In the next sections we analyze using }{\tt \DIFadd{mpHQR2}} \DIFadd{instead of }{\tt \DIFadd{HQR}} \DIFadd{within \cref{algo:blockHQR,algo:par_tsqr}.
%%DIF > TODO: what do these bounds mean?
%}
%
%\subsubsection{\DIFadd{BQR round down at inner product: }{\tt \DIFadd{mpBQR2}}}
%\DIFadd{Now, we analyze \cref{algo:blockHQR} implemented with \cref{assump:mp}. 
%At the $k^{th}$ block, we first apply the mixed precision HQR summarized in \cref{thm:mpHQR}.
%Next, we construct the WY representation, where we can now use \cref{eqn:mpdelty,eqn:mpapplyP,lem:3.7} to form
%}\begin{equation}
%\DIFadd{\|\hat{\bb{X}}_{k}^{(l)}- \bb{X}_k\|_F = \|(\hat{\bb{P}}_k^{(1)}\cdots \hat{\bb{P}}_k^{(r)})-(\bb{P}_k^{(1)}\cdots \bb{P}_k^{(r)}))\|_F \leq \tilde{\gamma}_{r}^{(l)} + r\tilde{\gamma}_{m}^{(h)}.
%}\end{equation}
%\DIFadd{Then, the 2-norm bound for the $j^{th}$ column of the $\bb{R}$ factor and the Frobenius norm bound for the orthogonal factor resulting from }{\tt \DIFadd{mpBQR2}} \DIFadd{are
%}\begin{align}
%\DIFadd{\|\hat{\bb{R}}_{mpBQR2}[:,j]\|_2 }&\DIFadd{= \|\hat{\bb{X}}_1\cdots\hat{\bb{X}}_N\bb{A}[:,j]\|_2\leq\left( N\tilde{\gamma}_{r}^{(l)} + n\tilde{\gamma}_{m}^{(h)}\right)\|\bb{A}[:,j]\|_2,}\\
%%DIF > \end{equation}
%%DIF > and the Frobenius norm error bound for the orthogonal factor is, 
%%DIF > \begin{equation}
%\DIFadd{\|\hat{\bb{Q}}_{mpBQR2}\|_F }&\DIFadd{\leq n^{1/2}\left(N\tilde{\gamma}_{r}^{(l)} + n\tilde{\gamma}_{m}^{(h)}\right) \approx \left(1+\frac{M_{l,h}}{m}\right)n^{3/2}\tilde{\gamma}_{m}^{(h)}. \label{eqn:mpBQR2}
%}\end{align}
%\DIFadd{Note that this error bound is of the same order as the error bound for }{\tt \DIFadd{mpHQR2}}\DIFadd{, shown in \cref{eqn:mpHQR2Q}.
%The corresponding error bound for }{\tt \DIFadd{mpBQR3}} \DIFadd{of \cref{sec:mp-3b} yielded low precision errors $r$ times smaller than that from 
%%DIF > a multiplicative factor of $(1+\frac{M_{l,h}}{rm})$ (see \cref{eqn:mpBQR3}).
%%DIF > This implies that 
%using \cref{assump:mp} inner products,
%%DIF >  introduces low precision error $r$ times larger than the low precision errors incurred from {\tt mpBQR3} (\cref{algo:mpBQR}),
%an unsurprising result as intermediate results are cast down more often in }{\tt \DIFadd{mpBQR2}}\DIFadd{.
%Furthermore, the $\tilde{\gamma}^{(l)}$ in this section requires $c=\cO(25)$, whereas the same notation in \cref{sec:mp-3b} assumes $c$ to be a \emph{small} positive integer.
%Therefore, the numerical stability of }{\tt \DIFadd{mpBQR2}} \DIFadd{is guaranteed at smaller matrix sizes than the numerical stability of }{\tt \DIFadd{mpBQR3}} \DIFadd{and BQR in high precision.
%While it is technically possible that the low precision errors introduced from utilizing \cref{assump:mp} do not dominate the errors incurred in }{\tt \DIFadd{mpBQR2}} \DIFadd{and }{\tt \DIFadd{mpHQR2}} \DIFadd{when $m\gg M_{l,h}$ and can result in accuracy comparable to that of }{\tt \DIFadd{mpBQR3}} \DIFadd{and high precision BQR, our numerical results in \cref{sec:NE} show that }{\tt \DIFadd{mpHQR2}} \DIFadd{is already unstable at $m\approx M_{l,h}$.
%%DIF > the mixed precision inner product can still had non-leading order error terms to the worst-case scenario.
%}\subsubsection{\DIFadd{TSQR round down at inner product: }{\tt \DIFadd{mpTSQR2}}}
%\DIFadd{Finally, we consider using \cref{assump:mp} in \cref{algo:par_tsqr}.
%This corresponds to replacing every instance of $n\tilde{\gamma}_{m'}$ for $m'\in\{2n, m2^{-L}\}$ in \cref{thm:moriTSQR} with $\tilde{\gamma}_n^{(l)} + n\tilde{\gamma}_{m'}^{(h)}$.
%We first consider the norm errors for the $j^{th}$ column of the $\bb{Q}$ factor computed by this mixed precision variant of \cref{algo:par_tsqr},
%}\begin{equation}
%\DIFadd{\|\hat{\bb{Q}}_{mpTSQR2}[:,j] -\bb{Q}[:,j]\|_2 \leq (L+1)\tilde{\gamma}_n^{(l)} +n(\tilde{\gamma}_{m2^{-L}}^{(h)} + L\tilde{\gamma}_{ 2n}^{(h)}).\label{eqn:mptsqr2Qcol}
%}\end{equation} 
%\DIFadd{Then, the matrix norm error bound is 
%}\begin{align}
%\DIFadd{\|\hat{\bb{Q}}_{mpTSQR2}-\bb{Q}\|_F \leq n^{1/2}(L+1)\tilde{\gamma}_n^{(l)} +n^{3/2}(\tilde{\gamma}_{m2^{-L}}^{(h)} + L\tilde{\gamma}_{ 2n}^{(h)})}\\
%\DIFadd{\approx \left(1+ \frac{M_{l,h}L}{m2^{-L}+ 2Ln}\right)n^{3/2}(\tilde{\gamma}_{m2^{-L}}^{(h)} + L\tilde{\gamma}_{ 2n}^{(h)}),\label{eqn:mptsqr2Q}
%}\end{align}
%\DIFadd{and contributes larger low precision rounding errors than in \cref{eqn:mpTSQR3}.
%%DIF > \paragraph{{\tt mpTSQR2} and {\tt mpHQR2} error bound comparison}
%If the }{\tt \DIFadd{mpTSQR2}} \DIFadd{error bound were to outperform that of }{\tt \DIFadd{mpHQR2}}\DIFadd{, we now need integers $m, n > 0$, and $L\geq 0$ that satisfy
%}\begin{equation*}
%\DIFadd{1\gg n^{1/2}\left(\tilde{\gamma}_{n}^{(l)} + n\tilde{\gamma}_{m}^{(h)}\right) \gg n^{1/2}\left((L+1)\tilde{\gamma}_n^{(l)} +n(\tilde{\gamma}_{m2^{-L}}^{(h)} + L\tilde{\gamma}_{ 2n}^{(h)})\right).%DIF > ,
%}\end{equation*}
%%DIF > where $d=\lfloor\frac{(m-1) u_s}{u_w}\rfloor$, $d_1 = \lfloor{(\frac{m}{2^L}-1)\frac{u_s}{u_w}\rfloor}$, and $d_2 =\lfloor \frac{(2n-1)u_s}{u_w}\rfloor$. 
%
%\DIFaddend In contrast to the analysis for uniform precision settings, large $L$ values do not necessarily reduce the error bounds of TSQR. 
%While large $L$ can imply $m\gg m2^{-L}+2Ln$, it does not always lead to $d \gg d_1+Ld_2$.
%Although the theoretical error bounds do not give a clear indication of the worst-case performances of HQR and TSQR in mixed-precision settings, TSQR outperformed HQR on ill-conditioned matrices within our numerical simulations.
%
%These experiments are discussed in detail in the next section.%Section~\ref{sec:NE}.
%
%
%
%\DIFdelbegin \subsection{\DIFdel{Numerical Experiment}}
%%DIFAUXCMD
%\addtocounter{subsection}{-1}%DIFAUXCMD
%\DIFdelend %DIF > %TODO: conclude
%\DIFaddbegin 
%
%
%\section{\DIFadd{Numerical Experiments}}\DIFaddend \label{sec:NE}
%%DIF < \subsection{Single Precision}
%\DIFdelbegin \DIFdel{In Section~\ref{sec:HTSQR}, we theorized that conditions exist where TSQR could outperform HQR and that these conditions were hard to identify in mixed-precision settings. 
%An empirical comparison of these two QR factorization algorithms in double precision can be found in \cite{Mori2012}, where they conclude that deeper TSQR tends to produce more accurate QR factorizations than HQR.
%However, using TSQR with deep levels (large $L$) can actually start to perform worse than TSQR with shallower levels (smaller $L$), since deeper levels require more FLOPs.
%We instead focused on comparing HQR and TSQR performances in a mixed-precision setting.
%Our numerical simulations show that TSQR can still outperform HQR in low, mixed-precision settings in practice even though the theoretical bounds do not guarantee stability.
%%DIF < are all $\mathcal{O}(10)$ (c.f. Table~\ref{table:HTSQRerr}), and get larger as we go from HQR to TSQR, and larger as we increase $L$. 
%Our empirical results do not behave as the theoretical bounds suggest, and even show opposite trends at times. 
%This discrepancy highlights the shortcomings of deterministic error bounds that are too pessimistic. }%DIFDELCMD < \par
%%DIFDELCMD < %%%
%%DIF < showed that there exists conditions in which the worst-case rounding error bound for TSQR can be expected to be smaller than that of HQR. 
%%DIF < These conditions are met more easily in high, uniform-precision settings than low mixed-precision settings.
%%DIF < The success of TSQR in high, uniform-precision settings is explored and discussed in \cite{Mori2012}, where they conclude that the computed relative error, $\|\hat{\bb{Q}}\hat{\bb{R}}-A\|_F$, tends to decrease as they increase $L$, but only up to a certain point.
%%DIF < This may appear to contradict our conclusion in Section~\ref{sec:upHTSQR}, where we found that as long as $\frac{L}{2^{L-1}}\ll 1$, there e 
%%DIF < \subsubsection{Experiment Details}
%\DIFdelend %DIF > \input{NE2}
%\DIFaddbegin 
%
%\DIFadd{We conducted several numerical experiments to confirm the validity of the error bounds formed in \cref{sec:mpanalysis}.
%First, we tested \cref{algo:hhQR,algo:blockHQR,algo:par_tsqr,algo:mpBQR}, }{\tt \DIFadd{mpHQR2}}\DIFadd{, }{\tt \DIFadd{mpBQR2}}\DIFadd{, and }{\tt \DIFadd{mpTSQR2}} \DIFadd{for varying matrix sizes.
%Then, we compared the performance of }{\tt \DIFadd{mpHQR2}} \DIFadd{and }{\tt \DIFadd{mpTSQR2}}\DIFadd{, and lastly, tested varying block sizes in \cref{algo:mpBQR} for a fixed matrix size. 
%}\DIFaddend We used Julia\DIFdelbegin \DIFdel{v1.0.4 for all of the numerical simulations. 
%This programming language allows half precision storage as well as }\DIFdelend \DIFaddbegin \DIFadd{, a programming language which allows fp16 storage and }\DIFaddend {\tt castup} and {\tt castdown} operations \DIFdelbegin \DIFdel{to and from single and double precisions, but has no half precision }\DIFdelend \DIFaddbegin \DIFadd{between types in }{\DIFadd{fp16, fp32, fp64}}\DIFadd{, but no built-in fp16 }\DIFaddend arithmetic.
%Therefore, we relied on using \DIFdelbegin \DIFdel{Algorithm~\ref{algo:simulate} }\DIFdelend \DIFaddbegin \DIFadd{\cref{algo:simulate} }\DIFaddend for $f\in \text{OP} \cup\{{\tt dot\_product}\}$ to simulate \DIFdelbegin \DIFdel{half and mixed-precision arithmetic operations.%DIF < Specifically, we approximated , and 
%%DIF < To simulate the mixed-precision setting described in Assumption~\ref{assump:mp} with $u_p = 0$ (which implies $z=1$), we used Algorithm~\ref{algo:simulate} for the dot product routine.
%%DIF < That is, for $\bb{x}_{\text{half}},\bb{y}_{\text{half}}\in\F_{\text{half}}^m$, we approximated $\fl(\bb{x}_{\text{half}}^{\top}\bb{y}_{\text{half}})$ with {\tt simHalf}$(${\tt dot\_product} $, \bb{x}_{\text{half}}, \bb{y}_{\text{half}})$ to simulate mixed-precision dot products.
%%DIF < We used Algorithm~\ref{algo:simulate} for all other operations as well to simulate half precision arithmetic.
%For HQR, we created a mixed-precision version of the LAPACK routine xGEQRF, where the dot product subroutine was approximated by $\fl(\bb{x}_{\text{half}}^{\top}\bb{y}_{\text{half}})$ with }\DIFdelend \DIFaddbegin \DIFadd{\cref{assump:mp} and TensorCore bFMAs.}\par
%
%\DIFadd{In \cref{sec:algo,sec:mpanalysis}, we gave the forward error bounds for $\bb{R}$ and $\bb{Q}$ separately. 
%Since our numerical experiments instead measure a backward error, $\|\hat{\bb{Q}}\hat{\bb{R}}-\bb{A}\|_F$, and an orthogonal error, $\|\hat{\bb{Q}}^{\top}\hat{\bb{Q}}-\bb{I}\|_2$, we show how to convert general forward errors into those computed quantities.
%Given $\|(\hat{\bb{R}}-\bb{R})[:,j]\|_2\leq \epsilon_R \|\bb{A}[:,j]\|_2$ and $\|\hat{\bb{Q}}-\bb{Q}\|_F\leq \epsilon_Q$,
%}\begin{align}
%\DIFadd{\|(\hat{\bb{Q}}\hat{\bb{R}}-\bb{A})}[\DIFadd{:,j}]\DIFadd{\|_2 }&\DIFadd{\leq (\epsilon_R+\epsilon_Q + \epsilon_R\epsilon_Q)\|\bb{A}[:,j]\|_2,\;\; j=1:n,\quad \text{see \cite{Higham2002}},}\\
%\DIFadd{\|\hat{\bb{Q}}\hat{\bb{R}}-\bb{A}\|_F }&\DIFadd{\leq n^{1/2}(\epsilon_R+\epsilon_Q + \epsilon_R\epsilon_Q)\|\bb{A}\|_F, \label{eqn:QRA} }\\
%\DIFadd{\|\hat{\bb{Q}}^{\top}\hat{\bb{Q}}-\bb{I}\|_2 }&\DIFadd{\leq \|\hat{\bb{Q}}^{\top}\hat{\bb{Q}}-\bb{I}\|_F \simeq 2\epsilon_Q,\quad\text{see \cite{Mori2012}} \label{eqn:QQI}.
%}\end{align}
%%DIF > \subsection{Varying matrix sizes}
%%DIF > \subsection{Varying block sizes in {\tt mpBQR3}}
%\begin{figure}[h!]%DIF > {r}{.53\textwidth}
%	\centering
%	\vspace{-10pt}
%	\includegraphics[width=0.45\textwidth]{./figures/sizefig.png}
%	\includegraphics[width=0.45\textwidth]{./figures/unblocked.pdf}
%	%DIF > \includegraphics[width=0.45\textwidth]{./figures/mpBQR3-blocksize.png}
%	\caption{\label{fig:sizemp2}\DIFaddFL{Left plot: Relative backward error of all HH QR factorization algorithms discussed in \cref{sec:algo,sec:mpanalysis} with varying matrix sizes. 
%		Right plot: }{\tt \DIFaddFL{mpHQR2}} \DIFaddFL{errors for $4000$-by-$100$ matrices with varying condition numbers.}}
%	%DIF > Right plot: Relative backward error of {\tt mpBQR3} for $8192$-by-$8192$ matrices are shown with block size $r$ ranging from $4$ to $4096$.}
%	\vspace{-10pt}
%\end{figure} 
%%DIF > \begin{wrapfigure}{l}{.4\textwidth}
%%DIF > 	\centering
%%DIF > 	\vspace{-10pt}
%%DIF > 	\includegraphics[width=.3\textwidth]{./figures/unblocked.pdf}
%%DIF > 	\caption{\label{fig:unblocked} HQR errors for matrices with varying condition numbers.}
%%DIF > 	%\vspace{-10pt}	
%%DIF > \end{wrapfigure}
%%DIF > \subsection{Varying condition numbers}
%%DIF > Lastly, we compared {\tt mpHQR2} against {\tt mpTSQR2}.
%\DIFadd{The left plot of \cref{fig:sizemp2} shows the backward error for HQR, BQR, TSQR, and their mixed precision variants.
%We increased the number of rows $m$ from $1000$ to $13949$, 
%%DIF >  $18694$
%while keeping $n=m/4$, $r=n/4$, and $L=2$ and condition number at $1000$ by generating $\bb{A}={\tt castdown}(\bb{Q}_1\bb{D}\bb{Q}_2)$ where $\bb{Q}_1\in\F_h^{m\times n}$, $\bb{Q}_2\in\F_h^{n\times n}$ are orthogonal and $\bb{D}=\mathrm{Diagonal}(\{\log_{10}(0),\cdots, \log_{10}(-3)\})\in\F_h^{n\times n}$.  
%The three clusters each correspond to: top, \cref{assump:mp}; middle, TensorCore bFMAs; and bottom, uniform precision implementations in fp32.
%The high precision and bFMA implementations scale similarly to each other when increasing the matrix size, whereas the \cref{assump:mp} variants grow unstable more quickly.
%In addition, while HQR, BQR, and TSQR perform similarly in high precision and when using bFMAs, }\DIFaddend {\tt \DIFdelbegin \DIFdel{simHalf}\DIFdelend \DIFaddbegin \DIFadd{mpTSQR2}\DIFaddend } \DIFdelbegin \DIFdel{$($}\DIFdelend \DIFaddbegin \DIFadd{is less accurate by a quarter to a half order of magnitude in comparison to }\DIFaddend {\tt \DIFdelbegin \DIFdel{dot\_product}\DIFdelend \DIFaddbegin \DIFadd{mpBQR2}} \DIFadd{and }{\tt \DIFadd{mpHQR2}\DIFaddend }\DIFdelbegin \DIFdel{$, \bb{x}_{\text{half}}, \bb{y}_{\text{half}})$ to simulate the mixed-precision setting described in Assumption~\ref{assump:mp} with $u_p = 0$ (which implies $z=1$), and we used Algorithm~\ref{algo:simulate} on all other basic operations in OP to simulate half/storage precision arithmetic.
%%DIF < Using these simulated operations as subroutines for 
%%DIF < By using these simulated half and mixed precision versions of basic operations at subroutines for our implementation of the LAPACK routine xGEQRF, we 
%%DIF < To implement half and mixed-precision simulations within HQR, we wrote our own versions of it that almost replicates LAPACK routine xGEQRF, where the disparity only comes from the storage format of the information required to build the $\bb{Q}$ factor. 
%This HQR was then used as a subroutine of TSQR as well.
%There are cases where the rounding will differ between the mixed-precision setting and the way we mimic it, i.  
%e., basic operations that are meant to be in half/storage precision arithmetic, but are instead casted up to single and back down, as the tiebreaker within correct rounding may lead to different results than true half/storage precision arithmetic.
%All in all, our experiments nearly replicated the mixed-precision setting we assumed for the error analysis in Sections~\ref{sec:HQRre} and \ref{sec:TSQRre}.
%}\DIFdelend \DIFaddbegin \DIFadd{.
%The specifications for $m,n,L,M_{l,h}$ for this experiment derive the upper bound for $\|\Delta \bb{Q}_{mpTSQR2}\|_F$, \cref{eqn:mptsqr2Q}, to be larger than that of $\|\Delta \bb{Q}_{mpHQR2}\|_F$, \cref{eqn:mpHQR2Q}.
%However, a more careful comparison of }{\tt \DIFadd{mpHQR2}} \DIFadd{and }{\tt \DIFadd{mpTSQR2}} \DIFadd{show that there exists a regime where }{\tt \DIFadd{mpTSQR2}} \DIFadd{can outperform }{\tt \DIFadd{mpHQR2}}\DIFadd{.}\DIFaddend \par
%%DIF < Although we kept the matrix size constant, we varied the condition numbers of these matrices by the method described below.
%%DIF < We speculated that matrices with larger condition numbers would behave closer to the ``worst-case scenario'' with respect to rounding errors.
%%DIF < Table~\ref{table:HTSQRerr} shows the theoretical error bounds from Section~\ref{sec:HQRre} and \ref{sec:TSQRre} that correspond to the conditions of our experiment.
%%DIF < Stability is not guaranteed for any of these QR factorization methods. 
%%DIF < 
%%DIF < \begin{table}[h]
%%DIF < 	\centering
%%DIF < 	\begin{tabular}{||c|c|c|c|c|c|c|c||} 
%%DIF < 		\hline
%%DIF < 		$L$ & $0$ & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ \\ \hline
%%DIF < 		$n^{3/2}(\gamma_w^{(6d_1+ 25)}+L\gamma_w^{(6d_2+ 25)})$ & {\tt 9.36} & {\tt 18.73} & {\tt 28.09} & {\tt 37.46} & {\tt 46.82} & {\tt 56.19} & {\tt 65.55}\\ \hline
%%DIF < 	\end{tabular}
%%DIF < 	\caption{Error bounds for when $m=4000$, $n=100$, $u_w=u_{\text{half}}$, $u_s={\text{single}}$, and $d_1,d_2$ are defined in Section~\ref{sec:TSQRre}. Error bound for HQR is recovered when $L=0$.}
%%DIF < 	\label{table:HTSQRerr}
%%DIF < \end{table}
%\DIFdelbegin %DIFDELCMD < 
%
%%DIFDELCMD < %%%
%%DIF < \paragraph{Constructing Test Matrices}
%\DIFdelend \DIFaddbegin \begin{figure}[h!]%DIF > {r}{.53\textwidth}
%	\centering
%	%DIF > \vspace{-10pt}
%	\includegraphics[width=0.75\textwidth]{./figures/allTSQR2.pdf}
%	\caption{\label{fig:allTSQR} \DIFaddFL{Left plot shows the relative error of QR factorization for matrices with condition numbers ranging from 5.3 to 101, and the right plot shows the errors for matrices with condition numbers ranging from 1.1 to 5.3. }}
%	\vspace{-10pt}
%\end{figure}
%\begin{wrapfigure}{l}{.4\textwidth}
%	\centering
%	\vspace{-10pt}
%	\includegraphics[width=0.4\textwidth]{./figures/mpBQR3-blocksize.png}
%	\caption{\label{fig:mpBQR3}\DIFadd{Relative backward error of }{\tt \DIFadd{mpBQR3}} \DIFadd{for $8192$-by-$8192$ matrices are shown with block size $r$ ranging from $4$ to $4096$.}}
%	\vspace{-10pt}
%\end{wrapfigure}
%\DIFadd{Note that an empirical comparison of the two algorithms implemented in fp64 arithmetic were reported in \cite{Mori2012}, and we omit the comparison against }{\tt \DIFadd{mpBQR2}} \DIFadd{since it performs very similarly to }{\tt \DIFadd{mpHQR2}}\DIFadd{.
%%DIF > TODO: talk why omit BQR? or wait for BQR
%%DIF > The numerical experiments show that TSQR can outperform HQR in mixed precision settings in practice even though the theoretical bounds do not guarantee stability.
%%DIF > In addition, the empirical results do not always behave in the same trend as the theoretical bounds suggest, which highlights the shortcomings of deterministic error bounds that are too pessimistic.
%}\DIFaddend Following example from \cite{Mori2012}, we used $m$-by-$n$ random matrices, \DIFdelbegin \DIFdel{$\bb{A}_{\alpha}$,
%constructed via
%}\begin{displaymath}
%\DIFdel{\bb{A}_{\alpha} = \bb{Q'}(\alpha \bb{E} + \bb{I})/\|\bb{Q'}(\alpha \bb{E} + \bb{I})\|_F,
%%DIFDELCMD < \label{eqn:genRM}%%%
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdelend \DIFaddbegin \DIFadd{$\bb{A}_{\alpha} = \bb{Q'}(\alpha \bb{E} + \bb{I})/\|\bb{Q'}(\alpha \bb{E} + \bb{I})\|_F$,
%%DIF > \label{eqn:genRM}
%%DIF > \end{equation}
%}\DIFaddend where $\bb{Q'}\in\mathbb{R}^{m\times n}$ is \DIFdelbegin \DIFdel{a random orthogonal matrix }\DIFdelend \DIFaddbegin \DIFadd{orthogonal }\DIFaddend and $\bb{E}\in\R^{n\times n}$ is the matrix of $1$'s. 
%\DIFdelbegin \DIFdel{The random orthogonal matrix }\DIFdelend \DIFaddbegin \DIFadd{We constructed }\DIFaddend $\bb{Q'}$ \DIFdelbegin \DIFdel{is generated by taking a }\DIFdelend \DIFaddbegin \DIFadd{by computing the default }\DIFaddend QR factorization of \DIFdelbegin \DIFdel{an iid $4000$-by-$100$ matrix sampled from $Unif(0,1)$, and we used the built-in QR factorization function in Julia}\DIFdelend \DIFaddbegin \DIFadd{matrix $\bb{\Omega}\in\F_{fp64}^{4000\times100}$ in Julia, which performs BQR with $r=36$ entirely in fp64 arithmetic, and elements of the random matrix $\bb{\Omega}$ were sampled from $Unif(0,1)\cap \F_{fp64}$}\DIFaddend .
%By construction, $\bb{A}_{\alpha}$ has 2-norm condition number $n\alpha+1$. 
%By varying $\alpha$ from {\tt 1e-4} to {\tt 1}, we varied the condition number from $1.1$ to $101$, and we generated $10$ samples for each value of $\alpha$.
%\DIFdelbegin %DIFDELCMD < 
%
%%DIFDELCMD < %%%
%%DIF < \subsubsection{Results}
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{We generated random matrices of size $4000$-by-$100$ using Equation~\ref{eqn:genRM} and computed their HQR and TSQR for $L=1, \cdots, 6$ in a mixed-precision setting that simulates Assumption~\ref{assump:mp} with $z=1$.
%}\DIFdelend The relative backward error, $\|\hat{\bb{Q}}\hat{\bb{R}}-\bb{A}\|_F/\|\bb{A}\|_F$, was computed by casting up $\hat{\bb{Q}}$, $\hat{\bb{R}}$, and $\bb{A}$ to \DIFdelbegin \DIFdel{double precision }\DIFdelend \DIFaddbegin \DIFadd{fp64 }\DIFaddend to compute the Frobenius norms.
%\DIFdelbegin \DIFdel{Note that the mixed-precision HQR error bounds $n\tilde{\gamma}_{w}^{(6d+6z+13)}$ and $n^{3/2}\tilde{\gamma}_{w}^{(6d+6z+13)}$ for }\DIFdelend \DIFaddbegin \DIFadd{Plugging in }\DIFaddend $m=4000$\DIFdelbegin \DIFdel{and $n=100$ are }\DIFdelend \DIFaddbegin \DIFadd{, $n=100$, $u^{(l)}=u^{(fp16)}$, $u^{(h)}=u^{(fp32)}$, and $c=1$ (for $\tilde{\gamma}$) into the error bounds for }\DIFaddend {\tt \DIFdelbegin \DIFdel{0.936}\DIFdelend \DIFaddbegin \DIFadd{mpHQR2}\DIFaddend } \DIFaddbegin \DIFadd{combined with \cref{eqn:QRA,eqn:QQI} are approximately }{\tt \DIFadd{1.179}} \DIFaddend and {\tt \DIFdelbegin \DIFdel{9.364}\DIFdelend \DIFaddbegin \DIFadd{1.146}\DIFaddend }\DIFdelbegin \DIFdel{respectively, and the mixed-precision }\DIFdelend \DIFaddbegin \DIFadd{.
%These error bounds are \emph{relative} and these worst-case bounds do not guarantee errors below 100\%.
%The }\DIFaddend TSQR bounds for \DIFdelbegin \DIFdel{$L=1,\cdots, 5$  }\DIFdelend \DIFaddbegin \DIFadd{the same parameters for $L=1:5$ }\DIFaddend are even larger, which indicates that \DIFdelbegin \DIFdel{our error bounds do not guarantee stability .}%DIFDELCMD < \par
%%DIFDELCMD < 
%
%%DIFDELCMD < \begin{wrapfigure}{l}{.4\textwidth}
%%DIFDELCMD < 	\centering
%%DIFDELCMD < 	\vspace{-10pt}
%%DIFDELCMD < 	\includegraphics[width=.4\textwidth]{./figures/unblocked.pdf}
%%DIFDELCMD < 	%%%
%%DIFDELCMD < \caption{%
%{%DIFAUXCMD
%%DIFDELCMD < \label{fig:unblocked} %%%
%\DIFdel{HQR errors for matrices with varying condition numbers.}}
%	%DIFAUXCMD
%%DIF < \vspace{-10pt}	
%%DIFDELCMD < \end{wrapfigure}
%%DIFDELCMD < %%%
%\DIFdel{Figure~\ref{fig:unblocked} }\DIFdelend \DIFaddbegin \DIFadd{stability is not guaranteed. 
%The right plot of \cref{fig:sizemp2} }\DIFaddend shows the backward errors of \DIFdelbegin \DIFdel{mixed precision HQR }\DIFdelend \DIFaddbegin {\tt \DIFadd{mpHQR2}} \DIFaddend increasing as the theoretical condition numbers of the generated random matrices increase, and these errors correspond to the error data on the vertical axis, $L=0$, of Figure~\ref{fig:allTSQR}.
%In addition to the errors from \DIFdelbegin \DIFdel{HQR}\DIFdelend \DIFaddbegin {\tt \DIFadd{mpHQR2}}\DIFaddend , Figure~\ref{fig:allTSQR} shows the errors from \DIFdelbegin \DIFdel{mixed precision TSQR }\DIFdelend \DIFaddbegin {\tt \DIFadd{mpTSQR2s}} \DIFaddend of levels varying from $L=1$ to $L=5$, where each line represents the errors of HQR and variants of TSQR calculated from the same random test matrix.
%Figure~\ref{fig:allTSQR} reveals two different trends for the errors as we deepen the complexity of the QR algorithm from \DIFdelbegin \DIFdel{HQR to TSQR with 5 levels}\DIFdelend \DIFaddbegin {\tt \DIFadd{mpHQR2}} \DIFadd{to }{\tt \DIFadd{mpTSQR2}} \DIFadd{with $L=5$}\DIFaddend . 
%One trend occurs for matrices with smaller condition numbers, where \DIFdelbegin \DIFdel{HQR and all levels of TSQR are }\DIFdelend \DIFaddbegin {\tt \DIFadd{mpHQR2}} \DIFadd{is }\DIFaddend stable, but \DIFdelbegin \DIFdel{deepening the levels of TSQR worsens the errors. 
%The other }\DIFdelend \DIFaddbegin {\tt \DIFadd{mpTSQR2}} \DIFadd{with higher levels yield larger errors. 
%Another }\DIFaddend trend occurs for matrices with higher condition numbers, where single-level and 2-level \DIFdelbegin \DIFdel{TSQR }\DIFdelend \DIFaddbegin {\tt \DIFadd{mpTSQR2}} \DIFaddend yield smaller errors than \DIFdelbegin \DIFdel{HQR}\DIFdelend \DIFaddbegin {\tt \DIFadd{mpHQR2}}\DIFaddend . 
%In these cases, \DIFdelbegin \DIFdel{TSQR }\DIFdelend \DIFaddbegin \DIFadd{errors from }{\tt \DIFadd{mpTSQR2}} \DIFaddend with 3 or more levels \DIFdelbegin \DIFdel{have errors }\DIFdelend \DIFaddbegin \DIFadd{are }\DIFaddend similar to or worse than \DIFaddbegin \DIFadd{their }\DIFaddend 2-level \DIFdelbegin \DIFdel{TSQR, but those errors tend to not rise above the HQR errors}\DIFdelend \DIFaddbegin \DIFadd{variants, but generally do not exceed those of }{\tt \DIFadd{mpHQR2}} \DIFadd{most of the times}\DIFaddend .
%These results suggests that TSQR can \DIFdelbegin \DIFdel{significantly }\DIFdelend outperform HQR even in \DIFdelbegin \DIFdel{mixed-precision }\DIFdelend \DIFaddbegin \DIFadd{mixed precision }\DIFaddend settings, and particularly when HQR is unstable due to larger condition numbers.
%Although this experiment focused on condition numbers, identifying other properties that point to better performance of TSQR than HQR can further broaden the potential use of \DIFdelbegin \DIFdel{mixed-precision }\DIFdelend \DIFaddbegin \DIFadd{mixed precision }\DIFaddend TSQR in applications.\DIFaddbegin \par
%\DIFaddend %The first trend When the error is low enough for the unblocked QR factorization, TSQR performs worse for these matrices.
%%Recall that machine precision for half-precision is about $10^{-3}$. 
%%This shows that the traditional QR factorization had been very good to begin with.
%%Finally, even when TSQR is \textit{successful} initially, we can see that too many initial blocks can become a problem as well. 
%%
%%Overall, this figure shows a variety of results that encourage further exploration. 
%%We have shown that TSQR can improve on certain matrices where the unblocked HQR algorithm was highly unstable in half-precision.
%%Identifying which matrix properties correlate to TSQR and why can help broaden the possibility of using lower precision arithmetic for QR factorizations.
%\DIFdelbegin %DIFDELCMD < 
%
%%DIFDELCMD < \begin{figure}[h!]%%%
%%DIF < {r}{.53\textwidth}
%	%DIFDELCMD < \centering
%%DIFDELCMD < 	%%%
%%DIF < \vspace{-10pt}
%	%DIFDELCMD < \includegraphics[width=0.8\textwidth]{./figures/allTSQR2.pdf}
%%DIFDELCMD < 	%%%
%%DIFDELCMD < \caption{%
%{%DIFAUXCMD
%%DIFDELCMD < \label{fig:allTSQR} %%%
%\DIFdelFL{Left plot shows the relative error of QR factorization for matrices with condition numbers ranging from 5.3 to 101, and the right plot shows the errors for matrices with condition numbers ranging from 1.1 to 5.3. }}
%	%DIFAUXCMD
%%DIFDELCMD < \vspace{-10pt}
%%DIFDELCMD < \end{figure} 
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\section{\DIFdel{Applications}}
%%DIFAUXCMD
%\addtocounter{section}{-1}%DIFAUXCMD
%%DIFDELCMD < \label{sec:Apps}
%%DIFDELCMD < %%%
%\DIFdel{Many applications in scientific computing typically employ double precision when lower precision may actually be sufficient. 
%Due to the advances in processors, FLOPs are now considered free, causing bandwidth and storage to be the computational bottleneck. 
%With the emergence of new technology that supports low precision and the need to reduce bandwidth and storage concerns, interest in mixed-precision algorithms has reemerged. 
%%DIF < However, identifying which applications can tolerate lower precision and still achieve sufficient results is a challenging task that wasn't particularly relevant prior to the emergence of new technology that supports low precision.
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{Since low and mixed precision settings benefit from speed-up and reduced storage, applications that process large amounts of data are potential candidates for this research.
%Here, we discuss our results from applying our mixed-precision HQR as a subroutine of an iterative eigensolver in the context of spectral clustering.}%DIFDELCMD < \par
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\paragraph{\DIFdel{Graph partitioning}} %DIFAUXCMD
%\addtocounter{paragraph}{-1}%DIFAUXCMD
%\DIFdel{A graph is defined by a set of nodes and a set of edges between the nodes.
%Partitioning, or clustering, is a task that seeks communities within a graph such that nodes within a community are \emph{more similar} to each other than to nodes outside of that community. 
%In datasets where the true communities are known, we can use pairwise-precision and pairwise-recall (see \cite{GraphChallenge})  which are defined in Definition~\ref{def:P&R}) to evaluate the accuracy of a clustering task.
%}%DIFDELCMD < \begin{definition}
%%DIFDELCMD < 	%DIFDELCMD < \label{def:P&R}%%%
%%DIFDELCMD < 	%%%
%%DIF < Some relevant evaluation metrics of classification of clustering tasks are precision and recall. 
%	%DIF < Precision is the fraction of relevant instances among the retrieved instances.
%    \DIFdel{Pairwise-precision and pairwise-recall are measured by checking for every pair of nodes if the pair is classified into the same cluster (positive), or else (negative).
%}\begin{displaymath}
%	\DIFdel{\text{Precision} = \frac{\#\text{True Positive}}{\#\text{True Positive}+\#\text{False Positive}}, 
%	\qquad
%	%DIF < %% \end{equation} 
%	%DIF < Recall is the fraction of relevant instances that have been retrieved over the total amount of relevant instances.
%	%DIF < %% \begin{equation}
%	\text{Recall} = \frac{\#\text{True Positive}}{\#\text{True Positive}+\#\text{False Negative}}.
%	}\end{displaymath}
%%DIFAUXCMD
%%DIFDELCMD < \end{definition}
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\subsection{\DIFdel{Spectral Graph Clustering}}
%%DIFAUXCMD
%\addtocounter{subsection}{-1}%DIFAUXCMD
%%DIFDELCMD < \label{sec:cluster}
%%DIFDELCMD < %%%
%\DIFdel{Some spectral clustering methods utilize identifying $k$ dominant eigenvectors of a similarity matrix of a graph, which then can be used to identify $k$ clusters. 
%Another potential use of iterative eigensolvers for spectral clustering is in identifying the second smallest eigenvalue and its eigenvector pair, called the Fiedler value and vector.
%%DIF < Whether convergence to the Fiedler vector is quick or not is more uncertain than in identifying dominant eigenvectors.
%In addition, many eigenproblems outside of spectral clustering only require finding a few eigen pairs.
%This family of problems tends to admit tall-and-skinny matrix structures and could utilize TSQR as well. 
%We will use subspace iteration, a variant of the power method defined in Algorithm~\ref{algo:subIter} that uses a QR factorization of a tall-and-skinny matrix at each iteration and that quickly converges to the dominant eigenvectors.
%Although we only experimented with comparing mixed-precision HQR to uniform precision HQR, TSQR could also be used in this application. 
%}%DIFDELCMD < \par
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\subsubsection{\DIFdel{Subspace Iteration}}
%%DIFAUXCMD
%\addtocounter{subsubsection}{-1}%DIFAUXCMD
%\DIFdel{Subspace iteration is a modification of the power method, which computes an invariant subspace with dimension $p > 1$ (see \cite{Bai2000}).
%A variant of this algorithm is shown below in Algorithm~\ref{algo:subIter}.
%}%DIFDELCMD < 
%
%%DIFDELCMD < \begin{algorithm2e}
%%DIFDELCMD < 	\DontPrintSemicolon %%%
%%DIF <  Some LaTeX compilers require you to use \dontprintsemicolon instead
%	%DIFDELCMD < \KwIn{Adjacency matrix $\bb{A}\in\{0, 1\}^{m \times m}$ where $m \geq n$, {\tt max\_iter}, the maximum number of iterations, $\tau$ the threshold for the eigenspace error, and $k$, the suspected number of clusters.}
%%DIFDELCMD < 	\KwOut{$\bb{Q}$}
%%DIFDELCMD < 	%%%
%\DIFdel{Initialize $\bb{Y}\in \mathbb{R}^{m\times k}$, a random matrix.
%	}%DIFDELCMD < \tcp{$Y$ would likely be full-rank.} 
%%DIFDELCMD < 	%%%
%\DIFdel{$\bb{Q}, \bb{R}\gets \tt{qr}(\bb{Y})$ 
%	}%DIFDELCMD < \For{$i=1, \cdots,$ {\tt max\_iter}}{
%%DIFDELCMD < 		$\bb{Y} \gets \bb{AQ}$\;
%%DIFDELCMD < 		\If{$\frac{\|\bb{Y}-\bb{QQ}^{\top}\bb{Y}\|_2}{\|\bb{Y}\|_2} < \tau$}{exit loop. \tcp{$\|\bb{Y}-\bb{QQ}^{\top}\bb{Y}\|_2$ is the eigenspace error.}}
%%DIFDELCMD < 		$\bb{Q, R} \gets {\tt qr}(\bb{Y})$
%%DIFDELCMD < 	}
%%DIFDELCMD < 	\Return %%%
%\DIFdel{$\bb{Q}$
%	}%DIFDELCMD < \caption{%
%{%DIFAUXCMD
%\DIFdel{$\bb{Q}=$ }%DIFDELCMD < {\tt %%%
%\DIFdel{subIter}%DIFDELCMD < }%%%
%\DIFdel{$(\bb{A}, \text{\tt max\_iter}, \tau, k)$. Find orthogonal basis (given by columns of output matrix $Q$) of an invariant subspace of the input adjacency matrix, $A$.}}
%	%DIFAUXCMD
%%DIFDELCMD < \label{algo:subIter}
%%DIFDELCMD < \end{algorithm2e}
%%DIFDELCMD < %%%
%\DIFdel{This algorithm is an iterative method with two possible stopping criteria: 1) the maximum number of iterations to complete before exiting the loop is declared as max\_iter, or 2) if the eigenspace error is smaller than $\tau$, then exit the loop.
%In practice, we added a third stopping criterion in the case that the declared $\tau$ value was too small, which would force an exit from the loop when the eigenspace error began to increase.
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\subsubsection{\DIFdel{Density-based Spatial clustering of Applications with Noise (DBSCAN)}}
%%DIFAUXCMD
%\addtocounter{subsubsection}{-1}%DIFAUXCMD
%\DIFdel{DBSCAN is a density-based spatial clustering algorithm introduced in \cite{EKSX1996} and is widely used in practice.
%This algorithm only requires input data, location of nodes, and two parameters, radius of neighborhoods and minimum number of points required to form a dense region. 
%The two parameters for the DBSCAN algorithm were tuned to provide the best result, given that we used the same set of parameters for the entire experiment.
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\subsection{\DIFdel{Experiment Details and Results}}
%%DIFAUXCMD
%\addtocounter{subsection}{-1}%DIFAUXCMD
%\DIFdel{Our main goal in this experiment was to test if the eigenspaces identified by lower precision HQR could produce sufficient graph partitioning. 
%We used subspace iteration (Algorithm~\ref{algo:subIter}) to identify eigenspaces, DBSCAN to partition the embedding of the nodes onto these eigenspaces, and precision and recall to evaluate clustering performances. 
%We used a static graph of $5000$ nodes  with $19$ known true partitions for the Graph Challenge \cite{GraphChallenge}, which are derived from block stochastic matrices . 
%The graphs we used were undirected and unweighted; the only elements in the adjacency matrices were $0$'s and $1$'s, which can easily be represented in half, single, and double precision floats.
%For $i=1, \cdots, 10$, let $\bb{Y}_{\text{half},i}\in\F_{\text{half}}^{5000\times 19}$ be the half precision storage of the $i^{th}$ random matrix.
%Since any half precision float can be exactly represented in single and double precisions, $\bb{Y}_{\text{half},i}$'s can be easily cast up to single and double precisions, $\bb{Y}_{\text{single},i}$ and $\bb{Y}_{\text{double},i}$.
%We performed mixed-precision HQR within subspace iteration initialized by $\bb{Y}_{\text{half},i}$'s, and uniform-precision HQR for subspace iteration initialized by $\bb{Y}_{\text{single},i}$'s  and $\bb{Y}_{\text{double},i}$'s.
%For trial $i = 1, \cdots, 10, $ we repeated the following steps.
%}%DIFDELCMD < \begin{enumerate}[Step 1.]
%%DIFDELCMD < 	\item %%%
%\DIFdel{Identify an orthogonal basis of dimension 19 (number of known true partitions)with subspace iteration using the appropriate HQR routine for $\bb{Y}_{\text{half},i}$, $\bb{Y}_{\text{single},i}$ and $\bb{Y}_{\text{double},i}$. 
%}%DIFDELCMD < \item %%%
%\DIFdel{Apply DBSCAN to the output matrices of previous step to cluster most nodes into communities and the remaining nodes as outliers. 
%	}%DIFDELCMD < \item %%%
%\DIFdel{Measure clustering performances of DBSCAN on the three different precision subspace iteration embeddings using precision and recall.
%}%DIFDELCMD < \end{enumerate}
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\paragraph{\DIFdel{Subspace Iteration Results}} 
%%DIFAUXCMD
%\addtocounter{paragraph}{-1}%DIFAUXCMD
%%DIFDELCMD < \begin{wrapfigure}{r}{.5\textwidth}
%%DIFDELCMD < 	\centering
%%DIFDELCMD < 	%%%
%%DIF < \vspace{-10pt}
%	%DIFDELCMD < \includegraphics[width=.5\textwidth]{./figures/5000-19subIter.pdf}
%%DIFDELCMD < 	%%%
%%DIFDELCMD < \caption{%
%{%DIFAUXCMD
%%DIFDELCMD < \label{fig:subIter} %%%
%\DIFdel{Eigenspace Error for subspace iteration with using double-, single-, and half- precision traditional Householder QR factorizations.}}
%	%DIFAUXCMD
%%DIFDELCMD < \vspace{-20pt}	
%%DIFDELCMD < \end{wrapfigure}
%%DIFDELCMD < %%%
%\DIFdel{Figure~\ref{fig:subIter} shows the eigenspace error, $\|\bb{Y}-\bb{QQ}^{\top}\bb{Y}\|_2/\|\bb{Y}\|_2$, from the subspace iteration step of one trial.
%The stopping criteria, $\tau$, were set to $5u_{\text{single}}$ and $5u_{\text{double}}$ for the uniform precision HQRs and $5u_{\text{half}}$ for the mixed-precision HQR.
%The solid lines are plotted to show the unit round-off values. 
%The uniform precision implementations of subspace iterations reached their stopping criterion set by $\tau$, and the mixed-precision implementation fluctuated close to but never dipped below $\tau$. 
%The convergence rate was approximately the same across the three different implementations, which suggests that the lower precision routines (mixed-precision HQR or uniform single precision HQR) can be used as a preconditioner for the double precision solution, and if paired with appropriate hardware could lead to increased computational efficiency.
%In addition, if double precision eigenspace error is not necessary to achieve sufficient clustering results, we can simply use the lower precision HQR subspace iterations as full eigensolvers. 
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\paragraph{\DIFdel{Clustering Results}} 
%%DIFAUXCMD
%\addtocounter{paragraph}{-1}%DIFAUXCMD
%\DIFdel{Table~\ref{table:PR} shows the worst-case precision and recall results from the $10$ trials for each subspace iteration implementation. 
%The DBSCAN algorithm and the calculation of precision and recall were computed in double precision, and the variance in precision and recall values for these $10$ trials were in the range of }\DIFdelend \DIFaddbegin \DIFadd{Finally, we discuss the last experiment which varied the block sizes for }\DIFaddend {\tt \DIFdelbegin \DIFdel{1e-6}\DIFdelend \DIFaddbegin \DIFadd{mpBQR3}\DIFaddend }.
%\DIFdelbegin \DIFdel{Subspace iteration that employs lower precision HQR results in a suboptimal solution to the basis, which has a larger loss in orthogonality when compared to the solution from subspace iteration that uses higher precision HQR. 
%However, clustering results show minimal difference in the precision and recall and suggests that a lower precision HQR within subspace iteration can still lead to a sufficiently accurate clustering. 
%%DIF < Possible future works include trying different clustering methods for Step 2. 
%}%DIFDELCMD < \begin{table}[h!]
%%DIFDELCMD < 	\centering
%%DIFDELCMD < %%%
%%DIF < %% 	\begin{tabular}{ |c|c|c| } 
%%DIF < %% 		\hline
%%DIF < %% 		HQR scheme & Precision & Recall \\ \hline 
%%DIF < %% 		mixed-precision & {\tt 0.9822} & {\tt 0.9393} \\ 
%%DIF < %% 		single-precision & {\tt 0.9817} & {\tt 0.9407} \\ 
%%DIF < %% 		double-precision &{\tt 0.9822} & {\tt 0.9405}\\
%%DIF < %% 		\hline
%%DIF < %% 	\end{tabular}
%%DIFDELCMD < \begin{tabular}{ |c|c|c|c| }
%%DIFDELCMD < \hline
%%DIFDELCMD < %%%
%\DIFdelFL{HQR Scheme }%DIFDELCMD < & %%%
%\DIFdelFL{Mixed Precision }%DIFDELCMD < & %%%
%\DIFdelFL{Single Precision }%DIFDELCMD < & %%%
%\DIFdelFL{Double Precision }%DIFDELCMD < \\ \hline
%%DIFDELCMD < %%%
%\DIFdelFL{Prec / Recall }%DIFDELCMD < & 
%%DIFDELCMD < %%%
%\DIFdelendFL \DIFaddbeginFL \DIFaddFL{\Cref{fig:mpBQR3} shows the error bounds and the computed value for the backward error for fp32 BQR and }\DIFaddendFL {\tt \DIFdelbeginFL \DIFdelFL{0.9822}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{mpBQR3}\DIFaddendFL } \DIFdelbeginFL \DIFdelFL{/ }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{for $m=n=8193$ and varying $r$. 
%The test matrices were sampled from $N(0,1)$, and their large size immediately pushes both error bounds above $1$ and numerical stability is not guaranteed.
%However, the high precision implementation yields backward error close to $u^{(fp32)}$ and }\DIFaddendFL {\tt \DIFdelbeginFL \DIFdelFL{0.9393}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{mpBQR3}\DIFaddendFL } \DIFdelbeginFL %DIFDELCMD < &
%%DIFDELCMD < %%%
%\DIFdelendFL \DIFaddbeginFL \DIFaddFL{yields errors near $u^{(fp16)}$ that follows the downward trend suggested by \cref{eqn:mpBQR3}.
%The rightmost data for }\DIFaddendFL {\tt \DIFdelbeginFL \DIFdelFL{0.9817}%DIFDELCMD < } %%%
%\DIFdelFL{/ }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{mpBQR3}} \DIFaddFL{(corresponds to $r=n/2$), is still between 2 and 3 orders of magnitude higher than its high precision variant. 
%There exist applications that are robust to QR factorization errors and could benefit from speed-ups of using bFMAs.
%%DIF > TODO: reference graph paper on arXiv (put on arXiv first).
%}
%
%\DIFaddFL{In conclusion, most of the experiments display the trends that error bounds in \cref{sec:algo,sec:mpanalysis} suggest, and bFMA variants perform in between the high precision and \cref{assump:mp} variants as expected.
%Also, a special case is shown that demonstrate }\DIFaddendFL {\tt \DIFdelbeginFL \DIFdelFL{0.9407}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{mpTSQR2}\DIFaddendFL } \DIFdelbeginFL %DIFDELCMD < &
%%DIFDELCMD < %%%
%\DIFdelendFL \DIFaddbeginFL \DIFaddFL{can outperform }\DIFaddendFL {\tt \DIFdelbeginFL \DIFdelFL{0.9822}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{mpHQR2}\DIFaddendFL } \DIFdelbeginFL \DIFdelFL{/ }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{despite having higher error bounds.
%Although further studies with larger problem sizes and timings would be beneficial in developing an }\DIFaddendFL {\tt \DIFdelbeginFL \DIFdelFL{0.9405}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{mpBQR3}\DIFaddendFL } \DIFdelbeginFL %DIFDELCMD < \\
%%DIFDELCMD < \hline
%%DIFDELCMD < \end{tabular}
%%DIFDELCMD < 	%%%
%%DIF < %% \caption{\label{table:PR} Minimum (worst-case) precision and recall values for $10$ trials of DBSCAN on graph with $5000$ nodes and $19$ true clusters.}
%	%DIFDELCMD < \caption{%
%{%DIFAUXCMD
%%DIFDELCMD < \label{table:PR} %%%
%\DIFdelFL{Minimum (worst-case) precision and recall for $10$ trials on graph with $5000$ nodes and $19$ true clusters.}}
%%DIFAUXCMD
%%DIFDELCMD < \end{table}
%%DIFDELCMD < %%%
%%DIF <  and calls for further investigations into a variable precision approach to other spectral clustering methods as well.
%%DIF < The mixed-precision implementation approached its best performance close to $10$ iterations, and continued to fluctuate near there without hitting the $\tau$ value. 
%%DIF < Nonetheless, we can see that the first $9$ iterations of the subspace iteration technique yielded the same eigenspace errors for all three precisions. 
%%DIF < The same pattern continued for single- and double- precision implementations until they reached single-precision unit round-off near $10^{-7}$, and the double-precision unit round-off near $10^{-15}$. 
%%DIF < Therefore, we can do with lower-precision QR factorizations that require less storage and faster computation time if low-precision eigenspace error is sufficient for spectral clustering.
%%DIF < Due to the random element of the initial matrix $\bb{Y}$ at the beginning of subspace iteration, there is some variability to its performance in identifying an invariant subspace. 
%%DIF < In addition, we chose the number of columns of $\bb{Y}$ to be the number of true clusters, which is usually unknown in practice.
%%DIF < \subsubsection{Results}
%%DIF < We used the $5000$ node static graph from \cite{GraphChallenge}, but varied the clustering results by using $10$ different random matrices as the initial $\bb{Y}$.
%%DIF < Therefore, for each HQR scheme (uniform double, uniform single, and mixed-precision) $10$ different initializations
%\DIFdelend \DIFaddbegin \DIFadd{with the optimal block size, $r$, our experiments confirm the intuition built from the error analysis in \cref{sec:mpanalysis}.
%}\DIFaddend 
%
%%DIF > Can see first submitted draft by uncommenting below. 
%%DIF > \input{intro}
%%DIF > \input{roundingerr}
%%DIF > \input{HQR}
%%DIF > \input{TSQR}
%%DIF > \input{applications}
%\section{Conclusion}
%\DIFdelbegin \DIFdel{Though the use of lower precision naturally reduces the bandwidth and storage needs, the }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend development of GPUs \DIFdelbegin \DIFdel{to }\DIFdelend \DIFaddbegin \DIFadd{that }\DIFaddend optimize low precision floating point arithmetic have accelerated the interest in half \DIFdelbegin \DIFdel{precision and mixed-precision algorithms . %DIF < the interest in half precision and mixed-precision algorithms that demonstrate speedier times, lower energy consumption, and lower memory usage. 
%}\DIFdelend \DIFaddbegin \DIFadd{and mixed precision algorithms that naturally reduces the bandwidth and storage needs. 
%%DIF > the interest in half precision and mixed precision algorithms that demonstrate speedier times, lower energy consumption, and lower memory usage. 
%}\DIFaddend Loss in precision, stability, and representable range offset for those advantages, but these shortcomings may have little to no impact in some applications.
%It may even be possible to navigate around those drawbacks with algorithmic design. \par 
%\DIFdelbegin %DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{The existing rounding error analysis cannot }\DIFdelend %DIF > Since existing rounding error analysis cannot accurately bound the behavior of mixed precision arithmetic.
%%DIF > We have developed a new framework for mixed precision rounding error analysis and applied it to HQR, a widely used linear algebra routine, and implemented it in an iterative eigensolver in the context of spectral clustering. 
%\DIFaddbegin \DIFadd{We present the algorithm and standard error analysis of HQR and its blocked variants (BQR and TSQR), modify the algorithms to support two mixed precision settings, and performed error analysis that }\DIFaddend accurately bound the \DIFdelbegin \DIFdel{behavior of mixed-precision arithmetic.
%We have developed a new framework for mixed-precision rounding error analysis and applied it to HQR, a widely used linear algebra routine, and implemented it in an iterative eigensolver in the context of spectral clustering. 
%The mixed-precision error analysis builds from the inner product routine, which }\DIFdelend \DIFaddbegin \DIFadd{mixed precision versions.
%One mixed precision setting is that of NVIDIA's TensorCore bFMAs, and the other is an ad hoc setting that mimics the bFMAs at the level of inner products.
%These two are presented to offer mixed precision arithmetic at both level-2 and 3 BLAS operations and 
%%DIF > The mixed precision error analysis builds from the inner product routine, which 
%}\DIFaddend can be applied to \DIFdelbegin \DIFdel{many }\DIFdelend other linear algebra tools as well.
%The new error bounds more accurately describe how rounding errors are accumulated in \DIFdelbegin \DIFdel{mixed-precision }\DIFdelend \DIFaddbegin \DIFadd{mixed precision }\DIFaddend settings.
%We also found that TSQR \DIFdelbegin \DIFdel{, a communication-avoiding, easily parallelizable QR factorization algorithm for tall-and-skinny matrices, }\DIFdelend can outperform HQR \DIFdelbegin \DIFdel{in mixed-precision settings }\DIFdelend \DIFaddbegin \DIFadd{under \cref{assump:mp} }\DIFaddend for ill-conditioned, extremely overdetermined cases, which suggests that some algorithms are more robust against lower precision arithmetic.
%\DIFdelbegin \DIFdel{As QR factorizations of tall-and-skinny matrices are common in spectral clustering, we experimented with introducing mixed-precision settings into graph partitioning problems.
%In particular, we applied DBSCAN to the spectral basis of a graph identified via subspace iteration that used our simulated mixed-precision HQR, which yielded clustering results tantamount to results from employing double-precision entirely.}%DIFDELCMD < \par
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{Although this work is focused on QR factorizations and applications in spectral clustering, the mixed precision round-off error analysis can be applied to other tasks and applications that can benefit from employing low precision computations. 
%While the emergence of technology that support low precision floats combats issues dealing with storage, now we need to consider how low precision affects stability of numerical algorithms. 
%}\DIFdelend %DIF > As QR factorizations of tall-and-skinny matrices are common in spectral clustering, we experimented with introducing mixed precision settings into graph partitioning problems.
%%DIF > In particular, we applied DBSCAN to the spectral basis of a graph identified via subspace iteration that used our simulated mixed precision HQR, which yielded clustering results tantamount to results from employing double-precision entirely.\par
%%DIF > Although this work is focused on QR factorizations and applications in spectral clustering, the mixed precision round-off error analysis can be applied to other tasks and applications that can benefit from employing low precision computations. 
%%DIF > While the emergence of technology that support low precision floats combats issues dealing with storage, now we need to consider how low precision affects stability of numerical algorithms. 
%%TODO: Talk about stability?
%\DIFdelbegin %DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdelend Future work is needed to test larger \DIFdelbegin \DIFdel{, more ill-conditioned problems with different mixed-precision settings, and to }\DIFdelend \DIFaddbegin \DIFadd{problem sizes with timed experiments to compare the loss in accuracy with the benefits of speed ups and 
%%DIF > , more ill-conditioned problems with different mixed precision settings, and to 
%}\DIFaddend explore other divide-and-conquer methods like TSQR that can harness parallel capabilities of GPUs while withstanding lower precisions. 
%\DIFdelbegin %DIFDELCMD < 
%
%%DIFDELCMD < \appendix
%%DIFDELCMD < %%%
%\section{\DIFdel{Proofs}}
%%DIFAUXCMD
%\addtocounter{section}{-1}%DIFAUXCMD
%\section{\DIFdel{Proofs}}%DIFAUXCMD
%\addtocounter{section}{-1}%DIFAUXCMD
%%DIFDELCMD < \label{appendix:A}
%%DIFDELCMD < %%%
%\subsection{\DIFdel{Lemma \ref{lem:mp} (Equation \ref{lem:mp1})}}
%%DIFAUXCMD
%\addtocounter{subsection}{-1}%DIFAUXCMD
%%DIFDELCMD < \begin{proof}
%%DIFDELCMD < 	%%%
%\DIFdel{We wish to round up to the lower precision, $p$, since $1\gg u_p \gg u_s$.  
%	Recall that $d := \left\lfloor k_s u_s  / u_p \right\rfloor$ and $r \leq \left\lfloor u_p  / u_s \right\rfloor$,
%	and note
%	%DIF < %% \begin{equation*}
%	$ k_pu_p+k_su_s = (k_p+d)u_p + r u_s \leq (k_p+d+1)u_p$. Then,
%	%DIF < %% \end{equation*}
%	}\begin{align*}
%	\DIFdel{\gamma_{p}^{(k_p)}+\gamma_{s}^{(k_s)}+\gamma_{p}^{(k_p)}\gamma_{s}^{(k_s)} 
%	}&\DIFdel{= \frac{k_pu_p}{1-k_pu_p} + \frac{k_su_s}{1-k_su_s} + \frac{k_pu_p}{1-k_pu_p}\frac{k_su_s}{1-k_su_s} }\\
%	\DIFdel{= \frac{k_pu_p+k_su_s-k_pk_su_pu_s}{1-(k_pu_p+k_su_s)+k_pk_su_pu_s} %DIF < %% \\
%	}&\DIFdel{\leq \frac{(k_p+d+1)u_p-k_pk_su_pu_s}{1-(k_p+d+1)u_p+k_pk_su_pu_s} }\\
%	&\DIFdel{< \frac{(k_p+d+1)u_p}{1-(k_p+d+1)u_p} = \gamma_{p}^{(k_p+d+1)}
%	}\end{align*}
%%DIFAUXCMD
%%DIFDELCMD < \end{proof}
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\subsection{\DIFdel{Inner Products}}
%%DIFAUXCMD
%\addtocounter{subsection}{-1}%DIFAUXCMD
%%DIFDELCMD < \label{appendix:IP}
%%DIFDELCMD < %%%
%\subsubsection{\DIFdel{Lemma \ref{lem:ip_a}}}
%%DIFAUXCMD
%\addtocounter{subsubsection}{-1}%DIFAUXCMD
%\DIFdel{Let $\dd_p$ and $\dd_s$ be rounding error incurred from products and summations.
%They are bounded by $|\dd_p| < u_p$ and $|\dd_s| < u_s$, following the notation in \cite{Higham2002}. Let $s_k$ denote the $k^{th}$ partial sum, and let $\hat{s_k}$ denote the floating point representation of the calculated $s_k$.
%Then,
%}\begin{align*}
%\DIFdel{\hat{s_1} }&\DIFdel{= \fl (\bb{x}_1\bb{y}_1) = \bb{x}_1\bb{y}_1(1 + \dd_{p,1}),}\\
%\DIFdel{\hat{s_2} }&\DIFdel{= \fl(\hat{s_1} + \bb{x}_2\bb{y}_2), }\\
%&\DIFdel{= \left[\bb{x}_1\bb{y}_1(1 + \dd_{p,1}) + \bb{x}_2\bb{y}_2(1 + \dd_{p,2})\right](1+\dd_{s,1}),}\\
%\DIFdel{\hat{s_3} }&\DIFdel{= \fl(\hat{s_2}+\bb{x}_3\bb{y}_3), }\\
%&\DIFdel{= \left(\left[\bb{x}_1\bb{y}_1(1 + \dd_{p,1}) + \bb{x}_2\bb{y}_2(1 + \dd_{p,2})\right](1+\dd_{s,1})  + \bb{x}_3\bb{y}_3(1+\dd_{p,3})\right)(1+\dd_{s,2}).
%}\end{align*}
%%DIFAUXCMD
%\DIFdel{We can see a pattern emerging. 
%The error for a general length $m$ vector dot product is then:
%}\begin{displaymath}
%\DIFdel{\label{eqn:dperr_1}
%\hat{s_m} = (\bb{x}_1\bb{y}_1+\bb{x}_2\bb{y}_2)(1+ \dd_{p,1})\prod_{k=1}^{m-1}(1+\dd_{s,k}) + \sum_{i=3}^n \bb{x}_i\bb{y}_i(1+\dd_{p,i})\left(\prod_{k=i-1}^{m-1}(1+\dd_{s,k})\right),
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdel{where each occurrence of $\dd_p$ and $\dd_s$ are distinct, but are still bound by $u_p$ and $u_s$.
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{Using Lemma \ref{lem:gamma}, we further simplify:
%}\begin{align*}
%\DIFdel{\fl(\bb{x}^{\top}\bb{y}) }&\DIFdel{= \hat{s_m} = (1+\tth_p^{(1)})(1+\tth_s^{(m-1)})\bb{x}^{\top}\bb{y}%DIF < %%\\
%%DIF < %%&
%= (\bb{x}+\Delta\bb{x})^{\top}\bb{y} = \bb{x}^{\top}(\bb{y}+\Delta\bb{y})
%}\end{align*}
%%DIFAUXCMD
%\DIFdel{Here $\Delta\bb{x}$ and $\Delta\bb{y}$ are vector perturbations.
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{By using Lemma \ref{lem:mp}, Equation \ref{lem:mp1}, we can bound the perturbations componentwise.
%Let $d:=\lfloor\frac{(m-1)u_s}{u_p}\rfloor$ such that $(m-1)u_s = d u_p + r u_s$. Then,
%}\begin{align*}
%\DIFdel{|\Delta \bb{x}| }&\DIFdel{\leq \gamma_p^{(d+2)}|\bb{x}| %DIF < %% \\
%\qquad \mbox{and} \qquad
%|\Delta \bb{y}| %DIF < %%&
%\leq \gamma_p^{(d+2)}|\bb{y}|.
%}\end{align*}
%%DIFAUXCMD
%\DIFdel{Furthermore, these bounds lead to a forward error result as shown in Equation \ref{eqn:ipforward},
%}\begin{displaymath}
%\DIFdel{%DIFDELCMD < \label{eqn:ipforward}%%%
%|\bb{x}^{\top}\bb{y}-\fl(\bb{x}^{\top}\bb{y})| \leq \gamma_p^{(d+2)}|\bb{x}|^{\top}|\bb{y}|.
%}\end{displaymath}
%%DIFAUXCMD
%%DIF < 
%%DIF < While this result does not guarantee a high relative accuracy when $|\bb{x}^{\top}\bb{y}| \ll |\bb{x}|^{\top}|\bb{y}|$, high relative accuracy is expected in some special cases.
%%DIF < For example, let $\bb{x}=\bb{y}$.
%%DIF < Then we have exactly $|\bb{x}^{\top}\bb{x}| = |\bb{x}|^{\top}|\bb{x}|=\|\bb{x}\|_2^2$.
%%DIF < This leads to
%%DIF < \begin{equation}
%%DIF < \left|\frac{\|\bb{x}\|_2^2 - \fl(\|\bb{x}\|_2^2)}{\|\bb{x}\|_2^2}\right| \leq \gamma_p^{(d+2)}
%%DIF < \end{equation}
%%DIF < 
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\subsubsection{\DIFdel{Corollary \ref{lem:ip_b}}}
%%DIFAUXCMD
%\addtocounter{subsubsection}{-1}%DIFAUXCMD
%\DIFdel{This proof follows similarly to the proof for Lemma \ref{lem:ip_a}.
%Since no error is incurred in the multiplication portion of the inner products, $\dd_s$ and $\dd_{st}$ are rounding error incurred from summations and storage.
%The error for a general $m$-length vector dot product is then
%}\begin{displaymath}
%\DIFdel{\label{eqn:dperr_2}
%\hat{s_m} = (\bb{x}_1\bb{y}_1+\bb{x}_2\bb{y}_2)\prod_{k=1}^{m-1}(1+\dd_{s,k}) + \sum_{i=3}^n \bb{x}_i\bb{y}_i\left(\prod_{k=i-1}^{m-1}(1+\dd_{s,k})\right).
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdel{Using Lemma \ref{lem:gamma}, we further simplify, 
%}\begin{displaymath}
%\DIFdel{\fl(\bb{x}^{\top}\bb{y}) = \hat{s_m} = (1+\tth_s^{(m-1)})\bb{x}^{\top}\bb{y}= (\bb{x}+\Delta\bb{x})^{\top}\bb{y} = \bb{x}^{\top}(\bb{y}+\Delta\bb{y}).
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdel{Here $\Delta\bb{x}$ and $\Delta\bb{y}$ are vector perturbations.
%}%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{By using Lemma \ref{lem:mp} equation \ref{lem:mp1}, we can bound the perturbations componentwise.
%Let $d:=\lfloor\frac{(m-1)u_s}{u_p}\rfloor$ such that $(m-1)u_s = d u_p + r u_s$. 
%}\begin{align*}
%\DIFdel{|\Delta \bb{x}| }&\DIFdel{\leq \gamma_p^{(d+1)}|\bb{x}| %DIF < %%\\
%\qquad \mbox{and} \qquad
%|\Delta \bb{y}| %DIF < %%&
%\leq \gamma_p^{(d+1)}|\bb{y}| 
%}\end{align*}
%%DIFAUXCMD
%\DIFdel{Furthermore, these bounds lead to a forward error result as shown in Equation \ref{eqn:ipforward2},
%}\begin{displaymath}
%\DIFdel{%DIFDELCMD < \label{eqn:ipforward2}%%%
%|\bb{x}^{\top}\bb{y}-\fl(\bb{x}^{\top}\bb{y})| \leq \gamma_p^{(d+1)}|\bb{x}|^{\top}|\bb{y}|.
%}\end{displaymath}
%%DIFAUXCMD
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\subsection{\DIFdel{Proof for Mixed-Precision HQR result}}
%%DIFAUXCMD
%\addtocounter{subsection}{-1}%DIFAUXCMD
%%DIFDELCMD < \label{Appendix:HQR}
%%DIFDELCMD < %%%
%\DIFdel{Here, we show a few results that are necessary for the proof for Theorem~\ref{thm:feHQR}.
%Lemma~\ref{lem:19.2} shows normwise results for a single mixed-precision Householder transformation performed on a vector, and Lemma~\ref{lem:19.3} builds on Lemma~\ref{lem:19.2} to show normwise results for multiple mixed-precision Householder transformations on a vector. 
%We build column-wise results for HQR based on these lemmas and then compute the matrix norms at the end.
%}%DIFDELCMD < \begin{lemma}
%%DIFDELCMD < 	%DIFDELCMD < \label{lem:19.2}%%%
%%DIFDELCMD < 	%%%
%\DIFdel{Let $\bb{x}\in\R^m$ and consider the computation of $\bb{y}=\hat{\bb P_v}\bb{x} = \bb{x}-\hat{\beta}\hat{\bb{v}}\hat{\bb{v}}^{\top}\bb{x}$, where $\hat{\bb{v}}$ has accumulated error shown in Lemma~\ref{lem:HQRv}.
%	Then, the computed $\hat{\bb{y}}$ satisfies 
%	}\begin{displaymath}
%	\DIFdel{\hat{\bb y} = (\bb{P}+\bb{\Delta P}) \bb{x},\quad \|\bb{\Delta P}\|_F\leq\gamma_w^{(6d+6z+13)},
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{where $\bb{P} = \bb{I}-\beta\bb{v}\bb{v}^{\top}$ is a Householder transformation.
%}%DIFDELCMD < \end{lemma}
%%DIFDELCMD < \begin{proof}
%%DIFDELCMD < 	%%%
%\DIFdel{Recall that the computed $\hat{\bb y}$ accumulates component-wise error shown in Equation~\ref{eqn:applyP}.
%	Even though we do not explicitly form $\bb{P}$, forming the normwise error bound for this matrix makes the analysis simple.
%	First, recall that any matrix $\bb{A}$ with rank $r$ has the following relations between its 2-norm and Frobenius norm,
%	%DIF < %%\begin{equation}
%	$\|\bb{A}\|_2\leq\|\bb{A}\|_F\leq\sqrt{r}\|\bb{A}\|_2$.
%	%DIF < %%\end{equation}
%	Then, we have 
%	}\begin{displaymath}
%	\DIFdel{%DIFDELCMD < \label{eqn:19.2a}%%%
%	\|\bb{y}\|_2 = \|\bb{P x}\|_2 \leq \|\bb{P}\|_2 \|\bb{x}\|_2 = \|\bb{x}\|_2,
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{since $\bb{P}$ is orthogonal and $\|\bb{P}\|_2=1$.
%	We now transition from the componentwise error to normwise error for $\bb{\Delta y}$, and write $\tilde{z} = 6d+6z+13$:
%	}\begin{displaymath}
%	\DIFdel{%DIFDELCMD < \label{eqn:19.2b}%%%
%	\|\bb{\Delta y}\|_2 = \left(\sum_{i=1}^m \bb{\Delta y}_i^2\right)^{1/2} \leq \gamma_w^{(\tilde{z})}\left(\sum_{i=1}^m \bb{y}_i^2\right)^{1/2} =  \gamma_w^{(\tilde{z})}\|\bb{y}\|_2
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{Combining Equations~\ref{eqn:19.2a} and \ref{eqn:19.2b}, we find
%	}\begin{displaymath}
%	\DIFdel{\frac{\|\bb{\Delta y}\|_2}{\|\bb{x}\|_2} \leq \gamma_w^{(\tilde{z})}. %DIFDELCMD < \label{eqn:19.2c}%%%
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{Now, notice that $\bb{\Delta P}$ is exactly $\frac{1}{\bb{x}^{\top}\bb{x}}\bb{\Delta y}\bb{x}^{\top}$; thus, 
%	}\begin{align*}
%	\DIFdel{(\bb{P}+\bb{\Delta P}) \bb{x} }&\DIFdel{= (\bb{P}+\frac{1}{\bb{x}^{\top}\bb{x}}\bb{\Delta y}\bb{x}^{\top})\bb{x}    %DIF < %%\\
%	%DIF < %%&
%	=\bb{P}\bb{x}  + \frac{\bb{x}^{\top}\bb{x}}{\bb{x}^{\top}\bb{x}}\bb{\Delta y} = \bb{y} + \bb{\Delta y}
%	}\end{align*}
%	%DIFAUXCMD
%\DIFdel{We can compute the Frobenius norm of $\bb{\Delta P}$ by using $\bb{\Delta P}_{ij} = \frac{1}{\|\bb{x}\|_2^2}\bb{\Delta y}_i\bb{x}_j$.
%	}\begin{align*}
%	\DIFdel{\|\bb{\Delta P}\|_F %DIF < %&= \left(\sum_{i=1}^m\sum_{j=1}^m\Delta \bb{P}_{ij}^2\right)^{1/2} 
%	= \left(\sum_{i=1}^m\sum_{j=1}^m\left(\frac{1}{\|\bb{x}\|_2^2}\bb{\Delta y}_i\bb{x}_j\right)^2\right)^{1/2} %DIF < %%\\
%	%DIF < %%&= \left(\frac{1}{\|\bb{x}\|_2^4}\sum_{i=1}^m\sum_{j=1}^m\bb{\Delta y}_i^2\bb{x}_j^2\right)^{1/2} 
%	%DIF < %%=\frac{1}{\|\bb{x}\|_2^2}\left(\sum_{i=1}^m \bb{\Delta y}_i^2\left( \sum_{j=1}^m\bb{x}_j^2\right)\right)^{1/2}\\
%	%DIF < %%&=\frac{1}{\|\bb{x}\|_2^2}\left(\|\bb{x}\|_2^2\sum_{i=1}^m\bb{\Delta y}_i^2 \right)^{1/2}
%	%DIF < %%=  \frac{\|\bb{x}\|_2\|\bb{\Delta y}\|_2}{\|\bb{x}\|_2^2} 
%	=  \frac{\|\bb{\Delta y}\|_2}{\|\bb{x}\|_2}
%	}\end{align*}
%	%DIFAUXCMD
%\DIFdel{Finally, using Equation~\ref{eqn:19.2c}, we find $\|\bb{\Delta P}\|_F \leq \gamma_w^{(\tilde{z})}$.
%}%DIFDELCMD < \end{proof}
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%%DIF < \begin{lemma}
%%DIF < 	\label{lem:3.7}
%%DIF < 	If $\bb{P}_j + \bb{\Delta P}_j\in\R^{m\times m}$ satisfies $\|\bb{\Delta P}_j\|_F\leq\delta_j\|\bb{P}_j\|_2$ for all $j$, then
%%DIF < 	\begin{equation}
%%DIF < 	\left|\left|\prod_{j=0}^m \left(\bb{P}_j + \bb{\Delta P}_j\right) - \prod_{j=0}^m \bb{P}_j\right|\right|_F\leq \left(\prod_{j=0}^m(1+\delta_j)-1\right)\prod_{j=0}^m\|\bb{P}_j\|_2
%%DIF < 	\end{equation}
%%DIF < \end{lemma}
%%DIFDELCMD < 
%
%%DIFDELCMD < \begin{lemma}
%%DIFDELCMD < 	%DIFDELCMD < \label{lem:19.3}%%%
%%DIFDELCMD < 	%%%
%\DIFdel{Consider applying a sequence of transformations in the set $\{\bb{P}_j\}_{j=1}^r\subset\R^{m\times m}$ to $\bb{x}\in\R^m$, where $\bb{P}_j$'s are all Householder transformations and where we will assume that $r\gamma_w^{(\tilde{z})}<\frac{1}{2}.$ 
%	Let $\bb{y} = \bb{P}_r\bb{P}_{r-1}\cdots\bb{P}_1\bb{x} = \bb{Q}^{\top}\bb{x}$.
%	Then, $\hat{\bb{y}} = (\bb{Q}+\bb{\Delta Q})^{\top}\bb{x}$, where 
%	}\begin{displaymath}
%	\DIFdel{\|\bb{\Delta Q}\|_F \leq r\gamma_w^{(\tilde{z})},\quad  \|\bb{\Delta y}\|_2 \leq r \gamma_w^{(\tilde{z})} \|\bb{y}\|_2.%DIFDELCMD < \label{eqn:19.3}%%%
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{In addition, if we let $\hat{\bb{y}} =\bb{Q}^{\top}(\bb{x} + \bb{\Delta x})$, then 
%	}\begin{displaymath}
%	\DIFdel{\|\bb{\Delta x}\|_2 \leq r \gamma_w^{(\tilde{z})} \|\bb{x}\|_2.%DIFDELCMD < \label{eqn:19.3c}%%%
%	}\end{displaymath}
%%DIFAUXCMD
%%DIFDELCMD < \end{lemma}
%%DIFDELCMD < 
%
%%DIFDELCMD < \begin{proof}
%%DIFDELCMD < 	%%%
%\DIFdel{As was for the proof for Lemma~\ref{lem:19.2}, we know $\bb{\Delta Q}^{\top} = \frac{1}{\|\bb{x}\|_2^2}\bb{\Delta y}\bb{x}^{\top}$.
%	Recall that the HQR factorization applies a series of Householder transformations on $\bb{A}$ to form $\bb{R}$, and applies the same series of Householder transformations in reverse order to $\bb{I}$ to form $\bb{Q}$.
%	Therefore, it is appropriate to assume that $\bb{x}$ is exact in this proof, and we form a forward bound on $\hat{\bb{y}}$.
%	However, we can still easily switch between forward and backward errors in the following way:
%	}\begin{align*}
%	\DIFdel{\hat{\bb{y}} }&\DIFdel{= \bb{y} + \bb{\Delta y} = \bb{Q}^{\top} (\bb{x}+\bb{\Delta x}) = (\bb{Q}+\bb{\Delta Q})^{\top} \bb{x},}\\
%	\DIFdel{\bb{\Delta y} }&\DIFdel{= \bb{\Delta Q}^{\top} \bb{x} = \bb{Q}^{\top}\bb{\Delta x}.
%	}\end{align*}
%	%DIFAUXCMD
%\DIFdel{In addition, we can switch between $\bb{\Delta y}$ and $\bb{\Delta x}$ by using the fact that $\|\bb{Q}\|_2 = 1$.
%	%DIF < So another way of formulating $\bb{\Delta Q}$ is given by $\frac{1}{\|\bb{x}\|_2^2}(\bb{Q}\bb{\Delta x})\bb{x}^{\top}$, where $\bb{\Delta x}$ can be understood as the backward error.\par	
%	}\paragraph{\DIFdel{Error bound for $\|\bb{\Delta y}\|_2$}}
%	%DIFAUXCMD
%\addtocounter{paragraph}{-1}%DIFAUXCMD
%\DIFdel{We will first find $\|\bb{\Delta Q}\|_2$, where this is NOT the forward error from forming $\bb{Q}$ with Householder transformations, but rather a backward error in accumulating Householder transformations. 
%	From Lemma~\ref{lem:19.2}, we have$ \|\bb{\Delta P}\|_F\leq\gamma_w^{(\tilde{z})} = \gamma_w^{(\tilde{z})} \|\bb{P}\|_2$ for any Householder transformation $\bb{P}\in\R^{m\times m}$, where $\tilde{z} = 6d+6z+13$ and $d=\lfloor\frac{(m-1)u_s}{u_w}\rfloor$, $z\in\{1,2\}$.
%	Therefore, this applies to the sequence of $\bb{P}_i$'s that form $\bb{Q}$ as well.
%	}%DIFDELCMD < 
%
%%DIFDELCMD < 	%%%
%\DIFdel{We will now use Lemma 3.7 from \cite{Higham2002} to bound $\bb{\Delta Q}$:
%	}\begin{align*}
%	\DIFdel{\bb{\Delta Q}^{\top}}& \DIFdel{= \left(\hat{\bb{Q}} - \bb{Q}\right)^{\top}= \prod_{i=r}^{1}\left(\bb{P}_i +\bb{\Delta P}_i\right) - \prod_{i=r}^{1}\bb{P}_i,}\\
%	\DIFdel{\|\bb{\Delta Q}\|_F = \|\bb{\Delta Q}^{\top}\|_F  }&\DIFdel{= \left|\left| \prod_{i=r}^{1}\left(\bb{P}_i +\bb{\Delta P}_i\right) - \prod_{i=r}^{1}\bb{P}_i \right|\right|_F,}\\
%	&\DIFdel{\leq \left(\prod_{i=r}^1(1+\gamma_w^{(\tilde{z})})-1\right)\prod_{i=r}^1\|\bb{P}_i\|_2 = \prod_{i=r}^1(1+\gamma_w^{(\tilde{z})})-1.
%	}\end{align*}
%	%DIFAUXCMD
%\DIFdel{The last equality results from the orthogonality of Householder matrices.}%DIFDELCMD < \par
%%DIFDELCMD < 	
%
%%DIFDELCMD < 	%%%
%\DIFdel{Consider the constant, $(1+\gamma_w^{(\tilde{z})})^r-1$.
%	From the very last rule in Lemma~\ref{lem:up}, we can generalize the following:
%	}\begin{displaymath}
%	\DIFdel{(1+\gamma_w^{(\tilde{z})})^r = (1+\gamma_w^{(\tilde{z})})^{r-2}(1+\gamma_w^{(\tilde{z})})(1+\gamma_w^{(\tilde{z})}) \leq  (1+\gamma_w^{(\tilde{z})})^{r-2}(1+\gamma_w^{(2\tilde{z})}) \leq \cdots \leq (1+\gamma_w^{(r\tilde{z})}).
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{So, our quantity of interest can be bound by $(1+\gamma_w^{(\tilde{z})})^r-1 \leq \gamma_w^{(r\tilde{z})}$.
%	}%DIFDELCMD < 
%
%%DIFDELCMD < 	%%%
%\DIFdel{Now we will use the following equivalent algebraic inequalities to get the final result.
%	}\begin{displaymath}
%	\DIFdel{0<a<b<1 \Leftrightarrow 1-a > 1-b \Leftrightarrow \frac{1}{1-a} <\frac{1}{1-b} \Leftrightarrow \frac{a}{1-a} < \frac{b}{1-b}
%	%DIFDELCMD < \label{eqn:algebra}%%%
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{In addition, we assume $r\gamma_w^{(\tilde{z})}< \frac{1}{2}$, such that 
%	}\begin{align*}
%	\DIFdel{(1+\gamma_w^{(\tilde{z})})^r-1 }&\DIFdel{\leq \gamma_w^{(r\tilde{z})} = \frac{r\tilde{z}u_w}{1-r\tilde{z}u_w}\qquad\text{(by definition)}}\\
%	&\DIFdel{\leq \frac{r\gamma_w^{(\tilde{z})}}{1-r\gamma_w^{(\tilde{z})}},\text{ since } r\tilde{z}u_w < r\gamma_w^{(\tilde{z})}\qquad\text{(by Equation \ref{eqn:algebra})}}\\
%	&\DIFdel{\leq 2 r \gamma_w^{(\tilde{z})}\qquad\text{(since $r\gamma_w^{(\tilde{z})}< \frac{1}{2}$ implies  $\frac{1}{1-r\gamma_w^{(\tilde{z})}} < 2$)}}\\
%	&\DIFdel{= r\tilde{\gamma}_w^{(\tilde{z})},
%	}\end{align*}
%	%DIFAUXCMD
%\DIFdel{where $\tilde{\gamma^{(m)}}:= \frac{cmu}{1-cmu}$ for some small integer, $c$.
%	If we had started with $(1+\tilde{\gamma}_w^{(\tilde{z})})^r-1$, we can still find $r\tilde{\gamma}_w^{(\tilde{z})}$ assuming that $2c$ is still a small integer. 
%	In conclusion, we have 
%	}\begin{displaymath}
%	\DIFdel{(1+\gamma_w^{(\tilde{z})})^r-1 \leq r\tilde{\gamma}_w^{(\tilde{z})},
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{which results in the bound for $\bb{\Delta Q}$ as shown in Equation~\ref{eqn:19.3}, $\|\bb{\Delta Q}\|_2 \leq \|\bb{\Delta Q}\|_F \leq r\gamma_w^{(\tilde{z})}$.
%	}%DIFDELCMD < 
%
%%DIFDELCMD < 	%%%
%\DIFdel{Next, we bound $\|\bb{\Delta y}\|_2 = \|\bb{\Delta Q x}\|_2 \leq \|\bb{\Delta Q}\|_2 \|\bb{x}\|_2 \leq  r\tilde{\gamma}_w^{(\tilde{z})}\|\bb{x}\|_2$.
%	}%DIFDELCMD < 
%
%%DIFDELCMD < 	%%%
%\paragraph{\DIFdel{Bound for $\|\bb{\Delta x}\|_2$}}
%	%DIFAUXCMD
%\addtocounter{paragraph}{-1}%DIFAUXCMD
%\DIFdel{We use the above result,
%	}\begin{displaymath}
%	\DIFdel{\|\bb{\Delta x}\|_2 = \|\bb{Q \Delta y}\|_2\| \leq \|\bb{Q}\|_2\|\bb{\Delta y}\|_2 = \|\bb{\Delta y}\|_2 \leq  r\tilde{\gamma}_w^{(\tilde{z})}\|\bb{x}\|_2.
%	}\end{displaymath}
%	%DIFAUXCMD
%\DIFdel{While $r\gamma^{(k)} = r\frac{ku}{1-ku} < \frac{rku}{1-rku} =\gamma^{(rk)}$ holds true when $r>0$ and $rku< 1$ are satisfied, the strict inequality implies that $r\gamma^{(k)}$ is a tighter bound than $\gamma^{(rk)}$.
%	However, $\gamma^{(rk)}$ is easier to work with using the rules in Lemma~\ref{lem:gamma}.
%}%DIFDELCMD < \end{proof}
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\subsubsection{\DIFdel{Proof for Theorem \ref{thm:feHQR}}}
%%DIFAUXCMD
%\addtocounter{subsubsection}{-1}%DIFAUXCMD
%\DIFdel{First, we use Lemma~\ref{lem:19.3} directly on columns of $\bb{A}$ and $\bb{I}_{m\times n}$ to get a result for columns of $\hat{\bb{R}}$ and $\hat{\bb{Q}}$.
%%DIF < Figure~\ref{fig:QRerr} shows that each elements of $\hat{\bb{Q}}$ and $\hat{\bb{R}}$ each go through different numbers of Householder transformations and Householder transformations for different lengths of vectors as well. 
%We will use the maximum number of transformations and the length of the longest vector on to which we perform a Householder transformation, that is, $n$ transformations of vectors of length $m$. 
%%DIF < Also, note that $n\gamma^{(k)} = \frac{nku}{1-ku}<\frac{nku}{1-nku} = \gamma^{(nk)}$ as long as $nku <1$.
%For $j$ in $\{1, \cdots, n\}$, the $j^{th}$ column of $\bb{R}$ and $\bb{Q}$ are the results of $j$ Householder transformations on $\bb{A}$ and $\bb{I}$:
%}\begin{align*}
%\DIFdel{\|\bb{\Delta Q}[:,j]\|_2 }&\DIFdel{\leq j\tilde{\gamma}_w^{(\tilde{z})}\|\hat{e}_{j}\|_2 < \tilde{\gamma}_w^{(j\tilde{z})}, }\\
%\DIFdel{\|\bb{\Delta R}[:,j]\|_2 }&\DIFdel{\leq j\tilde{\gamma}_w^{(\tilde{z})}\|\bb{A}[:,j]\|_2 < \tilde{\gamma}_w^{(j\tilde{z})}\|\bb{A}[:,j]\|_2.
%}\end{align*}
%%DIFAUXCMD
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{Finally, we relate columnwise 2-norms to matrix Frobenius norms.
%%DIF < \begin{lemma}
%%DIF < 	Let $\bb{A}=[\bb{c}_1 \cdots \bb{c}_n]\in\R^{m\times n}$, where $\bb{c_i}\in\R^m$ for $i=1, \cdots, n$. 
%%DIF < 	If $\max_{i \in\{1, \cdots, n\}}\|\bb{c}_i\| = \epsilon$, then
%%DIF < 	\begin{equation}
%%DIF < 	\|\bb{A}\|_F \leq \sqrt{n}\epsilon
%%DIF < 	\end{equation}
%%DIF < \end{lemma}
%%DIF < \begin{proof}
%%DIF < 	The Frobenius norm is exactly the 2-norm for vectors. 
%%DIF < 	\begin{equation*}
%%DIF < 	\|\bb{A}\|_F = \left(\sum_{i=1}^n \sum_{j=1}^m \bb{A}_{ij}^2\right)^{1/2} = \left(\sum_{i=1}^n\|\bb{c}_i\|_2^2\right)^{1/2}\leq \left(\sum_{i=1}^n \epsilon^2\right)^{1/2} = \sqrt{n}\epsilon
%%DIF < 	\end{equation*}
%%DIF < \end{proof}
%It is straightforward to see the result for the $\bb{Q}$ factor,
%}\begin{displaymath}
%\DIFdel{\|\bb{\Delta Q}\|_F = \left(\sum_{j=1}^n \|\bb{\Delta Q}[:,j]\|_2^2\right)^{1/2} \leq \left(\sum_{j=1}^n (j\tilde{\gamma}_w^{(\tilde{z})})^2 \|\hat{e}_j\|_2^2\right)^{1/2} \leq n^{3/2}\tilde{\gamma}_w^{(\tilde{z})}.
%}\end{displaymath}
%%DIFAUXCMD
%\DIFdel{Note that we bound $\sum_{j=1}^n j^2$ by $n^3$, but the summation is actually exactly $\frac{n(n+1)(2n+1)}{6}$. 
%Therefore, a tighter bound would replace $n^{3/2}$ with $\left(\frac{n(n+1)(2n+1)}{6}\right)^{1/2}$.}%DIFDELCMD < \par 
%%DIFDELCMD < %%%
%\DIFdel{We can bound the $\bb{R}$ factor in a similar way,
%}\begin{displaymath}
%\DIFdel{\|\bb{\Delta R}\|_F = \left(\sum_{j=1}^n \|\bb{\Delta R}[:,j]\|_2^2\right)^{1/2} \leq \left(\sum_{j=1}^n (j\tilde{\gamma}_w^{(\tilde{z})})^2 \|\bb{A}[:,j]\|_2^2\right)^{1/2} \leq n\tilde{\gamma}_w^{(\tilde{z})} \|\bb{A}\|_F.
%}\end{displaymath}
%%DIFAUXCMD
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{Obtaining the backward error from the HQR factorization,
%}\begin{displaymath}\DIFdel{\bb{\Delta A}=\bb{A}-\hat{\bb{Q}}\hat{\bb{R}}
%= \bb{A}-\bb{Q}\hat{\bb{R}} + \bb{Q}\hat{\bb{R}} - \hat{\bb{Q}}\hat{\bb{R}}
%= \bb{Q \Delta R} + \bb{\Delta Q} \hat{\bb{R}}.}\end{displaymath}
%%DIFAUXCMD
%%DIFDELCMD < 
%
%%DIFDELCMD < %%%
%\DIFdel{A columnwise result for $\hat{\bb{A}}$ is shown by
%}\begin{align*}
%\DIFdel{\|\bb{\Delta A}[:,j]\|_2 }& \DIFdel{= \|(\bb{Q \Delta R} + \bb{\Delta Q} \hat{\bb{R}} )}[\DIFdel{:,j}]\DIFdel{\|_2,}\\
%&\DIFdel{\leq \|\bb{Q \Delta R}[:,j]\|_2  + \|\bb{\Delta Q}\hat{\bb{R}}[:,j]\|_2,}\\
%&\DIFdel{\leq \|\bb{\Delta R}[:,j]\|_2 + \|\bb{\Delta Q}\|_2\|\hat{\bb{R}}[:,j]\|_2,}\\
%&\DIFdel{\leq \|\bb{\Delta R}[:,j]\|_2 + \|\bb{\Delta Q}\|_F\|(\bb{R} +\bb{\Delta R})}[\DIFdel{:,j}]\DIFdel{\|_2,}\\
%&\DIFdel{\leq j\tilde{\gamma}_w^{(\tilde{z})}\|\bb{A}[:,j]\|_2 + n^{3/2}\tilde{\gamma}_w^{(\tilde{z})} \|(\bb{Q}^{\top}\bb{A} +\bb{\Delta R})}[\DIFdel{:,j}]\DIFdel{\|_2,}\\
%&\DIFdel{\leq j\tilde{\gamma}_w^{(\tilde{z})}\|\bb{A}[:,j]\|_2 + n^{3/2}\tilde{\gamma}_w^{(\tilde{z})} \left(\|\bb{A}[:,j]\|_2 +\|\bb{\Delta R}[:,j]\|_2\right),}\\
%&\DIFdel{\leq \left(j\tilde{\gamma}_w^{(\tilde{z})} + n^{3/2}\tilde{\gamma}_w^{(\tilde{z})}  (1+j\tilde{\gamma}_w^{(\tilde{z})})\right)\|\bb{A}[:,j]\|_2,}\\
%&\DIFdel{=n^{3/2}\tilde{\gamma}_w^{(\tilde{z})}  \|\bb{A}[:,j]\|_2,
%}\end{align*}
%%DIFAUXCMD
%\DIFdel{where we assume $n\tilde{\gamma}_w^{(\tilde{z})}\ll 1$ and where the last equality sweeps all non-leading order terms into the arbitrary constant $c$ within the definition of $\tilde{\gamma}$,
%}\begin{displaymath}
%\DIFdel{\|\bb{\Delta A}\|_F = \left(\sum_{j=1}^n \|\bb{\Delta A}[:,j]\|_2^2\right)^{1/2} \leq \left(\sum_{j=1}^n (n^{3/2}\tilde{\gamma}_w^{(\tilde{z})})^2 \|\bb{A}[:,j]\|_2^2\right)^{1/2} \leq n^{3/2}\tilde{\gamma}_w^{(\tilde{z})} \|\bb{A}\|_F.
%}\end{displaymath}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \bibliography{report,../../../../library.bib,../../../../sans_library.bib}
%DIFDELCMD < %%%
\DIFdelend %DIF > \appendix
%DIF > \input{BQRdeets}
%DIF > \input{appendixMPD}
\DIFaddbegin \bibliography{../../../../../library.bib,../../../../../sans_library.bib,./report.bib}
\DIFaddend \bibliographystyle{siamplain}%ieeetr
\end{document}
