Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{HighamMary2020,
author = {Higham, Nicholas J. and Mary, Theo},
file = {:Users/luciaminahyang/Documents/school/research/Readings/HighamMary2020.pdf:pdf},
journal = {MIMS EPrint},
keywords = {floating-point arithmetic,probabilistic error bounds,rounding error analysis},
title = {{Sharper Probabilistic Backward Error Analysis for Basic Linear Algebra Kernels with Random Data}},
year = {2020}
}
@article{Cox2002,
abstract = {We develop a class of numerical methods for stiff systems, based on the method of exponential time differencing. We describe schemes with second- and higher-order accuracy, introduce new Runge-Kutta versions of these schemes, and extend the method to show how it may be applied to systems whose linear part is nondiagonal. We test the method against other common schemes, including integrating factor and linearly implicit methods, and show how it is more accurate in a number of applications. We apply the method to both dissipative and dispersive partial differential equations, after illustrating its behavior using forced ordinary differential equations with stiff linear parts. {\textcopyright} 2002 Elsevier Science (USA).},
author = {Cox, S. M. and Matthews, P. C.},
doi = {10.1006/jcph.2002.6995},
file = {:Users/luciaminahyang/Documents/school/research/Readings/CoxMatthews2002.pdf:pdf;:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Cox, Matthews - 2002 - Exponential time differencing for stiff systems.pdf:pdf},
issn = {00219991},
journal = {Journal of Computational Physics},
keywords = {Exponential time differencing,Integrating factor methods,Stiff systems},
number = {2},
pages = {430--455},
title = {{Exponential time differencing for stiff systems}},
volume = {176},
year = {2002}
}
@article{Sarlos2006,
abstract = {Recently several results appeared that show significant reduction in time for matrix multiplication, singular value decomposition as well as linear (ℓ2) regression, all based on data dependent random sampling. Our key idea is that low dimensional embeddings can be used to eliminate data dependence and provide more versatile, linear time pass efficient matrix computation. Our main contribution is summarized as follows. Independent of the recent results of Har-Peled and of Deshpande and Vempala, one of the first-and to the best of our knowledge the most efficient - relative error (1 + $\epsilon$) ∥ A - A k∥F approximation algorithms for the singular value decomposition of an m × n matrix A with M non-zero entries that requires 2 passes over the data and runs in time O ((M k/∈ + k log k) + (n + m) (k/∈ + k log k)2) log i/$\delta$). The first o(nd2) time (1 + ∈) relative error approximation algorithm for n × d linear (ℓ2) regression. A matrix multiplication and norm approximation algorithm that easily applies to implicitly given matrices and can be used as a black box probability boosting tool. {\textcopyright} 2006 IEEE.},
author = {Sarl{\'{o}}s, Tam{\'{a}}s},
doi = {10.1109/FOCS.2006.37},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Sarlos.pdf:pdf},
isbn = {0769527205},
issn = {02725428},
journal = {Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS},
pages = {143--152},
title = {{Improved approximation algorithms for large matrices via random projections}},
year = {2006}
}
@article{Garcia2014,
abstract = {We assess the accuracy and efficiency of several exponential time integration methods coupled to a spectral discretization of the three-dimensional Boussinesq thermal convection equations in rotating spherical shells. Exponential methods are compared to implicit-explicit (IMEX) multi-step methods already studied previously in [1]. The results of a wide range of numerical simulations highlight the superior accuracy of exponential methods for a given time step, especially when employed with large time steps and at low Ekman number. However, presently available implementations of exponential methods appear to be in general computationally more expensive than those of IMEX methods and further research is needed to reduce their computational cost per time step. A physically justified extrapolation argument suggests that some exponential methods could be the most efficient option for integrating flows near Earth's outer core conditions. {\textcopyright} 2014 Elsevier Inc.},
author = {Garcia, Ferran and Bonaventura, Luca and Net, Marta and S{\'{a}}nchez, Juan},
doi = {10.1016/j.jcp.2014.01.033},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Garcia2014.pdf:pdf},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {Exponential integrators,Geophysical modeling,Low Ekman number flows,Semi-implicit schemes,Thermal convection},
pages = {41--54},
publisher = {Elsevier Inc.},
title = {{Exponential versus IMEX high-order time integrators for thermal convection in rotating spherical shells}},
url = {http://dx.doi.org/10.1016/j.jcp.2014.01.033},
volume = {264},
year = {2014}
}
@article{Higueras2005,
abstract = {Over the last few years a great effort has been made to develop monotone high order explicit Runge-Kutta methods by means of their Shu-Osher representations. In this context, the stepsize restriction to obtain numerical monotonicity is normally computed using the optimal representation. In this paper we extend the Shu-Osher representations for any Runge-Kutta method giving sufficient conditions for monotonicity. We show how optimal Shu-Osher representations can be constructed from the Butcher tableau of a Runge-Kutta method. The optimum stepsize restriction for monotonicity is given by the radius of absolute monotonicity of the Runge-Kutta method [L. Ferracina and M. N. Spijker, SIAM J. Numer. Anal., 42 (2004), pp. 1073-1093], and hence if this radius is zero, the method is not monotone. In the Shu-Osher representation, methods with zero radius require negative coefficients, and to deal with them, an extra associate problem is considered. In this paper we interpret these schemes as representations of perturbed Runge-Kutta methods. We extend the concept of radius of absolute monotonicity and give sufficient conditions for monotonicity. Optimal representations can be constructed from the Butcher tableau of a perturbed Runge-Kutta method. ?? 2005 Society for Industrial and Applied Mathematics.},
author = {Higueras, Inmaculada},
doi = {10.1137/S0036142903427068},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Higueras{\_}RK.pdf:pdf},
issn = {00361429},
journal = {SIAM Journal on Numerical Analysis},
keywords = {Absolutely monotonic,CFL coefficient,Radius of absolute monotonicity,Representations,Runge-Kutta,SSP,Strong stability preserving},
number = {3},
pages = {924--948},
title = {{Representations of Runge-kutta methods and strong stability preserving methods}},
volume = {43},
year = {2005}
}
@article{LeReun2017,
abstract = {The combination of elliptical deformation of streamlines and vorticity can lead to the destabilization of any rotating flow via the elliptical instability. Such a mechanism has been invoked as a possible source of turbulence in planetary cores subject to tidal deformations. The saturation of the elliptical instability has been shown to generate turbulence composed of nonlinearly interacting waves and strong columnar vortices with varying respective amplitudes, depending on the control parameters and geometry. In this Letter, we present a suite of numerical simulations to investigate the saturation and the transition from vortex-dominated to wave-dominated regimes. This is achieved by simulating the growth and saturation of the elliptical instability in an idealized triply periodic domain, adding a frictional damping to the geostrophic component only, to mimic its interaction with boundaries. We reproduce several experimental observations within one idealized local model and complement them by reaching more extreme flow parameters. In particular, a wave-dominated regime that exhibits many signatures of inertial wave turbulence is characterized for the first time. This regime is expected in planetary interiors.},
archivePrefix = {arXiv},
arxivId = {1706.07378},
author = {{Le Reun}, Thomas and Favier, Benjamin and Barker, Adrian J. and {Le Bars}, Michael},
doi = {10.1103/PhysRevLett.119.034502},
eprint = {1706.07378},
file = {:Users/luciaminahyang/Documents/school/research/Readings/LeReun{\_}etal{\_}PRL{\_}17.pdf:pdf},
issn = {10797114},
journal = {Physical Review Letters},
number = {3},
pages = {1--6},
title = {{Inertial Wave Turbulence Driven by Elliptical Instability}},
volume = {119},
year = {2017}
}
@article{Sugiyama2009,
abstract = {The moisture mode in a simplified version of the quasi-equilibrium tropical circulation model (QTCM) of Neelin and Zeng is analyzed. Perturbation expansion based on the ratio of temperature tendency to adiabatic cooling simplifies the system and dispersion relationship. The weak temperature gradient (WTG) approximation of Sobel, Nilsson, and Polvani naturally emerges as the dynamical balance of the moisture mode. The condition of the expansion can be phrased in terms of the nondimensional wavenumber and is satisfied in the tropics even for the planetary scale. The WTG growth rate equation demonstrates that the moisture mode is unstable when moist static energy sources such as cloud radiative forcing and gustiness-enhanced evaporation exceed its export. Wind-induced surface heat exchange does not affect the growth rate at the leading order, although it propagates the mode eastward in the mean easterly wind. For typical values of parameters, the time scale of moisture mode instability is several days. Nonlinear WTG calculations show that the moisture mode nonlinearly saturates by a thermodynamic limiting process. In the standard parameter regime, a phase diagram reveals two stable fixed points in addition to the unstable solution of radiative-convective equilibrium. {\textcopyright} 2009 American Meteorological Society.},
author = {Sugiyama, Masahiro},
doi = {10.1175/2008JAS2690.1},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Sugiyama2009pdf.pdf:pdf},
issn = {00224928},
journal = {Journal of the Atmospheric Sciences},
number = {6},
pages = {1507--1523},
title = {{The moisture mode in the quasi-equilibrium tropical circulation model. Part I: Analysis based on the weak temperature gradient approximation}},
volume = {66},
year = {2009}
}
@article{Schreiber1989,
abstract = {A product {\$}Q = P{\_}1 \backslashcdots P{\_}r {\$}, of {\$}m \backslashtimes m{\$} Householder matrices can be written in the form {\$}Q = I + WY{\^{}}T {\$}, where W and Y are each {\$}m \backslashtimes r{\$}. This is called the {\$}WY{\$} representation of Q. It is of interest when implementing Householder techniques in high-performance computing environments that are especially good at matrix-matrix multiplication. In this note a storage-efficient way to implement the {\$}WY{\$} representation is described. In particular, it is shown how the matrix Q can be expressed in the form {\$}Q = I + YTY{\^{}}T {\$}, where {\$}Y \backslashin R{\^{}}{\{}m \backslashtimes r{\}} {\$} and {\$}T \backslashin R{\^{}}{\{}r \backslashtimes r{\}} {\$} with T upper triangular. Usually {\$}r \backslashll m{\$} and so this “compact” {\$}WY{\$} representation requires less storage. When compared with the recent block-reflector strategy proposed by Schreiber and Parlett [SIAM J. Numer. Anal, 25 (1988), pp. 189–205], the new technique still has a storage advantage and involves a comparable amount of work.},
author = {Schreiber, Robert and {Van Loan}, Charles},
doi = {10.1137/0910005},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Schreiber1989.pdf:pdf},
issn = {0196-5204},
journal = {SIAM Journal on Scientific and Statistical Computing},
keywords = {65f,65w,ams,block methods,e,h i 2vv where,householder matrices,householder transformations,i,introduction,least squares methods rely,many important eigenvalue and,matrices of the form,mos,on,subject classifications,v has unit 2-norm},
number = {1},
pages = {53--57},
title = {{A Storage-Efficient {\$}WY{\$} Representation for Products of Householder Transformations}},
volume = {10},
year = {1989}
}
@article{Damle2016,
abstract = {We present a new algorithm for spectral clustering based on a column-pivoted QR factorization that may be directly used for cluster assignment or to provide an initial guess for k-means. Our algorithm is simple to implement, direct, and requires no initial guess. Furthermore, it scales linearly in the number of nodes of the graph and a randomized variant provides significant computational gains. Provided the subspace spanned by the eigenvectors used for clustering contains a basis that resembles the set of indicator vectors on the clusters, we prove that both our deterministic and randomized algorithms recover a basis close to the indicators in Frobenius norm. We also experimentally demonstrate that the performance of our algorithm tracks recent information theoretic bounds for exact recovery in the stochastic block model. Finally, we explore the performance of our algorithm when applied to a real world graph.},
archivePrefix = {arXiv},
arxivId = {1609.08251},
author = {Damle, Anil and Minden, Victor and Ying, Lexing},
eprint = {1609.08251},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Damle, Minden, Ying - 2016 - Robust and efficient multi-way spectral clustering.pdf:pdf},
title = {{Robust and efficient multi-way spectral clustering}},
url = {http://arxiv.org/abs/1609.08251},
year = {2016}
}
@article{Saad2003,
abstract = {Since the first edition of this book was published in 1996, tremendous progress has been made in the scientific and engineering disciplines regarding the use of iterative methods for linear systems. The size and complexity of the new generation of linear and nonlinear systems arising in typical applications has grown. Solving the three-dimensional models of these problems using direct solvers is no longer effective. At the same time, parallel computing has penetrated these application areas as it became less expensive and standardized. Iterative methods are easier than direct solvers to implement on parallel computers but require approaches and solution algorithms that are different from classical methods. Iterative Methods for Sparse Linear Systems, Second Edition gives an in-depth, up-to-date view of practical algorithms for solving large-scale linear systems of equations. These equations can number in the millions and are sparse in the sense that each involves only a small number of unknowns. The methods described are iterative, i.e., they provide sequences of approximations that will converge to the solution. This new edition includes a wide range of the best methods available today. The author has added a new chapter on multigrid techniques and has updated material throughout the text, particularly the chapters on sparse matrices, Krylov subspace methods, preconditioning techniques, and parallel preconditioners. Material on older topics has been removed or shortened, numerous exercises have been added, and many typographical errors have been corrected. The updated and expanded bibliography now includes more recent works emphasizing new and important research topics in this field.},
author = {Saad, Yousef},
doi = {10.1137/1.9780898718003},
file = {:Users/luciaminahyang/Documents/school/research/Readings/saad.pdf:pdf},
journal = {Iterative Methods for Sparse Linear Systems},
title = {{Iterative Methods for Sparse Linear Systems}},
year = {2003}
}
@article{Page1998a,
author = {Page, Lawrence},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Page et al. - 1998 - The PageRank Citation Ranking Bringing Order to the Web.pdf:pdf},
title = {{The PageRank Citation Ranking: Bringing Order to the Web}},
year = {1998}
}
@article{Kingma2019,
abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
archivePrefix = {arXiv},
arxivId = {1906.02691},
author = {Kingma, Diederik P. and Welling, Max},
doi = {10.1561/2200000056},
eprint = {1906.02691},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Welling - 2019 - An introduction to variational autoencoders.pdf:pdf},
issn = {19358245},
journal = {Foundations and Trends in Machine Learning},
number = {4},
pages = {307--392},
title = {{An introduction to variational autoencoders}},
volume = {12},
year = {2019}
}
@article{Qr,
author = {Qr, Full and Factorization, Q R},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Qr, Factorization - Unknown - 4 4.1 QR Factorization.pdf:pdf},
pages = {36--47},
title = {{4 4.1 QR Factorization}}
}
@article{Tilgner1999,
author = {Tilgner, A},
doi = {10.1002/(SICI)1097-0363(19990730)30:6<713::AID-FLD859>3.0.CO;2-Y},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Tilgner1999.pdf:pdf},
issn = {0271-2091},
journal = {International Journal for Numerical Methods in Fluids},
keywords = {implicit time steps,precession,rotating fluids,spectral methods},
month = {jul},
number = {6},
pages = {713--724},
title = {{Spectral methods for the simulation of incompressible flows in spherical shells}},
url = {https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1097-0363(19990730)30:6{\%}3C713::AID-FLD859{\%}3E3.0.CO;2-Y},
volume = {30},
year = {1999}
}
@book{Nazarenko2011,
abstract = {Wave turbulence is the statistical mechanics of random waves with a broadband spectrum interacting via non-linearity.To understand its difference from non-random well-tuned coherent waves, one could compare the sound of thunder to a piece of classical music.Wave turbulence is surprisingly common and important in a great variety of physical settings, starting with the most familiar ocean waves to waves at quantum scales or to much longer waves in astrophysics.We will provide a basic overview of the wave turbulence ideas, approaches andmain results emphasising the physics of the phenomena and using qualitative descriptions avoiding, whenever possible, involvedmathematical derivations. In particular, dimensional analysis will be used for obtaining the key scaling solutions in wave turbulence – Kolmogorov–Zakharov (KZ) spectra.},
author = {Nazarenko, Sergey},
booktitle = {Lecture Notes in Physics},
doi = {10.1007/978-3-642-15942-8},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Wave{\_}Turbulence{\_}Nazarenko.pdf:pdf;:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Nazarenko - 2011 - Lecture Notes in Physics Vol 825 Wave Turbulence.pdf:pdf;:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Nazarenko - 2011 - Lecture Notes in Physics Vol 825 Wave Turbulence(2).pdf:pdf},
isbn = {9783642159411},
keywords = {equations,kinetic equations,kolmogorov,non-equilibrium statistical mechanics,non-linear partial differential equations,turbulence,waves,zakharov spectra},
title = {{Lecture Notes in Physics Vol 825: Wave Turbulence}},
url = {http://link.springer.com/10.1007/978-3-642-15942-8},
volume = {825},
year = {2011}
}
@article{Craig2013,
abstract = {If the influence of humidity on cumulus convection causes moist regions of the tropical troposphere to become moister and dry regions to become drier, and if horizontal mixing of moisture is not rapid enough to overcome this tendency, then the atmosphere will tend to separate into increasingly large moist and dry regions through a process of coarsening. We present a simple model for the moisture budget of the free troposphere, including subsidence drying, convective moistening, and horizontal mixing, and a constraint on total precipitation representing radiative-convective equilibrium. When initialized with a spatially uncorrelated moisture distribution, the model shows self-organization of precipitation with two main stages: A coarsening stage where the correlation length grows proportional to time to the power 1/2 and a droplet stage where precipitation is confined to a decreasing number of circular moist regions. A potential function is introduced to characterize the tendency for self-organization, which could be a useful diagnostic for analyzing cloud-resolving model simulations. {\textcopyright} 2013. Her Majesty the Queen in Right of Canada. American Geophysical Union.},
author = {Craig, G. C. and MacK, J. M.},
doi = {10.1002/jgrd.50674},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Craig{\_}Mack{\_}JGR{\_}13.pdf:pdf},
issn = {21698996},
journal = {Journal of Geophysical Research Atmospheres},
number = {16},
pages = {8761--8769},
title = {{A coarsening model for self-organization of tropical convection}},
volume = {118},
year = {2013}
}
@article{Baltensperger2001,
abstract = {We introduce the collocation method based on linear rational interpolation for solving general hyperbolic problems, prove its stability and its convergence in weighted norms and give numerical examples for its use. {\textcopyright} 2001 Elsevier Science B.V. All rights reserved.},
author = {Baltensperger, Richard and Berrut, Jean Paul},
doi = {10.1016/S0377-0427(00)00552-5},
file = {:Users/luciaminahyang/Documents/school/research/Readings/BaltenspergerBerrut2001.pdf:pdf},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
keywords = {Conformal point shifts,Linear rational interpolation,Spectral method,Time evolution partial differential equations},
number = {1-2},
pages = {243--258},
title = {{The linear rational collocation method}},
volume = {134},
year = {2001}
}
@book{Strikwerda,
author = {Strikwerda, John C},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Strikwerda.pdf:pdf},
isbn = {0898715679},
title = {{and Partial Differential and Partial Differential Second Edition}}
}
@book{Dongarra2000,
abstract = {Large-scale problems of engineering and scientific computing often require solutions of eigenvalue and related problems. This book gives a unified overview of theory, algorithms, and practical software for eigenvalue problems. The material is accessible for the first time to experts as well as many nonexpert users who need to choose the best state-of-the-art algorithms and software for their problems. Using an informal decision tree, just enough theory is introduced to identify the relevant mathematical structure that determines the best algorithm for each problem. The algorithm “leaves” of the decision tree range from the classical QR algorithm, which is most suitable for small dense matrices, to iterative algorithms for very large generalized eigenvalue problems. Algorithms are presented in a unified style as templates, with different levels of detail suitable for readers ranging from beginning students to experts.},
author = {Dongarra, Jack J and Kennedy, Ken},
booktitle = {Templates for the Solution of Algebraic Eigenvalue Problems},
doi = {10.1137/1.9780898719581},
file = {:Users/luciaminahyang/Documents/school/research/Readings/TemplateFor/Templates for the Solution of Algebraic Eigenvalue Problems- A Practical Guide.pdf:pdf},
isbn = {0898714710},
title = {{Templates for the Solution of Algebraic Eigenvalue Problems}},
year = {2000}
}
@article{Higham2019b,
abstract = {Multilayered artificial neural networks are becoming a pervasive tool in a host of application fields. At the heart of this deep learning revolution are familiar concepts from applied and computational mathematics, notably from calculus, approximation theory, optimization, and linear algebra. This article provides a very brief introduction to the basic ideas that underlie deep learning from an applied mathematics perspective. Our target audience includes postgraduate and final year undergraduate students in mathematics who are keen to learn about the area. The article may also be useful for instructors in mathematics who wish to enliven their classes with references to the application of deep learning techniques. We focus on three fundamental questions: What is a deep neural network? How is a network trained? What is the stochastic gradient method? We illustrate the ideas with a short MATLAB code that sets up and trains a network. We also demonstrate the use of state-of-the-art software on a large scale image classification problem. We finish with references to the current literature.},
archivePrefix = {arXiv},
arxivId = {1801.05894},
author = {Higham, Catherine F. and Higham, Desmond J.},
doi = {10.1137/18M1165748},
eprint = {1801.05894},
file = {:Users/luciaminahyang/Documents/school/research/Readings/DeepLearningHighams.pdf:pdf},
issn = {00361445},
journal = {SIAM Review},
keywords = {Back propagation,Chain rule,Convolution,Image classification,Neural network,Overfitting,Sigmoid,Stochastic gradient method,Supervised learning},
number = {4},
pages = {860--891},
title = {{Deep learning: An introduction for applied mathematicians}},
volume = {61},
year = {2019}
}
@article{Poulin,
author = {Poulin, Francis J. and Ko, William and Kevlahan, Nicholas K.-R and Fox-Kemper, Baylor},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Poulin{\_}etal{\_}JPO{\_}13{\_}Draft.pdf:pdf},
journal = {Geophysical and Astrophysical Fluid Dynamics},
title = {{Spectral characteristics of a turbulent wind-driven gyre flow}},
volume = {Submitted}
}
@article{MinchevWright,
abstract = {Recently, there has been a great deal of interest in the construction of exponential integrators. These integrators, as their name suggests, use the exponential function (and related functions) of the Jacobian or an approx- imation to it, inside the numerical method. However, unlike some of the recent literature suggests, integrators based on this philosophy have been known since at least 1960. The aim of this paper is to review exponential integrators, designed for first order problems, however, we will also briefly discuss some recent research into the construction of exponential integra- tors, for special problems. Our hope is, that with this article, by reviewing as much of the history of exponential integrators as reasonable possible, we can point interested readers to appropriate references and hopefully reduce the reinvention of known results.},
author = {Minchev, B. and Wright, W.},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Minchev{\_}Wright{\_}TechReport{\_}05.pdf:pdf},
journal = {Preprint Numerics},
pages = {1--45},
title = {{A review of exponential integrators for first order semi-linear problems}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:A+review+of+exponential+integrators+for+first+order+semi-linear+problems{\#}0},
volume = {2},
year = {2005}
}
@article{Nakatsukasa2019a,
abstract = {Rational minimax approximation of real functions on real intervals is an established topic, but when it comes to complex functions or domains, there appear to be no algorithms currently in use. Such a method is introduced here, the {\{}$\backslash$em AAA-Lawson algorithm,{\}} available in Chebfun. The new algorithm solves a wide range of problems on arbitrary domains in a fraction of a second of laptop time by a procedure consisting of two steps. First, the standard AAA algorithm is run to obtain a near-best approximation and a set of support points for a barycentric representation of the rational approximant. Then a "Lawson phase" of iteratively reweighted least-squares adjustment of the barycentric coefficients is carried out to improve the approximation to minimax.},
archivePrefix = {arXiv},
arxivId = {1908.06001},
author = {Nakatsukasa, Yuji and Trefethen, Lloyd N.},
eprint = {1908.06001},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Nakatsukasa, Trefethen - 2019 - An algorithm for real and complex rational minimax approximation.pdf:pdf},
keywords = {41a20,65d15,aaa algorithm,aaa-lawson algo-,ams subject classifications,barycentric formula,iteratively reweighted least-squares,rational approximation,rithm},
title = {{An algorithm for real and complex rational minimax approximation}},
url = {http://arxiv.org/abs/1908.06001},
year = {2019}
}
@article{Duersch2017,
abstract = {The dominant contribution to communication complexity in factorizing a matrix using QR with column pivoting is due to column-norm updates that are required to process pivot decisions. We use randomized sampling to approximate this process which dramatically reduces communication in column selection. We also introduce a sample update formula to reduce the cost of sampling trailing matrices. Using our column selection mechanism, we observe results that are comparable in quality to those obtained from the QRCP algorithm, but with performance near un-pivoted QR. We also demonstrate strong parallel scalability on shared-memory multiple core systems using an implementation in Fortran with OpenMP. This work immediately extends to produce low-rank truncated approximations of large matrices. We propose a truncated QR factorization with column pivoting that avoids trailing matrix updates which are used in current implementations of level-3 BLAS QR and QRCP. Provided the truncation rank is small, avoiding trailing matrix updates reduces approximation time by nearly half. By using these techniques and employing a variation on Stewart's QLP algorithm, we develop an approximate truncated SVD that runs nearly as fast as truncated QR.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.06820v2},
author = {Duersch, Jed A. and Gu, Ming},
doi = {10.1137/15M1044680},
eprint = {arXiv:1509.06820v2},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Duersch, Gu - 2017 - Randomized qr with column pivoting.pdf:pdf},
issn = {10957197},
journal = {SIAM Journal on Scientific Computing},
keywords = {Blocked algorithm,Column pivoting,Low-rank approximation,QR factorization,Random sampling,Sample update,Truncated SVD},
number = {4},
pages = {C263--C291},
title = {{Randomized qr with column pivoting}},
volume = {39},
year = {2017}
}
@article{BoutsidisGittens,
author = {Boutsidis, Christos and Gittens, Alex},
doi = {10.1137/120874540},
file = {:Users/luciaminahyang/Documents/school/research/Readings/BoutsidisGittens.pdf:pdf},
issn = {0895-4798},
journal = {SIAM Journal on Matrix Analysis and Applications},
keywords = {060656127,1,10,1137,65b99,65f10,65f15,65f50,accuracy,ams subject classifications,deflation,doi,gmres,introduction,is one,iterative techniques,linear systems,method,the generalized minimum residual},
month = {jan},
number = {3},
pages = {1301--1340},
title = {{Improved Matrix Algorithms via the Subsampled Randomized Hadamard Transform}},
url = {http://epubs.siam.org/doi/10.1137/120874540},
volume = {34},
year = {2013}
}
@article{Brin1998,
abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from 3 years ago. This paper provides an in-depth description of our large-scale web search engine - the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections, where anyone can publish anything they want. ?? 2012 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Brin, S. and Page, L.},
doi = {10.1016/S0169-7552(98)00110-X},
eprint = {1111.6189v1},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Brin, Page - 1998 - The anatomy of a large-scale hypertextual Web search engine BT - Computer Networks and ISDN Systems.pdf:pdf},
isbn = {01697552},
issn = {01697552},
journal = {Computer Networks and ISDN Systems},
keywords = {google,information retrieval,pagerank,search engines,world wide web},
number = {1-7},
pages = {107--117},
pmid = {726238533241861127},
title = {{The anatomy of a large-scale hypertextual Web search engine BT - Computer Networks and ISDN Systems}},
url = {http://dx.doi.org/10.1016/S0169-7552(98)00110-X{\%}5Cnhttp://apps.webofknowledge.com/full{\_}record.do?product=UA{\&}search{\_}mode=GeneralSearch{\&}qid=6{\&}SID=X1pOWPMuSmOv1SlwJ6f{\&}page=1{\&}doc=2},
volume = {30},
year = {1998}
}
@article{Flierl1993,
abstract = {A modeling study was conducted to examine the effects of time-dependent mesoscale meandering of a jet on nutrient-phytoplankton-zooplankton dynamics. Two cases for biology were examined: 1) phytoplankton in a mixed layer of fixed depth and 2) plankton at the base of a mixed layer (pycnocline) of variable depth. When the mixed layer depth is fixed, nutrient upwelling and dilution of the phytoplankton and zooplankton populations occur along the northward branch of the meander. The additional nutrients and reduced grazing pressure leads to significant enhancement (10-20{\%}) of the phytoplankton production and biomass, while the zooplankton biomass decreased similarly. For plankton on a material surface of variable depth, phytoplankton growth in the pycnocline is increased by the higher light levels encountered during along-isopycnal upwelling. The nutrients decrease and the zooplankton mass in the pycnocline increases by a small amount downstream of the phytoplankton peak. -from Authors},
author = {Flierl, G. R. and Davis, C. S.},
doi = {10.1357/0022240933224016},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Flierl{\_}Davis{\_}JMR{\_}93.pdf:pdf},
issn = {00222402},
journal = {Journal of Marine Research},
number = {3},
pages = {529--560},
title = {{Biological effects of Gulf Stream meandering}},
volume = {51},
year = {1993}
}
@article{Julien1998,
abstract = {The incompressible Navier-Stokes equation is considered in the limit of rapid rotation (small Ekman number). The analysis is limited to horizontal scales small enough so that both horizontal and vertical velocities are comparable, but the horizontal velocity components are still in geostrophic balance. Asymptotic analysis leads to a pair of nonlinear equations for the vertical velocity and vertical vorticity coupled by vertical stretching. Statistically stationary states are maintained against viscous dissipation by boundary forcing or energy injection at larger scales. For thermal forcing direct numerical simulation of the reduced equations reveals the presence of intense vortical structures spanning the layer depth, in excellent agreement with simulations of the Boussinesq equations for rotating convection by Julien et al. (1996).},
author = {Julien, Keith and Knobloch, Edgar and Werne, Joseph},
doi = {10.1007/s001620050092},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Julien, Knobloch, Werne - 1998 - A new class of equations for rotationally constrained flows.pdf:pdf},
issn = {09354964},
journal = {Theoretical and Computational Fluid Dynamics},
number = {3-4},
pages = {251--261},
title = {{A new class of equations for rotationally constrained flows}},
volume = {11},
year = {1998}
}
@article{Blanchard2020,
author = {Blanchard, Pierre and Higham, Nicholas J. and Lopez, Florent and Mary, Theo and Pranesh, Srikara},
doi = {10.1137/19M1289546},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Pierre2019.pdf:pdf;:Users/luciaminahyang/Documents/school/research/Readings/Blanchard2020.pdf:pdf},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
keywords = {1,65f05,65g50,a new development in,ams subject classifications,floating-point arithmetic,fused multiply-add,high performance computing is,introduction,lu factorization,matrix multiplication,nvidia gpu,rounding error analysis,tensor cores,the},
month = {jan},
number = {3},
pages = {C124--C141},
title = {{Mixed Precision Block Fused Multiply-Add: Error Analysis and Application to GPU Tensor Cores}},
url = {https://epubs.siam.org/doi/10.1137/19M1289546},
volume = {42},
year = {2020}
}
@article{Klingaman2015,
abstract = {Many theories for the Madden-Julian oscillation (MJO) focus on diabatic processes, particularly the evolution of vertical heating and moistening. Poor MJO performance in weather and climate models is often blamed on biases in these processes and their interactions with the large-scale circulation. We introduce one of the three components of a model evaluation project, which aims to connect MJO fidelity in models to their representations of several physical processes, focusing on diabatic heating and moistening. This component consists of 20 day hindcasts, initialized daily during two MJO events in winter 2009–2010. The 13 models exhibit a range of skill: several have accurate forecasts to 20 days lead, while others perform similarly to statistical models (8–11 days). Models that maintain the observed MJO amplitude accurately predict propagation, but not vice versa. We find no link between hindcast fidelity and the precipitationmoisture relationship, in contrast to other recent studies. There is also no relationship between models' performance and the evolution of their diabatic heating profiles with rain rate. A more robust association emerges between models' fidelity and net moistening: the highest-skill models show a clear transition from low-level moistening for light rainfall to midlevel moistening at moderate rainfall and upper level moistening for heavy rainfall. The midlevel moistening, arising from both dynamics and physics, may be most important. Accurately representing many processes may be necessary but not sufficient for capturing the MJO, which suggests that models fail to predict the MJO for a broad range of reasons and limits the possibility of finding a panacea.},
author = {Klingaman, Nicholas P. and Woolnough, Steven J. and Jiang, Xianan and Waliser, Duane and Xavier, Prince K. and Petch, Jon and Caian, Mihaela and Hannay, Cecile and Kim, Daehyun and Ma, Hsi Yen and Merryfield, William J. and Miyakawa, Tomoki and Pritchard, Mike and Ridout, James A. and Roehrig, Romain and Shindo, Eiki and Vitart, Frederic and Wang, Hailan and Cavanaugh, Nicholas R. and Mapes, Brian E. and Shelly, Ann and Zhang, Guang J.},
doi = {10.1002/2014JD022374},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Jiang{\_}et{\_}al-2015-Journal{\_}of{\_}Geophysical{\_}Research{\%}3A{\_}Atmospheres.pdf:pdf},
issn = {21562202},
journal = {Journal of Geophysical Research},
keywords = {10.1002/2014JD022375 and Madden-Julian oscillation,general circulation model,moist convection,multiscale interaction},
number = {10},
pages = {4690--4717},
title = {{Vertical structure and physical processes of the Madden-Julian oscillation: Linking hindcast fidelity to simulated diabatic heating and moistening}},
volume = {120},
year = {2015}
}
@article{Rojas1997,
abstract = {This paper provides a detailed description of the architecture of the Z1 and Z3 computing machines that Konrad Zuse designed in Berlin between 1936 and 1941. The necessary basic information was obtained from a careful evaluation of the patent application Zuse fifed in 1941. Additional insight was gained from a software simulation of the machine's logic. The Z1 was built using purely mechanical components; the Z3 used electromechanical relays. However, both machines shared a common logical structure, and their programming model was the same. I argue that both the Z1 and the Z3 possessed features akin to those of modern computers: The memory and processor were separate units, and the processor could handle floating-point numbers and compute the four basic arithmetical operations as well as the square root of a number. The program was stored on punched tape and was read sequentially. In the last section of this paper, I put the architecture of the Z1 and Z3 into historical perspective by offering a comparison with computing machines built in other countries.},
author = {Rojas, Ra{\'{u}}l},
doi = {10.1109/85.586067},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Rojas - 1997 - Konrad Zuse's legacy The architecture of the Z1 and Z3.pdf:pdf},
issn = {10586180},
journal = {IEEE Annals of the History of Computing},
number = {2},
pages = {5--16},
title = {{Konrad Zuse's legacy: The architecture of the Z1 and Z3}},
volume = {19},
year = {1997}
}
@article{Zhou2002,
abstract = {The statistical property of a growing scale-free network is studied based on an earlier model proposed by Krapivsky, Rodgers, and Redner [Phys. Rev. Lett. 86, 5401 (2001)], with the additional constraints of forbidding self-connection and multiple links of the same direction between any two nodes. Scaling exponents in the range of [formula presented] are obtained through Monte Carlo simulations and various clustering coefficients are calculated, one of which, [formula presented] is of the order of [formula presented] indicating that the network resembles a small world. The out-degree distribution has an exponential cutoff for large out degree. {\textcopyright} 2002 The American Physical Society.},
author = {Zhou, Haijun},
doi = {10.1103/PhysRevE.66.016125},
file = {:Users/luciaminahyang/Documents/school/research/Readings/ZhouAPS.pdf:pdf},
issn = {1063651X},
journal = {Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
number = {1},
pages = {1--6},
title = {{Scaling exponents and clustering coefficients of a growing random network}},
volume = {66},
year = {2002}
}
@article{Nakatsukasa2018,
author = {Nakatsukasa, Yuji and S{\`{e}}te, Olivier and Trefethen, Lloyd N.},
doi = {10.1137/16M1106122},
file = {:Users/luciaminahyang/Documents/school/research/Readings/NakatsukasaTrefethen2018.pdf:pdf},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
keywords = {080738970,10,1137,15a83,65k05,90c25,ams subject classifications,doi,grange dual function,la-,linearized bregman iteration,matrix completion,nuclear norm minimization,s algorithm,singular value thresholding,uzawa},
month = {jan},
number = {3},
pages = {A1494--A1522},
title = {{The AAA Algorithm for Rational Approximation}},
url = {https://epubs.siam.org/doi/10.1137/16M1106122},
volume = {40},
year = {2018}
}
@article{Abbe2016,
abstract = {The stochastic block model with two communities, or equivalently the planted bisection model, is a popular model of random graph exhibiting a cluster behavior. In the symmetric case, the graph has two equally sized clusters and vertices connect with probability p within clusters and q across clusters. In the past two decades, a large body of literature in statistics and computer science has focused on providing lower bounds on the scaling of |p - q| to ensure exact recovery. In this paper, we identify a sharp threshold phenomenon for exact recovery: if $\alpha$ = pn/log(n) and $\beta$ = qn/log(n) are constant (with $\alpha${\textgreater} $\beta$), recovering the communities with high probability is possible if ($\alpha$ + $\beta$/2) - √$\alpha$$\beta$ {\textgreater} 1 and is impossible if ($\alpha$ + $\beta$/2) - √$\alpha$$\beta$ {\textless} 1. In particular, this improves the existing bounds. This also sets a new line of sight for efficient clustering algorithms. While maximum likelihood (ML) achieves the optimal threshold (by definition), it is in the worst case NP-hard. This paper proposes an efficient algorithm based on a semidefinite programming relaxation of ML, which is proved to succeed in recovering the communities close to the threshold, while numerical experiments suggest that it may achieve the threshold. An efficient algorithm that succeeds all the way down to the threshold is also obtained using a partial recovery algorithm combined with a local improvement procedure.},
archivePrefix = {arXiv},
arxivId = {1405.3267},
author = {Abbe, Emmanuel and Bandeira, Afonso S. and Hall, Georgina},
doi = {10.1109/TIT.2015.2490670},
eprint = {1405.3267},
file = {:Users/luciaminahyang/Documents/school/research/Readings/spectral graph/AbbeBandeiraHall.pdf:pdf},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Clustering algorithms,Communities,Detection algorithms,Network theory (graphs),Statistical learning},
number = {1},
pages = {471--487},
title = {{Exact recovery in the stochastic block model}},
volume = {62},
year = {2016}
}
@article{Clarkson2013,
abstract = {We design a new distribution over poly(r$\epsilon$-1) × n matrices S so that for any fixed n×d matrix A of rank r, with probability at least 9/10, SAx 2 = (1± $\epsilon$) Ax 2 simultaneously for all x ∈ Rd. Such a matrix S is called a subspace embedding. Furthermore, SA can be computed in O(nnz(A))time, where nnz(A) is the number of non-zero entries of A. This improves over all previous subspace embeddings, which required at least $\Omega$(nd log d) time to achieve this property. We call our matrices S sparse embedding matrices. Using our sparse embedding matrices, we obtain the fastest known algorithms for overconstrained least-squares regression, low-rank approximation, approximating all leverage scores, and p-regression: • to output an xfor which Ax - b 2 ≤ (1 + $\epsilon$)min x Ax - b 2 for an n × d matrix A and an n × 1 column vector b, we obtain an algorithm running in O(nnz(A)) + ̃O (d3$\epsilon$-2) time, and another in O(nnz(A) log(1/$\epsilon$)) + ̃O (d3 log(1/$\epsilon$)) time. (Here ̃O(f) = f {\textperiodcentered} logO(1)(f).) • to obtain a decomposition of an n×n matrix A into a product of an n×k matrix L, a k ×k diagonal matrix D, and a n × k matrix W, for which A - LDW F ≤ (1 + $\epsilon$) A - Ak F , where Ak is the best rank-k approximation, our algorithm runs in O(nnz(A)) +̃O(nk2$\epsilon$-4 log n+k3$\epsilon$-5 log 2 n) time. • to output an approximation to all leverage scores of an n×d input matrix A simultaneously, with constant relative error, our algorithms run in O(nnz(A) log n)+ ̃O (r3) time. • to output an x for which Ax - b p ≤ (1 + $\epsilon$)min x Ax - b p for an n × d matrix A and an n × 1 column vector b, we obtain an algorithm running in O(nnz(A) log n) + poly(r$\epsilon$-1) time, for any constant 1 ≤ p {\textless} ∞. We optimize the polynomial factors in the above stated running times, and show various tradeoffs. Finally, we provide preliminary experimental results which suggest that our algorithms are of interest in practice. Copyright 2013 ACM.},
archivePrefix = {arXiv},
arxivId = {1207.6365},
author = {Clarkson, Kenneth L. and Woodruff, David P.},
doi = {10.1145/2488608.2488620},
eprint = {1207.6365},
file = {:Users/luciaminahyang/Documents/school/research/Readings/ClarksonWoodruff.pdf:pdf},
isbn = {9781450320290},
issn = {07378017},
journal = {Proceedings of the Annual ACM Symposium on Theory of Computing},
keywords = {Algorithms,Theory},
pages = {81--90},
title = {{Low rank approximation and regression in input sparsity time}},
year = {2013}
}
@article{Al-Mohy2011,
author = {Al-Mohy, Awad H. and Higham, Nicholas J.},
doi = {10.1137/100788860},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Al-MohyHigham2010.pdf:pdf},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
keywords = {35q62,35q86,35q93,49m15,65c40,65c60,74j20,74j25,ams subject classifications,dynamics,inverse problems,langevin,low-rank hessian,mcmc,stochastic newton,uncertainty quantification},
month = {jan},
number = {2},
pages = {488--511},
title = {{Computing the Action of the Matrix Exponential, with an Application to Exponential Integrators}},
url = {http://epubs.siam.org/doi/10.1137/100788860},
volume = {33},
year = {2011}
}
@article{Zakharov2004,
abstract = {The problem of turbulence is one of the central problems in theoretical physics. While the theory of fully developed turbulence has been widely studied, the theory of wave turbulence has been less studied, partly because it developed later. Wave turbulence takes place in physical systems of nonlinear dispersive waves. In most applications nonlinearity is small and dispersive wave interactions are weak. The weak turbulence theory is a method for a statistical description of weakly nonlinear interacting waves with random phases. It is not surprising that the theory of weak wave turbulence began to develop in connection with some problems of plasma physics as well as of wind waves. The present review is restricted to one-dimensional wave turbulence, essentially because finer computational grids can be used in numerical computations. Most of the review is devoted to wave turbulence in various wave equations, and in particular in a simple one-dimensional model of wave turbulence introduced by Majda, McLaughlin and Tabak in 1997. All the considered equations are model equations, but consequences on physical systems such as ocean waves are discussed as well. The main conclusion is that the range in which the theory of pure weak turbulence is valid is narrow. In general, wave turbulence is not completely weak. Together with the weak turbulence component, it can include coherent structures, such as solitons, quasisolitons, collapses or broad collapses. As a result, weak and strong turbulence coexist. In situations where coherent structures cannot develop, weak turbulence dominates. Even though this is primarily a review paper, new results are presented as well, especially on self-organized criticality and on quasisolitonic turbulence. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Zakharov, Vladimir and Dias, Fr{\'{e}}d{\'{e}}ric and Pushkarev, Andrei},
doi = {10.1016/j.physrep.2004.04.002},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Zakharov2004.pdf:pdf},
issn = {03701573},
journal = {Physics Reports},
keywords = {Dispersive waves,Hamiltonian systems,Quasisolitons,Solitons,Wave collapse,Weak turbulence},
number = {1},
pages = {1--65},
title = {{One-dimensional wave turbulence}},
volume = {398},
year = {2004}
}
@article{Bamber1975,
abstract = {Receiver operating characteristic graphs are shown to be a variant form of ordinal dominance graphs. The area above the latter graph and the area below the former graph are useful measures of both the size or importance of a difference between two populations and/or the accuracy of discrimination performance. The usual estimator for this area is closely related to the Mann-Whitney U statistic. Statistical literature on this area estimator is reviewed. For large sample sizes, the area estimator is approximately normally distributed. Formulas for the variance and the maximum variance of the area estimator are given. Several different methods of constructing confidence intervals for the area measure are presented and the strengths and weaknesses of each of these methods are discussed. Finally, the Appendix presents the derivation of a new mathematical result, the maximum variance of the area estimator over convex ordinal dominance graphs. {\textcopyright} 1975.},
author = {Bamber, Donald},
doi = {10.1016/0022-2496(75)90001-2},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Bamber1975.pdf:pdf},
issn = {10960880},
journal = {Journal of Mathematical Psychology},
number = {4},
pages = {387--415},
title = {{The area above the ordinal dominance graph and the area below the receiver operating characteristic graph}},
volume = {12},
year = {1975}
}
@article{Nvidia2017,
abstract = {Since the introduction of the pioneering CUDA GPU Computing platform over 10 years ago, each new NVIDIA{\textregistered} GPU generation has delivered higher application performance, improved power efficiency, added important new compute features, and simplified GPU programming. Today, NVIDIA GPUs accelerate thousands of High Performance Computing (HPC), data center, and machine learning applications. NVIDIA GPUs have become the leading computational engines powering the Artificial Intelligence (AI) revolution. NVIDIA GPUs accelerate numerous deep learning systems and applications including autonomous vehicle platforms, high-accuracy speech, image, and text recognition systems, intelligent video analytics, molecular simulations, drug discovery, disease diagnosis, weather forecasting, big data analytics, financial modeling, robotics, factory automation, real-time language translation, online search optimizations, and personalized user recommendations, to name just a few. The new NVIDIA{\textregistered} Tesla{\textregistered} V100 accelerator (shown in Figure 1) incorporates the powerful new Volta™ GV100 GPU. GV100 not only builds upon the advances of its predecessor, the Pascal™ GP100 GPU, it significantly improves performance and scalability, and adds many new features that improve programmability. These advances will supercharge HPC, data center, supercomputer, and deep learning systems and applications. This white paper presents the Tesla V100 accelerator and the Volta GV100 GPU architecture.},
author = {Nvidia},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Nvidia - 2017 - Nvidia Tesla V100 GPU Architecture.pdf:pdf},
journal = {White Paper},
keywords = {Volta},
number = {v1.1},
pages = {53},
title = {{Nvidia Tesla V100 GPU Architecture}},
url = {https://images.nvidia.com/content/volta-architecture/pdf/Volta-Architecture-Whitepaper-v1.0.pdf{\%}0Ahttp://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf{\%}0Ahttp://www.nvidia.com/content/gated-pdfs/Volta-Architecture-White},
year = {2017}
}
@article{Schreiber2019,
abstract = {High-performance computing trends towards many-core systems are expected to continue over the next decade. As a result, parallel-in-time methods, mathematical formulations which exploit additional degrees of parallelism in the time dimension, have gained increasing interest in recent years. In this work we study a massively parallel rational approximation of exponential integrators (REXI). This method replaces a time integration of stiff linear oscillatory and diffusive systems by the sum of the solutions of many decoupled systems, which can be solved in parallel. Previous numerical studies showed that this reformulation allows taking arbitrarily long time steps for the linear oscillatory parts. The present work studies the non-linear shallow-water equations on the rotating sphere, a simplified system of equations used to study properties of space and time discretization methods in the context of atmospheric simulations. After introducing time integrators, we first compare the time step sizes to the errors in the simulation, discussing pros and cons of different formulations of REXI. Here, REXI already shows superior properties compared to explicit and implicit time stepping methods. Additionally, we present wallclock-time-to-error results revealing the sweet spots of REXI obtaining either an over 6 × higher accuracy within the same time frame or an about 3 × reduced time-to-solution for a similar error threshold. Our results motivate further explorations of REXI for operational weather/climate systems.},
archivePrefix = {arXiv},
arxivId = {1805.06557},
author = {Schreiber, Martin and Schaeffer, Nathana{\"{e}}l and Loft, Richard},
doi = {10.1016/j.parco.2019.01.005},
eprint = {1805.06557},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Schreiber2019.pdf:pdf},
issn = {01678191},
journal = {Parallel Computing},
keywords = {Exponential integrators,High-performance computing,Non-linear shallow-water equations,Parallel-in-time,Rational approximations,Spherical harmonics,Time splitting methods},
pages = {56--65},
publisher = {Elsevier B.V.},
title = {{Exponential integrators with parallel-in-time rational approximations for the shallow-water equations on the rotating sphere}},
url = {https://doi.org/10.1016/j.parco.2019.01.005},
volume = {85},
year = {2019}
}
@article{Isherwood2018,
abstract = {Strong stability preserving (SSP) Runge-Kutta methods are often desired when evolving in time problems that have two components that have very different time scales. Where the SSP property is needed, it has been shown that implicit and implicit-explicit methods have very restrictive time-steps and are therefore not efficient. For this reason, SSP integrating factor methods may offer an attractive alternative to traditional time-stepping methods for problems with a linear component that is stiff and a nonlinear component that is not. However, the strong stability properties of integrating factor Runge-Kutta methods have not been established. In this work we show that it is possible to define explicit integrating factor Runge-Kutta methods that preserve the desired strong stability properties satisfied by each of the two components when coupled with forward Euler time-stepping, or even given weaker conditions. We define sufficient conditions for explicit integrating factor Runge-Kutta methods to be SSP, namely, that they are based on explicit SSP Runge-Kutta methods with nondecreasing abscissas. We find such methods of up to fourth order and up to ten stages, analyze their SSP coefficients, and prove their optimality in a few cases. We test these methods to demonstrate their convergence and to show that the SSP time-step predicted by the theory is generally sharp and that the nondecreasing abscissa condition is needed in our test cases. Finally, we show that on typical total variation diminishing linear and nonlinear test cases our new explicit SSP integrating factor Runge-Kutta methods out-perform the corresponding explicit SSP Runge-Kutta methods, implicit-explicit SSP Runge-Kutta methods, and some well-known exponential time differencing methods.},
archivePrefix = {arXiv},
arxivId = {1708.02595},
author = {Isherwood, Leah and Grant, Zachary J. and Gottlieb, Sigal},
doi = {10.1137/17M1143290},
eprint = {1708.02595},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Isherwood.pdf:pdf},
issn = {00361429},
journal = {SIAM Journal on Numerical Analysis},
keywords = {Integrating factor,Runge-Kutta,Strong stability preserving},
number = {6},
pages = {3276--3307},
title = {{Strong stability preserving integrating factor Runge-Kutta methods}},
volume = {56},
year = {2018}
}
@article{Nannicini2017,
abstract = {This paper is a gentle but rigorous introduction to quantum computing intended for discrete mathematicians. Starting from a small set of assumptions on the behavior of quantum computing devices, we analyze their main characteristics, stressing the differences with classical computers, and finally describe two well-known algorithms (Simon's algorithm and Grover's algorithm) using the formalism developed in previous sections. This paper does not touch on the physics of the devices, and therefore does not require any notion of quantum mechanics. Numerical examples on an implementation of Grover's algorithm using open-source software are provided.},
archivePrefix = {arXiv},
arxivId = {1708.03684},
author = {Nannicini, Giacomo},
eprint = {1708.03684},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Nannicini - 2017 - An Introduction to Quantum Computing, Without the Physics.pdf:pdf},
title = {{An Introduction to Quantum Computing, Without the Physics}},
url = {http://arxiv.org/abs/1708.03684},
year = {2017}
}
@article{Mikhailovskii1988,
abstract = {A possibility of a realization of Kolmogorov (power-law) spectra of weak turbulence of lower-hybrid drift waves in plasmas with hot ions and for some types of electron gradient waves in inhomogeneous magnetized plasmas is shown. The turbulence due to a vector or scalar nonlinearity is discussed. {\textcopyright} 1988.},
author = {Mikhailovskii, A. B. and Nazarenko, S. V. and Novakovskii, S. V. and Churicov, A. P. and Onishchenko, O. G.},
doi = {10.1016/0375-9601(88)90925-5},
file = {:Users/luciaminahyang/Documents/school/research/Readings/plasmaweak.pdf:pdf},
issn = {03759601},
journal = {Physics Letters A},
number = {7-8},
pages = {407--409},
title = {{Kolmogorov weakly turbulent spectra of some types of drift waves in plasmas}},
volume = {133},
year = {1988}
}
@article{Bandeira2015,
author = {Bandeira, Afonso S},
file = {:Users/luciaminahyang/Documents/school/research/Readings/spectral graph/Bandeira{\_}MaxCutSBM.pdf:pdf},
number = {1},
pages = {1--8},
title = {{MAT 585 : Max-Cut and Stochastic Block Model}},
volume = {2},
year = {2015}
}
@article{Connaughton2015,
abstract = {A detailed study of the Charney-Hasegawa-Mima model and its extensions is presented. These simple nonlinear partial differential equations suggested for both Rossby waves in the atmosphere and drift waves in a magnetically-confined plasma, exhibit some remarkable and nontrivial properties, which in their qualitative form, survive in more realistic and complicated models. As such, they form a conceptual basis for understanding the turbulence and zonal flow dynamics in real plasma and geophysical systems. Two idealised scenarios of generation of zonal flows by small-scale turbulence are explored: a modulational instability and turbulent cascades. A detailed study of the generation of zonal flows by the modulational instability reveals that the dynamics of this zonal flow generation mechanism differ widely depending on the initial degree of nonlinearity. The jets in the strongly nonlinear case further roll up into vortex streets and saturate, while for the weaker nonlinearities, the growth of the unstable mode reverses and the system oscillates between a dominant jet, which is slightly inclined to the zonal direction, and a dominant primary wave. A numerical proof is provided for the extra invariant in Rossby and drift wave turbulence- zonostrophy. While the theoretical derivations of this invariant stem from the wave kinetic equation which assumes weak wave amplitudes, it is shown to be relatively well-conserved for higher nonlinearities also. Together with the energy and enstrophy, these three invariants cascade into anisotropic sectors in the k-space as predicted by the Fj{\o}rtoft argument. The cascades are characterised by the zonostrophy pushing the energy to the zonal scales. A small scale instability forcing applied to the model has demonstrated the well-known drift wave-zonal flow feedback loop. The drift wave turbulence is generated from this primary instability. The zonal flows are then excited by either one of the generation mechanisms, extracting energy from the drift waves as they grow. Eventually the turbulence is completely suppressed and the zonal flows saturate. The turbulence spectrum is shown to diffuse in a manner which has been mathematically predicted. The insights gained from this simple model could provide a basis for equivalent studies in more sophisticated plasma and geophysical fluid dynamics models in an effort to fully understand the zonal flow generation, the turbulent transport suppression and the zonal flow saturation processes in both the plasma and geophysical contexts as well as other wave and turbulence systems where order evolves from chaos.},
archivePrefix = {arXiv},
arxivId = {1407.1896},
author = {Connaughton, Colm and Nazarenko, Sergey and Quinn, Brenda},
doi = {10.1016/j.physrep.2015.10.009},
eprint = {1407.1896},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Connaughton{\_}etal{\_}PR{\_}15.pdf:pdf},
issn = {03701573},
journal = {Physics Reports},
keywords = {Drift waves,Instability forcing,Modulational instability,Nonlocal turbulence,Rossby waves,Triple cascade,Zonal flow,Zonostrophy},
pages = {1--71},
publisher = {Elsevier B.V.},
title = {{Rossby and drift wave turbulence and zonal flows: The Charney-Hasegawa-Mima model and its extensions}},
url = {http://dx.doi.org/10.1016/j.physrep.2015.10.009},
volume = {604},
year = {2015}
}
@article{Dreher2016,
abstract = {The rise of big data systems has created a need for benchmarks to measure and compare the capabilities of these systems. Big data benchmarks present unique scalability challenges. The supercomputing community has wrestled with these challenges for decades and developed methodologies for creating rigorous scalable benchmarks (e.g., HPC Challenge). The proposed PageRank pipeline benchmark employs supercomputing benchmarking methodologies to create a scalable benchmark that is reflective of many real-world big data processing systems. The PageRank pipeline benchmark builds on existing prior scalable benchmarks (Graph500, Sort, and PageRank) to create a holistic benchmark with multiple integrated kernels that can be run together or independently. Each kernel is well defined mathematically and can be implemented in any programming environment. The linear algebraic nature of PageRank makes it well suited to being implemented using the GraphBLAS standard. The computations are simple enough that performance predictions can be made based on simple computing hardware models. The surrounding kernels provide the context for each kernel that allows rigorous definition of both the input and the output for each kernel. Furthermore, since the proposed PageRank pipeline benchmark is scalable in both problem size and hardware, it can be used to measure and quantitatively compare a wide range of present day and future systems. Serial implementations in C++, Python, Python with Pandas, Matlab, Octave, and Julia have been implemented and their single threaded performance has been measured.},
author = {Dreher, Patrick and Byun, Chansup and Hill, Chris and Gadepally, Vijay and Kuszmaul, Bradley and Kepner, Jeremy},
doi = {10.1109/IPDPSW.2016.89},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Dreher.pdf:pdf},
isbn = {9781509021406},
journal = {Proceedings - 2016 IEEE 30th International Parallel and Distributed Processing Symposium, IPDPS 2016},
keywords = {Benchmarking,Big data,PageRank,Supercomputing},
pages = {929--937},
title = {{PageRank pipeline benchmark: Proposal for a holistic system benchmark for big-data platforms}},
year = {2016}
}
@article{S.1995,
abstract = {Which of the following statements is true? • Users want “black box” software that they can use with complete confidence for general problem classes without having to understand the fine algorithmic details. • Users want to be able to tune data structures for a particular application, even if the software is not as reliable as that provided for general methods. It turns out both are true, for different groups of users. Traditionally, users have asked for and been provided with black box software in the form of mathematical libraries such as LAPACK, LINPACK, NAG, and IMSL. More recently, the high performance community has discovered that they must write custom software for their problem. Their reasons include inadequate functionality of existing software libraries, data structures that are not natural or convenient for a particular problem, and overly general software that sacrifices too much performance when applied to a special case of interest. Can we meet the needs of both groups of users? We believe we can. Accordingly, in this book, we introduce the use of templates. A template is a description of a general algorithm rather than the executable object code or the source code more commonly found in a conventional software library. Nevertheless, although templates are general descriptions of key algorithms, they offer whatever degree of customization the user may desire. For example, they can be configured for the specific data structure of a problem or for the specific computing system on which the problem is to run. We focus on the use of iterative methods for solving large sparse systems of linear equations. Many methods exist for solving such problems. The trick is to find the most effective method for the problem at hand. Unfortunately, a method that works well for one problem type may not work as well for another. Indeed, it may not work at all. Thus, besides providing templates, we suggest how to choose and implement an effective method, and how to specialize a method to specific matrix types. We restrict ourselves to iter ative methods, which work by repeatedly improving an approximate solution until it is accurate enough. These methods access the coefficient matrix A of the linear system only via the matrix vector product y = A {\textperiodcentered} x (and perhaps z = A T {\textperiodcentered} x). Thus the user need only supply a subroutine for computing y (and perhaps z) given x, which permits full exploitation of the sparsity or other special structure of A. We believe that after reading this book, applications developers will be able to use templates to get their program running on a parallel machine quickly. Nonspecialists will know how to choose and implement an approach to solve a particular problem. Specialists will be able to assemble and modify their codes—without having to make the huge investment that has, up to now, been required to tune large-scale applications for each particular machine. Finally, we hope that all users will gain a better understanding of the algorithms employed. While education has not been one of 1 the traditional goals of mathematical software, we believe that our approach will go a long way in providing such a valuable service.},
author = {S., G. W. and Barrett, Richard and Berry, Michael and Chan, Tony F. and Demmel, James and Donato, June and Dongarra, Jack and Eijkhout, Victor and Pozo, Roldan and Romine, Charles and van der Vorst, Henk},
doi = {10.2307/2153507},
file = {:Users/luciaminahyang/Documents/school/research/Readings/templatesLinear.pdf:pdf},
issn = {00255718},
journal = {Mathematics of Computation},
number = {211},
pages = {1349},
title = {{Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods.}},
volume = {64},
year = {1995}
}
@article{Demmel2007,
abstract = {In Demmel et al. (Numer. Math. 106(2), 199-224, 2007) we showed that a large class of fast recursive matrix multiplication algorithms is stable in a normwise sense, and that in fact if multiplication of n-by-n matrices can be done by any algorithm in O(n $\omega$+$\eta$ ) operations for any $\eta$ {\textgreater} 0, then it can be done stably in O(n $\omega$+$\eta$ ) operations for any $\eta$ {\textgreater} 0. Here we extend this result to show that essentially all standard linear algebra operations, including LU decomposition, QR decomposition, linear equation solving, matrix inversion, solving least squares problems, (generalized) eigenvalue problems and the singular value decomposition can also be done stably (in a normwise sense) in O(n $\omega$+$\eta$ ) operations. {\textcopyright} 2007 Springer-Verlag.},
archivePrefix = {arXiv},
arxivId = {math/0612264},
author = {Demmel, James and Dumitriu, Ioana and Holtz, Olga},
doi = {10.1007/s00211-007-0114-x},
eprint = {0612264},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Demmel2007{\_}Article{\_}FastLinearAlgebraIsStable.pdf:pdf;:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Demmel, Dumitriu, Holtz - 2007 - Fast linear algebra is stable.pdf:pdf},
issn = {0029599X},
journal = {Numerische Mathematik},
number = {1},
pages = {59--91},
primaryClass = {math},
title = {{Fast linear algebra is stable}},
volume = {108},
year = {2007}
}
@article{Schroder1996,
abstract = {When a container of fluid is oscillated vertically, capillary waves develop on the surface if the amplitude exceeds a critical value. Experimentally one finds that the motion of small particles on the surface of the fluid is close to Brownian. Here we study the relative motion of particle pairs. The experiment establishes that particle motion is strongly correlated over macroscopic distances. Our observations are in striking agreement with upper-ocean studies, and with theories that appear applicable to this “weak turbulence” problem, and in disagreement with experimental and theoretical results for two-dimensional large-scale atmospheric turbulence. {\textcopyright} 1996 The American Physical Society.},
author = {Schr{\"{o}}der, Elsebeth and Andersen, Jacob Sparre and Levinsen, Mogens T. and Alstr{\o}m, Preben and Goldburg, Walter I.},
doi = {10.1103/PhysRevLett.76.4717},
file = {:Users/luciaminahyang/Documents/school/research/Readings/PhysRevLett.76.4717.pdf:pdf},
issn = {10797114},
journal = {Physical Review Letters},
number = {25},
pages = {4717--4720},
title = {{Relative particle motion in capillary waves}},
volume = {76},
year = {1996}
}
@book{Radko2012,
abstract = {Double-diffusive convection is a mixing process driven by the interaction of two fluid components which diffuse at different rates. Leading expert Timour Radko presents the first systematic overview of the classical theory of double-diffusive convection in a coherent narrative, bringing together the disparate literature in this developing field. The book begins by exploring idealized dynamical models and illustrating key principles by examples of oceanic phenomena. Building on the theory, it then explains the dynamics of structures resulting from double-diffusive instabilities, such as the little-understood phenomenon of thermohaline staircases. The book also surveys non-oceanographic applications, such as industrial, astrophysical and geological manifestations, and discusses the climatic and biological consequences of double-diffusive convection. Providing a balanced blend of fundamental theory and real-world examples, this is an indispensable resource for academic researchers, professionals and graduate students in physical oceanography, fluid dynamics, applied mathematics, astrophysics, geophysics and climatology.},
address = {Cambridge},
author = {Radko, Timour},
booktitle = {Double-Diffusive Convection},
doi = {10.1017/CBO9781139034173},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Radko - 2012 - Double-diffusive convection.pdf:pdf;:Users/luciaminahyang/Documents/school/research/Readings/Radko{\_}Double{\_}Diffusive.pdf:pdf},
isbn = {9781139034173},
pages = {1--350},
publisher = {Cambridge University Press},
title = {{Double-diffusive convection}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781139034173},
volume = {9780521880},
year = {2012}
}
@book{Series1997,
abstract = {Smith, R. K., United States. Office of Naval Research, {\&} North Atlantic Treaty Organization. Scientific Affairs Division. (1997). The physics and parameterization of moist atmospheric convection. Paper presented at the , 505;no. 505.;},
author = {Series, Nato A S I},
booktitle = {The Physics and Parameterization of Moist Atmospheric Convection},
doi = {10.1007/978-94-015-8828-7},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Smith{\_}1997(book).pdf:pdf},
isbn = {9789048149605},
title = {{The Physics and Parameterization of Moist Atmospheric Convection}},
year = {1997}
}
@article{Bischof1987,
abstract = {A new way to represent products of Householder matrices is given that makes a typical Householder matrix algorithm rich in matrix-matrix multiplication. This is very desirable in that matrix-matrix multiplication is the operation of choice for an increasing number of important high performance computers. We tested the new representation by using it to compute the QR factorization on the FPS-164/MAX. Preliminary results indicate that it is a very efficient way to organize Householder computations.},
author = {Bischof, Christian and {Van Loan}, Charles},
doi = {10.1137/0908009},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Bischof1987.pdf:pdf},
issn = {0196-5204},
journal = {SIAM Journal on Scientific and Statistical Computing},
keywords = {1,65,ams,during the past five,environments,for,has been written about,householder matrices,introduction,matrix computations in high,mos,performance,qr factorization,subject classification,supercomputing,vector parallelism,years a great deal},
number = {1},
pages = {s2--s13},
title = {{The WY Representation for Products of Householder Matrices}},
volume = {8},
year = {1987}
}
@incollection{Hochbruck2015,
abstract = {This paper contains a short course on the construction, analy-sis, and implementation of exponential integrators for time depen-dent partial differential equations. A much more detailed recent review can be found in Hochbruck and Ostermann (2010). Here, we restrict ourselves to one-step methods for autonomous prob-lems. A basic principle for the construction of exponential integra-tors is the linearization of a semilinear or a nonlinear evolution equation. We distinguish exponential Runge–Kutta methods, us-ing a fixed linearization and exponential Rosenbrock-type meth-ods, which use a continuous linearization at the current approxi-mation of the solution. We present some of the convergence results and give a proof for the simplest method, the exponential Euler method. The fact that it is possible to construct explicit exponential integrators which obey error bounds even for abstract evolution equations comes at the price that one has to approximate products of matrix functions with vectors in the spatially discrete case. For an efficient implementation one has to combine the integrator with well-chosen algorithms from numerical linear algebra. We briefly sketch Krylov subspace methods for this task.},
author = {Hochbruck, Marlis},
doi = {10.1142/9789814675772_0002},
pages = {28--49},
title = {{A Short Course on Exponential Integrators}},
year = {2015}
}
@article{Ballard2010a,
abstract = {Algorithms have two costs: arithmetic and communication. The latter represents the cost of moving data, either between levels of a memory hierarchy, or between processors over a network. Communication often dominates arithmetic and represents a rapidly increasing proportion of the total cost, so we seek algorithms that minimize communication. In $\backslash$cite{\{}BDHS10{\}} lower bounds were presented on the amount of communication required for essentially all {\$}O(n{\^{}}3){\$}-like algorithms for linear algebra, including eigenvalue problems and the SVD. Conventional algorithms, including those currently implemented in (Sca)LAPACK, perform asymptotically more communication than these lower bounds require. In this paper we present parallel and sequential eigenvalue algorithms (for pencils, nonsymmetric matrices, and symmetric matrices) and SVD algorithms that do attain these lower bounds, and analyze their convergence and communication costs.},
archivePrefix = {arXiv},
arxivId = {1011.3077},
author = {Ballard, Grey and Demmel, James and Dumitriu, Ioana},
eprint = {1011.3077},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Ballard, Demmel, Dumitriu - 2010 - Minimizing Communication for Eigenproblems and the Singular Value Decomposition.pdf:pdf},
pages = {1--43},
title = {{Minimizing Communication for Eigenproblems and the Singular Value Decomposition}},
url = {http://arxiv.org/abs/1011.3077},
year = {2010}
}
@book{Higham2002,
abstract = {Description Accuracy and Stability of Numerical Algorithms gives a thorough, up-to-date treatment of the behavior of numerical algorithms in finite precision arithmetic. It combines algorithmic derivations, perturbation theory, and rounding error analysis, all enlivened by historical perspective and informative quotations. This second edition expands and updates the coverage of the first edition (1996) and includes numerous improvements to the original material. Two new chapters treat symmetric indefinite systems and skew-symmetric systems, and nonlinear systems and Newton's method. Twelve new sections include coverage of additional error bounds for Gaussian elimination, rank revealing LU factorizations, weighted and constrained least squares problems, and the fused multiply-add operation found on some modern computer architectures. An expanded treatment of Gaussian elimination incorporates rook pivoting, along with a thorough discussion of the choice of pivoting strategy and the effects of scaling. The book's detailed descriptions of floating point arithmetic and of software issues reflect the fact that IEEE arithmetic is now ubiquitous. Although not designed specifically as a textbook, this new edition is a suitable reference for an advanced course. It can also be used by instructors at all levels as a supplementary text from which to draw examples, historical perspective, statements of results, and exercises. With its thorough indexes and extensive, up-to-date bibliography, the book provides a mine of information in a readily accessible form.},
author = {Higham, Nicholas J.},
booktitle = {Accuracy and Stability of Numerical Algorithms},
doi = {10.2307/2669725},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Higham - 2002 - Accuracy and Stability of Numerical Algorithms.pdf:pdf;:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Higham - 2002 - Accuracy and Stability of Numerical Methods(2).pdf:pdf},
isbn = {0898715210},
issn = {01621459},
pages = {1--656},
title = {{Accuracy and Stability of Numerical Methods}},
year = {2002}
}
@article{Bathe1973,
abstract = {A survey of probably the most efficient solution methods currently in use for the problems Kϕ = $\omega$2Mϕ and K$\Psi$ = $\lambda$KG$\Psi$ is presented. In the eigenvalue problems the stiffness matrices K and KG and the mass matrix M can be full or banded; the mass matrix can be diagonal with zero diagonal elements. The choice is between the well‐known QR method, a generalized Jacobi iteration, a new determinant search technique and an automated sub‐space iteration. The system size, the bandwidth and the number of required eigenvalues and eigenvectors determine which method should be used on a particular problem. The numerical advantages of each solution technique, operation counts and storage requirements are given to establish guidelines for the selection of the appropriate algorithm. A large number of typical solution times are presented. Copyright {\textcopyright} 1973 John Wiley {\&} Sons Ltd.},
author = {Bathe, Klaus‐J{\"{u}}rgen ‐J and Wilson, Edward L.},
doi = {10.1002/nme.1620060207},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Bathe, Wilson - 1973 - Solution methods for eigenvalue problems in structural mechanics.pdf:pdf},
issn = {10970207},
journal = {International Journal for Numerical Methods in Engineering},
number = {2},
pages = {213--226},
title = {{Solution methods for eigenvalue problems in structural mechanics}},
volume = {6},
year = {1973}
}
@article{Fairbanks2016,
abstract = {Spectral partitioning (clustering) algorithms use eigenvectors to solve network analysis problems. The relationship between numerical accuracy and network mining quality is insufficiently understood. We show that analyzing numerical accuracy and network mining quality together leads to an algorithmic improvement. Specifically, we study spectral partitioning using sweep cuts of approximate eigenvectors of the normalized graph Laplacian. We introduce a novel, theoretically sound, parameter free stopping criterion for iterative eigensolvers designed for graph partitioning. On a corpus of social networks, we validate this stopping criterion by showing the number of iterations is reduced by a factor of 4.15 on average, and the conductance is increased by only a factor of 1.24 on average. Regression analysis of these results shows that the decrease in the number of iterations needed is greater for problems with a small spectral gap, thus our stopping criterion helps more on harder problems. Experiments show that alternative stopping criteria are insufficient to ensure low conductance partitioning on real world networks. While our method guarantees partitions that satisfy the Cheeger Inequality, we find that it typically beats this guarantee on real world graphs.},
author = {Fairbanks, James P. and Zakrzewska, Anita and Bader, David A.},
doi = {10.1109/ASONAM.2016.7752209},
file = {:Users/luciaminahyang/Documents/school/research/Readings/spectral graph/FairbanksZakrzewskaBader.pdf:pdf},
isbn = {9781509028467},
journal = {Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2016},
number = {Theorem 2},
pages = {25--32},
publisher = {IEEE},
title = {{New stopping criteria for spectral partitioning}},
year = {2016}
}
@article{Ipsen2019a,
abstract = {Probabilistic models are proposed for bounding the forward error in the numerically computed inner product (dot product, scalar product) between of two real {\$}n{\$}-vectors. We derive probabilistic perturbation bounds, as well as probabilistic roundoff error bounds for the sequential accumulation of the inner product. These bounds are non-asymptotic, explicit, and make minimal assumptions on perturbations and roundoffs. The perturbations are represented as independent, bounded, zero-mean random variables, and the probabilistic perturbation bound is based on Azuma's inequality. The roundoffs are also represented as bounded, zero-mean random variables. The first probabilistic bound assumes that the roundoffs are independent, while the second one does not. For the latter, we construct a Martingale that mirrors the sequential order of computations. Numerical experiments confirm that our bounds are more informative, often by several orders of magnitude, than traditional deterministic bounds -- even for small vector dimensions{\~{}}{\$}n{\$} and very stringent success probabilities. In particular the probabilistic roundoff error bounds are functions of {\$}\backslashsqrt{\{}n{\}}{\$} rather than{\~{}}{\$}n{\$}, thus giving a quantitative confirmation of Wilkinson's intuition. The paper concludes with a critical assessment of the probabilistic approach.},
archivePrefix = {arXiv},
arxivId = {1906.10465},
author = {Ipsen, Ilse C. F. and Zhou, Hua},
eprint = {1906.10465},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Ipsen, Zhou - 2019 - Probabilistic Error Analysis for Inner Products.pdf:pdf},
keywords = {perturbation bounds,random variables,roundo ff errors,sums of random variables},
pages = {1--22},
title = {{Probabilistic Error Analysis for Inner Products}},
url = {http://arxiv.org/abs/1906.10465},
year = {2019}
}
@article{Halko2011b,
abstract = {Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed-either explicitly or implicitly-to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, robustness, and/or speed. These claims are supported by extensive numerical experiments and a detailed error analysis. The specific benefits of randomized techniques depend on the computational environment. Consider the model problem of finding the k dominant components of the singular value decomposition of an m × n matrix. (i) For a dense input matrix, randomized algorithms require O(mn log(k)) floating-point operations (flops) in contrast to O(mnk) for classical algorithms. (ii) For a sparse input matrix, the flop count matches classical Krylov subspace methods, but the randomized approach is more robust and can easily be reorganized to exploit multiprocessor architectures. (iii) For a matrix that is too large to fit in fast memory, the randomized techniques require only a constant number of passes over the data, as opposed to O(k) passes for classical algorithms. In fact, it is sometimes possible to perform matrix approximation with a single pass over the data. {\textcopyright} 2011 Society for Industrial and Applied Mathematics.},
archivePrefix = {arXiv},
arxivId = {0909.4061},
author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
doi = {10.1137/090771806},
eprint = {0909.4061},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Halko, Martinsson, Tropp - 2011 - Finding structure with randomness Probabilistic algorithms for constructing approximate matrix decompo.pdf:pdf},
issn = {00361445},
journal = {SIAM Review},
keywords = {Dimension reduction,Eigenvalue decomposition,Interpolative decomposition,Johnson-Lindenstrauss lemma,Matrix approximation,Parallel algorithm,Pass-efficient algorithm,Principal component analysis,Random matrix,Randomized algorithm},
number = {2},
pages = {217--288},
title = {{Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions}},
volume = {53},
year = {2011}
}
@article{Lambaerts2011,
abstract = {We derive a two-layer rotating shallow-water model for a moist atmosphere with water vapor condensation and related diabatic heating. Moist convection is represented by additional mass exchanges between the layers, which are determined from the moist enthalpy conservation principle, and related to the precipitation. Various boundary conditions at the lower and upper boundaries may be used. We show that the model reproduces in appropriate limits the main simplified models used previously in the literature for description of the large-scale moist-convective dynamics and precipitation fronts, namely the linear two-layer baroclinic models, the nonlinear two-layer quasigeostrophic model, and the nonlinear one-layer moist-convective rotating shallow-water model. We study the properties of the equations of the model, with special attention to the hyperbolicity loss, which is inherent to multilayer shallow-water models, and to the front propagation. Numerical illustrations of these properties are given with the help of a recently proposed high-resolution finite-volume numerical scheme with precipitation sources/sinks. {\textcopyright} 2011 American Institute of Physics.},
author = {Lambaerts, J. and Lapeyre, G. and Zeitlin, V. and Bouchut, F.},
doi = {10.1063/1.3582356},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Lambaerts{\_}etal{\_}PoF{\_}11.pdf:pdf},
issn = {10706631},
journal = {Physics of Fluids},
number = {4},
pages = {1--24},
title = {{Simplified two-layer models of precipitating atmosphere and their properties}},
volume = {23},
year = {2011}
}
@article{Martinsson2020a,
abstract = {This survey describes probabilistic algorithms for linear algebra computations, such as factorizing matrices and solving linear systems. It focuses on techniques that have a proven track record for real-world problem instances. The paper treats both the theoretical foundations of the subject and the practical computational issues. Topics covered include norm estimation; matrix approximation by sampling; structured and unstructured random embeddings; linear regression problems; low-rank approximation; subspace iteration and Krylov methods; error estimation and adaptivity; interpolatory and CUR factorizations; Nystr$\backslash$"om approximation of positive-semidefinite matrices; single view ("streaming") algorithms; full rank-revealing factorizations; solvers for linear systems; and approximation of kernel matrices that arise in machine learning and in scientific computing.},
archivePrefix = {arXiv},
arxivId = {2002.01387},
author = {Martinsson, Per-Gunnar and Tropp, Joel},
eprint = {2002.01387},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Martinsson, Tropp - 2020 - Randomized Numerical Linear Algebra Foundations {\&} Algorithms.pdf:pdf},
title = {{Randomized Numerical Linear Algebra: Foundations {\&} Algorithms}},
url = {http://arxiv.org/abs/2002.01387},
year = {2020}
}
@article{Garcia2010,
abstract = {A numerical study of several time integration methods for solving the three-dimensional Boussinesq thermal convection equations in rotating spherical shells is presented. Implicit and semi-implicit time integration techniques based on backward differentiation and extrapolation formulae are considered. The use of Krylov techniques allows the implicit treatment of the Coriolis term with low storage requirements. The codes are validated with a known benchmark, and their efficiency is studied. The results show that the use of high-order methods, especially those with time step and order control, increase the efficiency of the time integration, and allows to obtain more accurate solutions. {\textcopyright} 2010 Elsevier Inc.},
author = {Garcia, F. and Net, M. and Garc{\'{i}}a-Archilla, B. and S{\'{a}}nchez, J.},
doi = {10.1016/j.jcp.2010.07.004},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Garcia2010.pdf:pdf},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {Backward differentiation-extrapolation formulae,Krylov methods,Spectral methods,Spherical shells,Thermal convection,Time integration methods},
number = {20},
pages = {7997--8010},
publisher = {Elsevier Inc.},
title = {{A comparison of high-order time integrators for thermal convection in rotating spherical shells}},
url = {http://dx.doi.org/10.1016/j.jcp.2010.07.004},
volume = {229},
year = {2010}
}
@article{Ahfock2017,
abstract = {Sketching is a probabilistic data compression technique that has been largely developed in the computer science community. Numerical operations on big datasets can be intolerably slow; sketching algorithms address this issue by generating a smaller surrogate dataset. Typically, inference proceeds on the compressed dataset. Sketching algorithms generally use random projections to compress the original dataset and this stochastic generation process makes them amenable to statistical analysis. We argue that the sketched data can be modelled as a random sample, thus placing this family of data compression methods firmly within an inferential framework. In particular, we focus on the Gaussian, Hadamard and Clarkson-Woodruff sketches, and their use in single pass sketching algorithms for linear regression with huge {\$}n{\$}. We explore the statistical properties of sketched regression algorithms and derive new distributional results for a large class of sketched estimators. A key result is a conditional central limit theorem for data oblivious sketches. An important finding is that the best choice of sketching algorithm in terms of mean square error is related to the signal to noise ratio in the source dataset. Finally, we demonstrate the theory and the limits of its applicability on two real datasets.},
archivePrefix = {arXiv},
arxivId = {1706.03665},
author = {Ahfock, Daniel and Astle, William J. and Richardson, Sylvia},
eprint = {1706.03665},
file = {:Users/luciaminahyang/Documents/school/research/Readings/AhfockArXiv.pdf:pdf},
month = {jun},
pages = {1--58},
title = {{Statistical properties of sketching algorithms}},
url = {http://arxiv.org/abs/1706.03665},
year = {2017}
}
@article{Bouchut2009,
abstract = {We describe a shallow-water type atmospheric model which includes the transport of moisture as well as related precipitation and convection effects. The model combines hydrodynamic nonlinearity of the standard shallow-water model with the intrinsic nonlinearity due to the precipitation threshold. It allows for both theoretical treatment by the method of characteristics and efficient numerical resolution using shock-capturing finite-volume schemes. Linearized in the dynamical sector, the model adequately reproduces the propagation of the edge of precipitation regions (precipitation fronts) found in earlier studies. Results of numerical experiments on simple wave scattering upon a moisture front are in agreement with analytical results and highlight the role of dissipative reflector played by precipitating zones. We also analyze the evolution of a disturbance propagating in a uniformly saturated region and obtain criteria for precipitation front formation. Finally, we simulate wave breaking as an example of essentially nonlinear phenomenon and show how moist effects modify the classical shock formation scenario. {\textcopyright} 2009 American Institute of Physics.},
author = {Bouchut, Fran{\c{c}}ois and Lambaerts, Julien and Lapeyre, Guillaume and Zeitlin, Vladimir},
doi = {10.1063/1.3265970},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Bouchout.pdf:pdf},
issn = {10706631},
journal = {Physics of Fluids},
number = {11},
pages = {1--19},
title = {{Fronts and nonlinear waves in a simplified shallow-water model of the atmosphere with moisture and convection}},
volume = {21},
year = {2009}
}
@article{Zakharov1971,
author = {Zakharov, V. E. and Filonenko, N. N.},
doi = {10.1007/BF00915178},
file = {:Users/luciaminahyang/Documents/school/research/Readings/ZakharovFilonenko.pdf:pdf},
issn = {0021-8944},
journal = {Journal of Applied Mechanics and Technical Physics},
number = {5},
pages = {37--40},
title = {{Weak turbulence of capillary waves}},
url = {http://link.springer.com/10.1007/BF00915178},
volume = {8},
year = {1971}
}
@article{Chikira2013,
abstract = {The eastward-propagating intraseasonal oscillation represented by the Chikira-Sugiyama cumulus parameterization in an atmospheric general circulation model is compared with observations and reanalyses. The scheme is characterized by state-dependent entrainment rates that vertically vary affected by the environment. The zonal wavenumber-frequency power spectrum shows a strong signal corresponding to the Madden-Julian oscillation. The eastward-propagating feature of the convective region and accompanying anomalous zonal wind structure is well extracted by the first and second modes of the combined empirical orthogonal function (CEOF) with a reasonable explained variance for the first mode, though the second mode is not sufficiently reproduced. The basic features of the composited anomalous fields including moisture, temperature, and vertical and zonal winds resemble those of the reanalysis in both the free troposphere and surface air. The anomalous free-tropospheric moisture exhibits its westward tilt and the peak moist static energy of the surface air is shifted eastward as in the reanalysis. The anomalous low-level zonal wind changes its direction to the east of the convective center. The model's outstanding deficiencies include the weak convective activity over the Indian Ocean, weaker westward tilt, and seemingly underestimated shallow convection. {\textcopyright} 2013 American Meteorological Society.},
author = {Chikira, Minoru and Sugiyama, Masahiro},
doi = {10.1175/JAS-D-13-034.1},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Sugiyama-jas-d-13-034.1.pdf:pdf},
issn = {15200469},
journal = {Journal of the Atmospheric Sciences},
number = {12},
pages = {3920--3939},
title = {{Eastward-propagating intraseasonal oscillation represented by chikira-sugiyama cumulus parameterization. Part I: Comparison with observation and reanalysis}},
volume = {70},
year = {2013}
}
@article{HighamPranesh2019b,
author = {Higham, Nicholas J. and Pranesh, Srikara},
doi = {10.1137/19M1251308},
file = {:Users/luciaminahyang/Documents/school/research/Readings/HighamPranesh2019b.pdf:pdf},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
keywords = {10,1137,15b05,18m1205406,35r11,65f08,65f10,ams subject classifications,doi,krylov subspace method,multilevel toeplitz matrix,preconditioning,symmetrization,toeplitz matrix},
month = {jan},
number = {5},
pages = {C585--C602},
title = {{Simulating Low Precision Floating-Point Arithmetic}},
url = {https://epubs.siam.org/doi/10.1137/19M1251308},
volume = {41},
year = {2019}
}
@article{Demmel2012,
abstract = {We present parallel and sequential dense QR factorization algorithms that are both optimal (up to polylogarithmic factors) in the amount of communication they perform and just as stable as Householder QR. We prove optimality by deriving new lower bounds for the number of multiplications done by "non-Strassen-like" QR, and using these in known communication lower bounds that are proportional to the number of multiplications. We not only show that our QR algorithms attain these lower bounds (up to polylogarithmic factors), but that existing LAPACK and ScaLAPACK algorithms perform asymptotically more communication. We derive analogous communication lower bounds for LU factorization and point out recent LU algorithms in the literature that attain at least some of these lower bounds. The sequential and parallel QR algorithms for tall and skinny matrices lead to significant speedups in practice over some of the existing algorithms, including LAPACK and ScaLAPACK, for example, up to 6.7 times over ScaLAPACK. A performance model for the parallel algorithm for general rectangular matrices predicts significant speedups over ScaLAPACK. {\textcopyright} 2012 Society for Industrial and Applied Mathematics.},
archivePrefix = {arXiv},
arxivId = {0808.2664},
author = {Demmel, James and Grigori, Laura and Hoemmen, Mark and Langou, Julien},
doi = {10.1137/080731992},
eprint = {0808.2664},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Demmel et al. - 2012 - Communication-optimal parallel and sequential QR and LU factorizations.pdf:pdf},
issn = {10648275},
journal = {SIAM Journal on Scientific Computing},
keywords = {LU factorization,Linear algebra,QR factorization},
number = {1},
title = {{Communication-optimal parallel and sequential QR and LU factorizations}},
volume = {34},
year = {2012}
}
@article{Huang2009,
abstract = {Spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis, image processing and data mining. However, the computational and/or communication resources required by the method in processing large-scale data are often prohibitively high, and practitioners are often required to perturb the original data in various ways (quantization, downsampling, etc) before invoking a spectral algorithm. In this paper, we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering. We show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the Laplacian matrix. From this result we derive approximate upper bounds on the clustering error. We show that this bound is tight empirically across a wide range of problems, suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a specification of permitted loss in clustering performance.},
author = {Huang, Ling and Yan, Donghui and Jordan, Michael I. and Taft, Nina},
file = {:Users/luciaminahyang/Documents/school/research/Readings/spectral graph/perturbedspectral.pdf:pdf},
isbn = {9781605609492},
journal = {Advances in Neural Information Processing Systems 21 - Proceedings of the 2008 Conference},
pages = {705--712},
title = {{Spectral clustering with perturbed data}},
year = {2009}
}
@article{HighamPranesh2019a,
author = {Higham, Nicholas J. and Pranesh, Srikara and Zounon, Mawussi},
doi = {10.1137/18M1229511},
file = {:Users/luciaminahyang/Documents/school/research/Readings/HighamPranesh2019a.pdf:pdf},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
month = {jan},
number = {4},
pages = {A2536--A2551},
title = {{Squeezing a Matrix into Half Precision, with an Application to Solving Linear Systems}},
url = {https://epubs.siam.org/doi/10.1137/18M1229511},
volume = {41},
year = {2019}
}
@article{Grooms2011a,
abstract = {The linear stability of IMEX (IMplicit-EXplicit) methods and exponential integrators for stiff systems of ODEs arising in the discrete solution of PDEs is examined for nonlinear PDEs with both linear dispersion and dissipation, and a clear method of visualization of the linear stability regions is proposed. Predictions are made based on these visualizations and are supported by a series of experiments on five PDEs including quasigeostrophic equations and stratified Boussinesq equations. The experiments, involving 24 IMEX and exponential methods of third and fourth order, confirm the predictions of the linear stability analysis, that the methods are typically limited by small eigenvalues of the linear term and by eigenvalues on or near the imaginary axis rather than by large eigenvalues near the negative real axis. The experiments also demonstrate that IMEX methods achieve comparable stability to exponential methods, and that exponential methods are significantly more accurate only when the problem is nearly linear. Novel IMEX predictor-corrector methods are also derived. {\textcopyright} 2011 Elsevier Inc.},
author = {Grooms, Ian and Julien, Keith},
doi = {10.1016/j.jcp.2011.02.007},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Grooms, Julien - 2011 - Linearly implicit methods for nonlinear PDEs with linear dispersion and dissipation.pdf:pdf},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {Exponential integrators,IMEX,Semi-implicit,Stability},
number = {9},
pages = {3630--3650},
title = {{Linearly implicit methods for nonlinear PDEs with linear dispersion and dissipation}},
volume = {230},
year = {2011}
}
@article{Arithmetic,
abstract = {Numerical analysis is concerned with how to solve a problem numerically, i.e., how to develop a sequence of numerical calculations to get a satisfactory answer. Part of this process is the consideration of the errors that arise in these calculations, from the errors in the arithmetic operations or from other sources.},
author = {Arithmetic, Computer and Arithmetic, Computer},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Arithmetic, Arithmetic - Unknown - {\&}gt 2. Error and Computer Arithmetic.pdf:pdf},
title = {{{\textgreater} 2. Error and Computer Arithmetic}},
url = {http://www.math.pitt.edu/{~}trenchea/math1070/MATH1070{\_}2{\_}Error{\_}and{\_}Computer{\_}Arithmetic.pdf}
}
@article{Spielman2019,
author = {Spielman, Daniel A},
file = {:Users/luciaminahyang/Documents/school/research/Readings/spectral graph/Spielman.pdf:pdf},
title = {{Spectral and Algebraic Graph Theory Incomplete Draft, dated December 4, 2019}},
year = {2019}
}
@article{Friedli1978,
author = {Friedli, Armin},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Friedl1978.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Numerical treatment of differential equations},
pages = {35--50},
title = {{Verallgemeinerte Runge–Kutta Verfahren zur L{\"{o}}sung steifer Differentialgleichungssysteme.}},
volume = {631},
year = {1978}
}
@book{Horn1985,
abstract = {Linear algebra and matrix theory have long been fundamental tools in mathematical disciplines as well as fertile fields for research. In this book the authors present classical and recent results of matrix analysis that have proved to be important to applied mathematics. Facts about matrices, beyond those found in an elementary linear algebra course, are needed to understand virtually any area of mathematical science, but the necessary material has appeared only sporadically in the literature and in university curricula. As interest in applied mathematics has grown, the need for a text and reference offering a broad selection of topics in matrix theory has become apparent, and this book meets that need. This volume reflects two concurrent views of matrix analysis. First, it encompasses topics in linear algebra that have arisen out of the needs of mathematical analysis. Second, it is an approach to real and complex linear algebraic problems that does not hesitate to use notions from analysis. Review and miscellanea -- Eigenvalues, eigenvectors, and similarity.},
author = {Horn, Roger A. and Johnson, Charles R.},
booktitle = {Matrix Analysis},
doi = {10.1017/cbo9780511810817},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Horn, Johnson - 1985 - Matrix Analysis.pdf:pdf},
isbn = {9780521548236},
title = {{Matrix Analysis}},
year = {1985}
}
@article{Mezzadri2006a,
abstract = {We discuss how to generate random unitary matrices from the classical compact groups U(N), O(N) and USp(N) with probability distributions given by the respective invariant measures. The algorithm is straightforward to implement using standard linear algebra packages. This approach extends to the Dyson circular ensembles too. This article is based on a lecture given by the author at the summer school on Number Theory and Random Matrix Theory held at the University of Rochester in June 2006. The exposition is addressed to a general mathematical audience.},
archivePrefix = {arXiv},
arxivId = {math-ph/0609050},
author = {Mezzadri, Francesco},
eprint = {0609050},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Mezzadri - 2006 - How to generate random matrices from the classical compact groups.pdf:pdf},
issn = {1088-9477},
primaryClass = {math-ph},
title = {{How to generate random matrices from the classical compact groups}},
url = {http://arxiv.org/abs/math-ph/0609050},
year = {2006}
}
@article{Woodruff2014,
abstract = {This survey highlights the recent advances in algorithms for numerical linear algebra that have come from the technique of linear sketching, whereby given a matrix, one first compresses it to a much smaller matrix by multiplying it by a (usually) random matrix with certain properties. Much of the expensive computation can then be performed on the smaller matrix, thereby accelerating the solution for the original problem. In this survey we consider least squares as well as robust regression problems, low rank approximation, and graph sparsification. We also discuss a number of variants of these problems. Finally, we discuss the limitations of sketching methods.},
archivePrefix = {arXiv},
arxivId = {1411.4357},
author = {Woodruff, David P.},
doi = {10.1561/0400000060},
eprint = {1411.4357},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Woodruff.pdf:pdf},
issn = {15513068},
journal = {Foundations and Trends in Theoretical Computer Science},
number = {1-2},
pages = {1--157},
title = {{Sketching as a tool for numerical linear algebra}},
volume = {10},
year = {2014}
}
@book{Jacobs2010,
abstract = {Stochastic processes are an essential part of numerous branches of physics, as well as in biology, chemistry, and finance. This textbook provides a solid understanding of stochastic processes and stochastic calculus in physics, without the need for measure theory. In avoiding measure theory, this textbook gives readers the tools necessary to use stochastic methods in research with a minimum of mathematical background. Coverage of the more exotic Levy processes is included, as is a concise account of numerical methods for simulating stochastic systems driven by Gaussian noise. The book concludes with a non-technical introduction to the concepts and jargon of measure-theoretic probability theory. With over 70 exercises, this textbook is an easily accessible introduction to stochastic processes and their applications, as well as methods for numerical simulation, for graduate students and researchers in physics.},
author = {Jacobs, Kurt},
booktitle = {Stochastic Processes for Physicists: Understanding Noisy Systems},
doi = {10.1017/CBO9780511815980},
file = {:Users/luciaminahyang/Documents/school/research/Readings/StochProc4Phys{\_}Jacobs.pdf:pdf},
isbn = {9780511815980},
pages = {1--188},
title = {{Stochastic processes for physicists: Understanding noisy systems}},
year = {2010}
}
@article{Xie2017,
abstract = {A simple model of nonlinear salt-finger convection in two dimensions is derived and studied. The model is valid in the limit of a small solute to heat diffusivity ratio and a large density ratio, which is relevant to both oceanographic and astrophysical applications. Two limits distinguished by the magnitude of the Schmidt number are found. For order one Schmidt numbers, appropriate for astrophysical applications, a modified Rayleigh-B{\'{e}}nard system with large-scale damping due to a stabilizing temperature is obtained. For large Schmidt numbers, appropriate for the oceanic setting, the model combines a prognostic equation for the solute field and a diagnostic equation for inertia-free momentum dynamics. Two distinct saturation regimes are identified for the second model: The weakly driven regime is characterized by a large-scale flow associated with a balance between advection and linear instability, while the strongly-driven regime produces multiscale structures, resulting in a balance between energy input through linear instability and energy transfer between scales. For both regimes, we analytically predict and numerically confirm the dependence of the kinetic energy and salinity fluxes on the ratio between solutal and thermal Rayleigh numbers. The spectra and probability density functions are also computed.},
author = {Xie, Jin Han and Miquel, Benjamin and Julien, Keith and Knobloch, Edgar},
doi = {10.3390/fluids2010006},
file = {:Users/luciaminahyang/Documents/school/research/Readings/XieMiquel.pdf:pdf},
issn = {23115521},
journal = {Fluids},
keywords = {Asymptotic expansion,Reduced model,Salt-finger convection,Turbulence},
number = {1},
pages = {1--26},
title = {{A reduced model for salt-finger convection in the small diffusivity ratio limit}},
volume = {2},
year = {2017}
}
@article{Berrut2004,
abstract = {Barycentric interpolation is a variant of Lagrange polynomial interpolation that is fast and stable. It deserves to be known as the standard method of polynomial interpolation.},
author = {Berrut, Jean Paul and Trefethen, Lloyd N.},
doi = {10.1137/S0036144502417715},
file = {:Users/luciaminahyang/Documents/school/research/Readings/BerrutTrefethen2004.pdf:pdf},
issn = {00361445},
journal = {SIAM Review},
keywords = {Barycentric formula,Interpolation},
number = {3},
pages = {501--517},
title = {{Barycentric Lagrange interpolation}},
volume = {46},
year = {2004}
}
@article{Nakatsukasa2019,
abstract = {Rational minimax approximation of real functions on real intervals is an established topic, but when it comes to complex functions or domains, there appear to be no algorithms currently in use. Such a method is introduced here, the {\{}$\backslash$em AAA-Lawson algorithm,{\}} available in Chebfun. The new algorithm solves a wide range of problems on arbitrary domains in a fraction of a second of laptop time by a procedure consisting of two steps. First, the standard AAA algorithm is run to obtain a near-best approximation and a set of support points for a barycentric representation of the rational approximant. Then a "Lawson phase" of iteratively reweighted least-squares adjustment of the barycentric coefficients is carried out to improve the approximation to minimax.},
archivePrefix = {arXiv},
arxivId = {1908.06001},
author = {Nakatsukasa, Yuji and Trefethen, Lloyd N.},
eprint = {1908.06001},
file = {:Users/luciaminahyang/Documents/school/research/Readings/NakatsukasaTrefethen2019.pdf:pdf},
keywords = {41a20,65d15,aaa algorithm,aaa-lawson algo-,ams subject classifications,barycentric formula,iteratively reweighted least-squares,rational approximation,rithm},
title = {{An algorithm for real and complex rational minimax approximation}},
url = {http://arxiv.org/abs/1908.06001},
year = {2019}
}
@article{Fairbanks2017,
abstract = {Many common methods for data analysis rely on linear algebra. We provide new results connecting data analysis error to numerical accuracy in the context of spectral graph partitioning. We provide pointwise convergence guarantees so that spectral blends (linear combinations of eigenvectors) can be employed to solve data analysis problems with confidence in their accuracy.We apply this theory to an accessible model problem, the ring of cliques, by deriving the relevant eigenpairs and finding necessary and sufficient solver tolerances. Analysis of the ring of cliques provides an upper bound on eigensolver tolerances for graph partitioning problems. These results bridge the gap between linear algebra based data analysis methods and the convergence theory of iterative approximation methods. These results explain how the combinatorial structure of a problem can be recovered much faster than numerically accurate solutions to the associated linear algebra problem.},
author = {Fairbanks, James P. and Bader, David A. and Sanders, Geoffrey D.},
doi = {10.1093/comnet/cnw033},
file = {:Users/luciaminahyang/Documents/school/research/Readings/spectral graph/FairbanksBaderSanders.pdf:pdf},
issn = {20511329},
journal = {Journal of Complex Networks},
keywords = {Approximation algorithms,Community detection,Data analysis,Data mining,Eigenvalues and eigenfunctions,Graph partitioning,Iterative methods,Laplace equations,Partitioning algorithms},
number = {4},
pages = {551--580},
title = {{Spectral partitioning with blends of eigenvectors}},
volume = {5},
year = {2017}
}
@article{Tensors2008b,
author = {Tensors, Structured},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Tensors - 2008 - Copyright {\textcopyright} by SIAM . Unauthorized reproduction of this article is prohibited (3).pdf:pdf},
journal = {Society},
keywords = {060655936,1,10,1137,15a18,15a21,15a69,5,ams subject classifications,decomposition is now mature,doi,fast algorithms,introduction,multilinear svd,structured and unstructured tensors,the subject of multilinear},
number = {3},
pages = {1008--1021},
title = {{Copyright {\textcopyright} by SIAM . Unauthorized reproduction of this article is prohibited .}},
volume = {30},
year = {2008}
}
@book{Higham2008,
abstract = {A thorough and elegant treatment of the theory of matrix functions and numerical methods for computing them, including an overview of applications, new and unpublished research results, and improved algorithms. Key features include a detailed treatment of the matrix sign function and matrix roots; a development of the theory of conditioning and properties of the Fre;chet derivative; Schur decomposition; block Parlett recurrence; a thorough analysis of the accuracy, stability, and computational cost of numerical methods; general results on convergence and stability of matrix iterations; and a chapter devoted to the f(A)b problem. Ideal for advanced courses and for self-study, its broad content, references and appendix also make this book a convenient general reference. Contains an extensive collection of problems with solutions and MATLAB implementations of key algorithms. 1. Theory of matrix functions -- 2. Applications -- 3. Conditioning -- 4. Techniques for general functions -- 5. Matrix sign function -- 6. Matrix square root -- 7. Matrix pth root -- 8. The polar decomposition -- 9. Schur-Parlett algorithm -- 10. Matrix exponential -- 11. Matrix logarithm -- 12. Matrix cosine and sine -- 13. Function of matrix times vector : f (A)b -- 14. Miscellany.},
author = {Higham, Nicholas J.},
booktitle = {Mathematica},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Higham{\_}FunctionsofMatrices/Higham{\_}FunctionsofMatrices.pdf:pdf},
isbn = {9780898716467},
keywords = {matrix exponential},
mendeley-tags = {matrix exponential},
title = {{Functions of Matrices Functions of Matrices Theory and Computation}},
year = {2008}
}
@article{Dunton2019,
abstract = {The future of high-performance computing, specifically on future Exascale computers, will presumably see memory capacity and bandwidth fail to keep pace with data generated, for instance, from massively parallel partial differential equation (PDE) systems. Current strategies proposed to address this bottleneck entail the omission of large fractions of data, as well as the incorporation of {\$}\backslashtextit{\{}in situ{\}}{\$} compression algorithms to avoid overuse of memory. To ensure that post-processing operations are successful, this must be done in a way that a sufficiently accurate representation of the solution is stored. Moreover, in situations where the input/output system becomes a bottleneck in analysis, visualization, etc., or the execution of the PDE solver is expensive, the the number of passes made over the data must be minimized. In the interest of addressing this problem, this work focuses on the utility of pass-efficient, parallelizable, low-rank, matrix decomposition methods in compressing high-dimensional simulation data from turbulent flows. A particular emphasis is placed on using coarse representation of the data -- compatible with the PDE discretization grid -- to accelerate the construction of the low-rank factorization. This includes the presentation of a novel single-pass matrix decomposition algorithm for computing the so-called interpolative decomposition. The methods are described extensively and numerical experiments on two turbulent channel flow data are performed. In the first (unladen) channel flow case, compression factors exceeding {\$}400{\$} are achieved while maintaining accuracy with respect to first- and second-order flow statistics. In the particle-laden case, compression factors of 100 are achieved and the compressed data is used to recover particle velocities.},
archivePrefix = {arXiv},
arxivId = {1905.13257},
author = {Dunton, Alec M. and Jofre, Llu{\'{i}}s and Iaccarino, Gianluca and Doostan, Alireza},
doi = {10.1016/j.jcp.2020.109704},
eprint = {1905.13257},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Dunton et al. - 2019 - Pass-efficient methods for compression of high-dimensional turbulent flow data.pdf:pdf},
issn = {0021-9991},
journal = {Journal of Computational Physics},
pages = {109704},
publisher = {Elsevier Inc.},
title = {{Pass-efficient methods for compression of high-dimensional turbulent flow data}},
url = {http://arxiv.org/abs/1905.13257},
year = {2019}
}
@article{Zhuzhunashvili2017,
abstract = {Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) is demonstrated to efficiently solve eigenvalue problems for graph Laplacians that appear in spectral clustering. For static graph partitioning, 10-20 iterations of LOBPCG without preconditioning result in ∼10× error reduction, enough to achieve 100{\%} correctness for all Challenge datasets with known truth partitions, e.g., for graphs with 5K/.1M (50K/1M) Vertices/Edges in 2 (7) seconds, compared to over 5,000 (30,000) seconds needed by the baseline Python code. Our Python code 100{\%} correctly determines 98 (160) clusters from the Challenge static graphs with 0.5M (2M) vertices in 270 (1,700) seconds using 10GB (50GB) of memory. Our single-precision MATLAB code calculates the same clusters at half time and memory. For streaming graph partitioning, LOBPCG is initiated with approximate eigenvectors of the graph Laplacian already computed for the previous graph, in many cases reducing 2-3 times the number of required LOBPCG iterations, compared to the static case. Our spectral clustering is generic, i.e. assuming nothing specific of the block model or streaming, used to generate the graphs for the Challenge, in contrast to the base code. Nevertheless, in 10-stage streaming comparison with the base code for the 5K graph, the quality of our clusters is similar or better starting at stage 4 (7) for emerging edging (snowballing) streaming, while the computations are over 100-1000 faster.},
archivePrefix = {arXiv},
arxivId = {1708.07481},
author = {Zhuzhunashvili, David and Knyazev, Andrew},
doi = {10.1109/HPEC.2017.8091045},
eprint = {1708.07481},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Zhuzhunashvili, Knyazev - 2017 - Preconditioned spectral clustering for stochastic block partition streaming graph challenge (Preliminar.pdf:pdf},
isbn = {9781538634721},
journal = {2017 IEEE High Performance Extreme Computing Conference, HPEC 2017},
title = {{Preconditioned spectral clustering for stochastic block partition streaming graph challenge (Preliminary version at arXiv.)}},
year = {2017}
}
@article{Situ2017,
author = {Situ, E X and Patma, Rafflesia},
doi = {10.24189/ncr.2017.014},
file = {:Users/luciaminahyang/Documents/school/research/Readings/jan02notes.pdf:pdf},
keywords = {as a,bogor botanic gardens,completely depending on,ex situ conservation,flowering,habitat disappearance,houses,indonesia,rafflesia,s largest flowers,species with the world,the family rafflesiaceae dumort,true parasite they are},
number = {2},
pages = {90--91},
title = {{============== Научные Заметки ============== ================ Research Notes ===============}},
volume = {2},
year = {2017}
}
@article{Auou2003,
abstract = {This study used a laboratory experiment with monetary incentives to test the impact of three personal factors (moral reasoning, value orientation and risk preference), and three situational factors (the presence/absence of audits, tax inequity, and peer reporting behavior), while controlling for the impact of other demographic characteristics, on tax compliance. Analysis of Covariance (ANCOVA) reveals that all the main effects analyzed are statistically significant and robustly influence tax compliance behavior. These results highlight the importance of obtaining a proper understanding of these factors for developing effective policies for increasing the level of compliance, and indicate that standard enforcement polices based on punishment alone should be supplemented by an information system that would acquaint tax payers with the compliance level of other tax payers; reinforce the concept of fairness of the tax system among tax payers; and develop programs that enhance and appeal to a taxpayer's moral conscience and reinforce social cohesion.},
author = {{\"{A}}{\"{u}}{\^{o}}{\'{u}}, {\"{O}}},
doi = {10.16309/j.cnki.issn.1007-1776.2003.03.004},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/{\"{A}}{\"{u}}{\^{o}}{\'{u}} - 2003 - No Title中国儒家教育文化 对职业教育的影响.pdf:pdf},
number = {1},
pages = {6--8},
title = {{No Title中国儒家教育文化 对职业教育的影响}},
year = {2003}
}
@article{Gottschalck2008,
abstract = {I. Definition: The MJO is an intraseasonal fluctuation or “wave” occurring in the global tropics. The MJO is responsible for the majority of weather variability in these regions and results in variations in several important atmospheric and oceanic parameters which include both lower- and upper-level wind speed and direction, cloudiness, rainfall, sea surface temperature (SST), and ocean surface evaporation. The MJO is a naturally occurring component of our coupled ocean-atmosphere system and the typical length of the MJO cycle or wave is approximately 30-60 days (Madden and Julian, 1971, 1972; Madden and Julian, 1994; Zhang, 2005). II. Characteristics:},
author = {Gottschalck, Jon and Higgins, Wayne},
doi = {10.1029/2004RG000158.1.INTRODUCTION},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Zhang2005.pdf:pdf},
isbn = {8755-1209},
issn = {8755-1209},
journal = {Prediction Center.[Available online at http {\ldots}},
number = {2004},
pages = {4},
title = {{Madden Julian Oscillation Impacts}},
url = {http://140.90.101.29/products/precip/CWlink/MJO/MJO{\_}1page{\_}factsheet.pdf},
year = {2008}
}
@article{Wu2014,
abstract = {The Fiedler vector of a graph plays a vital role in many applications. But it is usually very expensive in that it involves the solution of an eigenvalue problem. In this paper, we introduce the inverse power method incorporated with the Householder deflation to compute the Fiedler Vector. In the inverse power iterations, the coefficient matrix is formed implicitly, to take advantage of the sparsity. The linear systems encountered at each iteration must be symmetric positive definite, thus the conjugate gradient method is used. In addition, preconditioning techniques are introduced to reduce the computation cost. Any kind of preconditioning techniques with dropping can be used. For the graphs related to some of the sparse matrices downloaded from the UF Sparse Matrix Collection, the experiments are compared to the known novel schemes. The results show that the provided method is more accurate. While it is slower than MC73 sequentially, it has good parallel efficiency compared with TraceMin. In addition, it is insensitive to the selection of parameters, which is superior to the other two methods. {\textcopyright} 2014 Elsevier B.V. All rights reserved.},
author = {Wu, Jian Ping and Song, Jun Qiang and Zhang, Wei Min},
doi = {10.1016/j.cam.2014.03.018},
file = {:Users/luciaminahyang/Documents/school/research/Readings/WuSongZhang.pdf:pdf},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
keywords = {Conjugate gradient iteration,Eigenvalue problem,Fiedler vector,Parallel computing,Preconditioner,Sparse linear system},
pages = {101--108},
publisher = {Elsevier B.V.},
title = {{An efficient and accurate method to compute the Fiedler vector based on Householder deflation and inverse power iteration}},
url = {http://dx.doi.org/10.1016/j.cam.2014.03.018},
volume = {269},
year = {2014}
}
@article{Ascher1995,
author = {Ascher, Uri M. and Ruuth, Steven J. and Wetton, Brian T. R.},
doi = {10.1137/0732037},
file = {:Users/luciaminahyang/Documents/school/research/Readings/AscherIMEX.pdf:pdf},
issn = {0036-1429},
journal = {SIAM Journal on Numerical Analysis},
month = {jun},
number = {3},
pages = {797--823},
title = {{Implicit-Explicit Methods for Time-Dependent Partial Differential Equations}},
url = {http://epubs.siam.org/doi/10.1137/0732037},
volume = {32},
year = {1995}
}
@article{Morrison2005,
abstract = {Background: Interpretation of simple microarray experiments is usually based on the fold-change of gene expression between a reference and a "treated" sample where the treatment can be of many types from drug exposure to genetic variation. Interpretation of the results usually combines lists of differentially expressed genes with previous knowledge about their biological function. Here we evaluate a method - based on the PageRank algorithm employed by the popular search engine Google - that tries to automate some of this procedure to generate prioritized gene lists by exploiting biological background information. Results: GeneRank is an intuitive modification of PageRank that maintains many of its mathematical properties. It combines gene expression information with a network structure derived from gene annotations (gene ontologies) or expression profile correlations. Using both simulated and real data we find that the algorithm offers an improved ranking of genes compared to pure expression change rankings. Conclusion: Our modification of the PageRank algorithm provides an alternative method of evaluating microarray experimental results which combines prior knowledge about the underlying network. GeneRank offers an improvement compared to assessing the importance of a gene based on its experimentally observed fold-change alone and may be used as a basis for further analytical developments. {\textcopyright} 2005 Morrison et al; licensee BioMed Central Ltd.},
author = {Morrison, Julie L. and Breitling, Rainer and Higham, Desmond J. and Gilbert, David R.},
doi = {10.1186/1471-2105-6-233},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Morrison2005.pdf:pdf;:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Morrison et al. - 2005 - GeneRank Using search engine technology for the analysis of microarray experiments.pdf:pdf},
issn = {14712105},
journal = {BMC Bioinformatics},
pages = {1--14},
title = {{GeneRank: Using search engine technology for the analysis of microarray experiments}},
volume = {6},
year = {2005}
}
@article{Liu,
author = {Liu, Eric},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Liu - Unknown - Conditioning and Numerical Stability Yelp Applied Learning Group.pdf:pdf},
title = {{Conditioning and Numerical Stability Yelp Applied Learning Group}}
}
@misc{Snijders1997,
abstract = {A statistical approach to a posteriori blockmodeling for graphs is proposed. The model assumes that the vertices of the graph are partitioned into two unknown blocks and that the probability of an edge between two vertices depends only on the blocks to which they belong. Statistical procedures are derived for estimating the probabilities of edges and for predicting the block structure from observations of the edge pattern only. ML estimators can be computed using the EM algorithm, but this strategy is practical only for small graphs. A Bayesian estimator, based on Gibbs sampling, is proposed. This estimator is practical also for large graphs. When ML estimators are used, the block structure can be predicted based on predictive likelihood. When Gibbs sampling is used, the block structure can be predicted from posterior predictive probabilities. A side result is that when the number of vertices tends to infinity while the probabilities remain constant, the block structure can be recovered correctly with probability tending to 1.},
author = {Snijders, Tom A.B. and Nowicki, Krzysztof},
booktitle = {Journal of Classification},
doi = {10.1007/s003579900004},
file = {:Users/luciaminahyang/Documents/school/research/Readings/spectral graph/SnijdersNowicki1997.pdf:pdf},
issn = {01764268},
keywords = {Colored graph,EM algorithm,Gibbs sampling,Latent class model,Social network},
number = {1},
pages = {75--100},
title = {{Estimation and prediction for stochastic blockmodels for graphs with latent block structure}},
volume = {14},
year = {1997}
}
@article{Trefethen2000,
abstract = {آموزش Matlab},
author = {Trefethen, Lloyd N},
doi = {10.1137/1.9780898719598},
file = {:Users/luciaminahyang/Documents/school/research/Readings/spectralmethodsmatlab.pdf:pdf},
isbn = {978-0-89871-465-4},
journal = {Lloydia (Cincinnati)},
pages = {1--181},
title = {{Spectral Methods in MATLAB{\_}[www.mathworks.ir]}},
url = {papers2://publication/uuid/0C56B7B3-A1FF-4BF1-BD74-DD85B99317FA},
year = {2000}
}
@article{VanLoan1978,
author = {{Van Loan}, C.},
doi = {10.1109/TAC.1978.1101743},
file = {:Users/luciaminahyang/Documents/school/research/Readings/VanLoan1978.pdf:pdf},
issn = {0018-9286},
journal = {IEEE Transactions on Automatic Control},
month = {jun},
number = {3},
pages = {395--404},
title = {{Computing integrals involving the matrix exponential}},
url = {http://ieeexplore.ieee.org/document/1101743/},
volume = {23},
year = {1978}
}
@article{Sidje1998,
author = {Sidje, Roger B},
doi = {10.1145/285861.285868},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Sidje.pdf:pdf},
issn = {00983500},
journal = {ACM Transactions on Mathematical Software},
month = {mar},
number = {1},
pages = {130--156},
title = {{Expokit: a software package for computing matrix exponentials}},
url = {http://portal.acm.org/citation.cfm?doid=285861.285868},
volume = {24},
year = {1998}
}
@article{Wielonsky2001,
abstract = {In this paper, we study asymptotic properties of rational functions that interpolate the exponential function. The interpolation is performed with respect to a triangular scheme of complex conjugate points lying in bounded rectangular domains included in the horizontal strip Imz{\textless}2$\pi$. Moreover, the height of these domains cannot exceed some upper bound which depends on the type of rational functions. We obtain different convergence results and precise estimates for the error function in compact sets of C that generalize the classical properties of Pad{\'{e}} approximants to the exponential function. The proofs rely on, among others, Walsh's theorem on the location of the zeros of linear combinations of derivatives of a polynomial and on Rolle's theorem for real exponential polynomials in the complex domain. {\textcopyright} 2001 Academic Press.},
author = {Wielonsky, F.},
doi = {10.1006/jath.2001.3581},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Wielonsky2001.pdf:pdf},
issn = {00219045},
journal = {Journal of Approximation Theory},
keywords = {Error estimates,Exponential function,Rational interpolation in the complex plane},
number = {2},
pages = {344--368},
title = {{Rational Approximation to the Exponential Function with Complex Conjugate Interpolation Points}},
volume = {111},
year = {2001}
}
@article{Huang2009a,
abstract = {Spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis, image processing and data mining. However, the computational and/or communication resources required by the method in processing large-scale data are often prohibitively high, and practitioners are often required to perturb the original data in various ways (quantization, downsampling, etc) before invoking a spectral algorithm. In this paper, we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering. We show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the Laplacian matrix. From this result we derive approximate upper bounds on the clustering error. We show that this bound is tight empirically across a wide range of problems, suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a specification of permitted loss in clustering performance.},
author = {Huang, Ling and Yan, Donghui and Jordan, Michael I. and Taft, Nina},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Missclassification.pdf:pdf},
isbn = {9781605609492},
journal = {Advances in Neural Information Processing Systems 21 - Proceedings of the 2008 Conference},
number = {January 2019},
pages = {705--712},
title = {{Spectral clustering with perturbed data}},
year = {2009}
}
@article{Krogstad2005a,
abstract = {The integrating factor (IF) method for numerical integration of stiff nonlinear PDEs has the disadvantage of producing large error coefficients when the linear term has large norm. We propose a generalization of the IF method, and in particular construct multistep-type methods with several orders of magnitude improved accuracy. We also consider exponential time differencing (ETD) methods, and point out connections with a particular application of the commutator-free Lie group methods. We present a new fourth order ETDRK method with improved accuracy. The methods considered are compared in several numerical examples. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
author = {Krogstad, Stein},
doi = {10.1016/j.jcp.2004.08.006},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Krogstad - 2005 - Generalized integrating factor methods for stiff PDEs.pdf:pdf},
issn = {00219991},
journal = {Journal of Computational Physics},
keywords = {Exponential time differencing,Integrating factor methods,Lie group methods,Stiff systems},
number = {1},
pages = {72--88},
title = {{Generalized integrating factor methods for stiff PDEs}},
volume = {203},
year = {2005}
}
@article{Sun2019,
abstract = {This paper describes a new algorithm for computing a low-Tucker-rank approximation of a tensor. The method applies a randomized linear map to the tensor to obtain a sketch that captures the important directions within each mode, as well as the interactions among the modes. The sketch can be extracted from streaming or distributed data or with a single pass over the tensor, and it uses storage proportional to the degrees of freedom in the output Tucker approximation. The algorithm does not require a second pass over the tensor, although it can exploit another view to compute a superior approximation. The paper provides a rigorous theoretical guarantee on the approximation error. Extensive numerical experiments show that that the algorithm produces useful results that improve on the state of the art for streaming Tucker decomposition.},
archivePrefix = {arXiv},
arxivId = {1904.10951},
author = {Sun, Yiming and Guo, Yang and Luo, Charlene and Tropp, Joel and Udell, Madeleine},
eprint = {1904.10951},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - 2019 - Low-Rank Tucker Approximation of a Tensor From Streaming Data.pdf:pdf},
keywords = {68q25,68r10,68u05,ams subject classifications,dimension reduction,randomized algorithm,sketching method,streaming algorithm,tensor compression,tucker decomposition},
title = {{Low-Rank Tucker Approximation of a Tensor From Streaming Data}},
url = {http://arxiv.org/abs/1904.10951},
year = {2019}
}
@article{Hottovy2015,
abstract = {A linear stochastic model is presented for the dynamics of water vapor and tropical convection. Despite its linear formulation, the model reproduces a wide variety of observational statistics from disparate perspectives, including (i) a cloud cluster area distribution with an approximate power law; (ii) a power spectrum of spatiotemporal red noise, as in the "background spectrum" of tropical convection; and (iii) a suite of statistics that resemble the statistical physics concepts of critical phenomena and phase transitions. The physical processes of the model are precipitation, evaporation, and turbulent advection-diffusion of water vapor, and they are represented in idealized form as eddy diffusion, damping, and stochastic forcing. Consequently, the form of the model is a damped version of the two-dimensional stochastic heat equation. Exact analytical solutions are available for many statistics, and numerical realizations can be generated for minimal computational cost and for any desired time step. Given the simple form of the model, the results suggest that tropical convection may behave in a relatively simple, random way. Finally, relationships are also drawn with the Ising model, the Edwards-Wilkinson model, the Gaussian free field, and the Schramm-Loewner evolution and its possible connection with cloud cluster statistics. Potential applications of the model include several situations where realistic cloud fields must be generated for minimal cost, such as cloud parameterizations for climate models or radiative transfer models.},
author = {Hottovy, Scott and Stechmann, Samuel N.},
doi = {10.1175/JAS-D-15-0119.1},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Hottovy{\_}Stechmann{\_}JAS{\_}15.pdf:pdf},
issn = {15200469},
journal = {Journal of the Atmospheric Sciences},
keywords = {Clouds,Convective parameterization,Models and modeling,Stochastic models},
number = {12},
pages = {4721--4738},
title = {{A spatiotemporal stochastic model for tropical precipitation and water vapor dynamics}},
volume = {72},
year = {2015}
}
@article{Grooms2011,
abstract = {The linear stability of IMEX (IMplicit-EXplicit) methods and exponential integrators for stiff systems of ODEs arising in the discrete solution of PDEs is examined for nonlinear PDEs with both linear dispersion and dissipation, and a clear method of visualization of the linear stability regions is proposed. Predictions are made based on these visualizations and are supported by a series of experiments on five PDEs including quasigeostrophic equations and stratified Boussinesq equations. The experiments, involving 24 IMEX and exponential methods of third and fourth order, confirm the predictions of the linear stability analysis, that the methods are typically limited by small eigenvalues of the linear term and by eigenvalues on or near the imaginary axis rather than by large eigenvalues near the negative real axis. The experiments also demonstrate that IMEX methods achieve comparable stability to exponential methods, and that exponential methods are significantly more accurate only when the problem is nearly linear. Novel IMEX predictor-corrector methods are also derived. {\textcopyright} 2011 Elsevier Inc.},
author = {Grooms, Ian and Julien, Keith},
doi = {10.1016/j.jcp.2011.02.007},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Grooms{\_}Julien{\_}JCP{\_}11.pdf:pdf},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {Exponential integrators,IMEX,Semi-implicit,Stability},
number = {9},
pages = {3630--3650},
title = {{Linearly implicit methods for nonlinear PDEs with linear dispersion and dissipation}},
volume = {230},
year = {2011}
}
@article{Martinsson2016a,
author = {Martinsson, Per-Gunnar and Voronin, Sergey},
doi = {10.1137/15M1026080},
file = {:Users/luciaminahyang/Documents/school/research/Readings/MartinssonVoronin2016.pdf:pdf},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
keywords = {080729062,10,1137,65m12,65m15,65m60,absorbing boundary,acoustic wave,ams subject classifications,condition,discontinuous galerkin,doi,energy conservation,optimal convergence,stability analysis},
month = {jan},
number = {5},
pages = {S485--S507},
title = {{A Randomized Blocked Algorithm for Efficiently Computing Rank-revealing Factorizations of Matrices}},
url = {http://epubs.siam.org/doi/10.1137/15M1026080},
volume = {38},
year = {2016}
}
@article{Halko2011a,
abstract = {Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed-either explicitly or implicitly-to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, robustness, and/or speed. These claims are supported by extensive numerical experiments and a detailed error analysis. The specific benefits of randomized techniques depend on the computational environment. Consider the model problem of finding the k dominant components of the singular value decomposition of an m × n matrix. (i) For a dense input matrix, randomized algorithms require O(mn log(k)) floating-point operations (flops) in contrast to O(mnk) for classical algorithms. (ii) For a sparse input matrix, the flop count matches classical Krylov subspace methods, but the randomized approach is more robust and can easily be reorganized to exploit multiprocessor architectures. (iii) For a matrix that is too large to fit in fast memory, the randomized techniques require only a constant number of passes over the data, as opposed to O(k) passes for classical algorithms. In fact, it is sometimes possible to perform matrix approximation with a single pass over the data. {\textcopyright} 2011 Society for Industrial and Applied Mathematics.},
archivePrefix = {arXiv},
arxivId = {0909.4061},
author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
doi = {10.1137/090771806},
eprint = {0909.4061},
file = {:Users/luciaminahyang/Documents/school/research/Readings/HalkoMartinsson2011.pdf:pdf},
issn = {00361445},
journal = {SIAM Review},
keywords = {Dimension reduction,Eigenvalue decomposition,Interpolative decomposition,Johnson-Lindenstrauss lemma,Matrix approximation,Parallel algorithm,Pass-efficient algorithm,Principal component analysis,Random matrix,Randomized algorithm},
number = {2},
pages = {217--288},
title = {{Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions}},
volume = {53},
year = {2011}
}
@article{Tanaka2004,
abstract = {The e?ects of discretization of the spectrum on the evolution of weak turbulence of surface gravity waves are investigated by direct numerical simulations. Discretization of the spectrum is implemented by imposing periodic boundary conditions on the horizontal plane. Initial wave 3elds are constructed so that they have the JONSWAP spectrum, and the nonlinear energy transfer among component waves is estimated by following their evolution deterministically according to the primitive governing equations. It is found that the discretized system can produce the same nonlinear energy transfer as that predicted theoretically for a continuous spectrum, even when the distribution of component waves on the wavenumber plane is so sparse that the corresponding area of the water surface on the horizontal plane is just 4?p×4?p, with ?p being the wavelength corresponding to the peak of the spectrum, provided that the ensemble averaging is taken over a su;cient number of realizations. The regime of “frozen” turbulence, which is known to appear in the weak turbulence of capillary waves with a discretized spectrum, is not observed in the case of gravity waves. This di?erence in the e?ects of discretization of the spectrum on the weak turbulence of capillary waves and that of gravity waves is discussed in relation to the number of trios or quartets of component waves which satisfy the conditions for quasi-resonant nonlinear interactions.},
author = {Tanaka, Mitsuhiro and Yokoyama, Naoto},
doi = {10.1016/j.fluiddyn.2003.12.001},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Tanaka2004.pdf:pdf},
issn = {0169-5983},
journal = {Fluid Dynamics Research},
month = {mar},
number = {3},
pages = {199--216},
title = {{Effects of discretization of the spectrum in water-wave turbulence}},
url = {http://stacks.iop.org/1873-7005/34/i=3/a=A05?key=crossref.ce82e943715413097fdfa562dc582d9b},
volume = {34},
year = {2004}
}
@article{Cavaglieri2015,
abstract = {Implicit/explicit (IMEX) Runge-Kutta (RK) schemes are effective for time-marching ODE systems with both stiff and nonstiff terms on the RHS; such schemes implement an (often A-stable or better) implicit RK scheme for the stiff part of the ODE, which is often linear, and, simultaneously, a (more convenient) explicit RK scheme for the nonstiff part of the ODE, which is often nonlinear. Low-storage RK schemes are especially effective for time-marching high-dimensional ODE discretizations of PDE systems on modern (cache-based) computational hardware, in which memory management is often the most significant computational bottleneck. In this paper, we develop and characterize eight new low-storage implicit/explicit RK schemes which have higher accuracy and better stability properties than the only low-storage implicit/explicit RK scheme available previously, the venerable second-order Crank-Nicolson/Runge-Kutta-Wray (CN/RKW3) algorithm that has dominated the DNS/LES literature for the last 25 years, while requiring similar storage (two, three, or four registers of length N) and comparable floating-point operations per timestep.},
author = {Cavaglieri, Daniele and Bewley, Thomas},
doi = {10.1016/j.jcp.2015.01.031},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Cavaglieri{\_}Bewley{\_}JCP{\_}15.pdf:pdf},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {L stability,Low-storage IMEXRK methods,SSP/TVD methods,Stiff ODE/PDE/DAE time marching},
pages = {172--193},
publisher = {Elsevier Inc.},
title = {{Low-storage implicit/explicit Runge-Kutta schemes for the simulation of stiff high-dimensional ODE systems}},
url = {http://dx.doi.org/10.1016/j.jcp.2015.01.031},
volume = {286},
year = {2015}
}
@article{Yin2019,
abstract = {A parallel and fully implicit method is developed for numerical simulations of the thermal convection of an incompressible fluid in a rotating spherical shell. The method is based on a pseudo-compressibility approach with dual time stepping in order to tackle with the numerical difficulty of the solenoidal condition in the incompressible Navier-Stokes equations. The numerical solution of the nonlinear governing equations is based on the method of lines, whereby the partial differential equations are transformed into ordinary differential equations by a suitable spatial discretization. A second-order cell-centered finite volume discretization based on a collocated cubed-sphere grid is used here. In order to relax the time step limit and accurately integrate the semi-discrete nonlinear ordinary differential equations in time, we employ a second-order explicit-first-step, single-diagonal-coefficient, diagonally implicit Runge–Kutta (ESDIRK) method with adaptive time stepping. The nonlinear algebraic system arising at each pseudo-time step is solved by a Newton–Krylov–Schwarz algorithm to achieve good load balance on modern supercomputers. The numerical results are in good agreement with the benchmark solutions and more accurate than those obtained with a fractional step approach in our previous research. Large-scale tests on the Tianhe-2 supercomputer indicate that the fully implicit solver can achieve good parallel scalabilities in both strong and weak senses.},
author = {Yin, Liang and Yang, Chao and Ma, Shi Zhuang and Zhang, Ke Ke},
doi = {10.1016/j.compfluid.2019.104278},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Yin2019.pdf:pdf},
issn = {00457930},
journal = {Computers and Fluids},
keywords = {Core convection,Cubed-sphere grid,Fully implicit method,Parallel simulation,Pseudo-compressibility approach},
title = {{Parallel and fully implicit simulations of the thermal convection in the Earth's outer core}},
volume = {193},
year = {2019}
}
@book{Parlett1998,
abstract = {According to Parlett, ‘Vibrations are everywhere, and so too are the eigenvalues associated with them. As mathematical models invade more and more disciplines, we can anticipate a demand for eigenvalue calculations in an ever richer variety of contexts'. Anyone who performs these calculations will welcome the reprinting of Parlett's book (originally published in 1980). In this unabridged, amended version, Parlett covers aspects of the problem that are not easily found elsewhere. The chapter titles convey the scope of the material succinctly. The aim of the book is to present mathematical knowledge that is needed in order to understand the art of computing eigenvalues of real symmetric matrices, either all of them or only a few. The author explains why the selected information really matters and he is not shy about making judgments. The commentary is lively but the proofs are terse.},
author = {Parlett, Beresford N.},
booktitle = {Mathematics of Computation},
doi = {10.1137/1.9781611971163},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Parlett1998.pdf:pdf},
isbn = {978-0-89871-402-9},
issn = {00255718},
month = {jan},
number = {156},
pages = {599},
publisher = {Society for Industrial and Applied Mathematics},
title = {{The Symmetric Eigenvalue Problem}},
url = {http://epubs.siam.org/doi/book/10.1137/1.9781611971163},
volume = {37},
year = {1998}
}
@article{Markidis2018,
abstract = {The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called Tensor Core that performs one matrix-multiply-and-accumulate on 4x4 matrices per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta microarchitecture, provides 640 Tensor Cores with a theoretical peak performance of 125 Tflops/s in mixed precision. In this paper, we investigate current approaches to program NVIDIA Tensor Cores, their performances and the precision loss due to computation in mixed precision. Currently, NVIDIA provides three different ways of programming matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS GEMM. After experimenting with different approaches, we found that NVIDIA Tensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100 GPU, seven and three times the performance in single and half precision respectively. A WMMA implementation of batched GEMM reaches a performance of 4 Tflops/s. While precision loss due to matrix multiplication with half precision input might be critical in many HPC applications, it can be considerably reduced at the cost of increased computation. Our results indicate that HPC applications using matrix multiplications can strongly benefit from using of NVIDIA Tensor Cores.},
archivePrefix = {arXiv},
arxivId = {1803.04014},
author = {Markidis, Stefano and Chien, Steven Wei Der and Laure, Erwin and Peng, Ivy Bo and Vetter, Jeffrey S.},
doi = {10.1109/IPDPSW.2018.00091},
eprint = {1803.04014},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Markidis2018.pdf:pdf},
isbn = {9781538655559},
journal = {Proceedings - 2018 IEEE 32nd International Parallel and Distributed Processing Symposium Workshops, IPDPSW 2018},
keywords = {GEMM,GPU Programming,Mixed Precision,NVIDIA Tensor Cores},
pages = {522--531},
publisher = {IEEE},
title = {{NVIDIA tensor core programmability, performance {\&} precision}},
year = {2018}
}
@article{Gleich2015,
abstract = {Google's PageRank method was developed to evaluate the importance of web-pages via their link structure. The mathematics of PageRank, however, are entirely general and apply to any graph or network in any domain. Thus, PageRank is now regularly used in bibliometrics, social and information network analysis, and for link prediction and recommendation. It's even used for systems analysis of road networks, as well as biology, chemistry, neuroscience, and physics. We'll see the mathematics and ideas that unite these diverse applications. {\textcopyright} 2015 Society for Industrial and Applied Mathematics.},
archivePrefix = {arXiv},
arxivId = {1407.5107},
author = {Gleich, David F.},
doi = {10.1137/140976649},
eprint = {1407.5107},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Gleich.pdf:pdf;:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Gleich - 2015 - PageRank beyond the web.pdf:pdf},
issn = {00361445},
journal = {SIAM Review},
keywords = {Markov chain,PageRank},
number = {3},
pages = {321--363},
title = {{PageRank beyond the web}},
volume = {57},
year = {2015}
}
@article{Tensors2008,
author = {Tensors, Structured},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Tensors - 2008 - Copyright {\textcopyright} by SIAM . Unauthorized reproduction of this article is prohibited (2).pdf:pdf},
journal = {Society},
keywords = {060655936,1,10,1137,15a18,15a21,15a69,5,ams subject classifications,decomposition is now mature,doi,fast algorithms,introduction,multilinear svd,structured and unstructured tensors,the subject of multilinear},
number = {3},
pages = {1008--1021},
title = {{Copyright {\textcopyright} by SIAM . Unauthorized reproduction of this article is prohibited .}},
volume = {30},
year = {2008}
}
@book{Hochbruck2010,
abstract = {In this paper we consider the construction, analysis, implementation and application of exponential integrators. The focus will be on two types of stiff problems. The first one is characterized by a Jacobian that possesses eigenvalues with large negative real parts. Parabolic partial differential equations and their spatial discretization are typical examples. The second class consists of highly oscillatory problems with purely imaginary eigenvalues of large modulus. Apart from motivating the construction of exponential integrators for various classes of problems, our main intention in this article is to present the mathematics behind these methods. We will derive error bounds that are independent of stiffness or highest frequencies in the system. Since the implementation of exponential integrators requires the evaluation of the product of a matrix function with a vector, we will briefly discuss some possible approaches as well. The paper concludes with some applications, in which exponential integrators are used. Copyright {\textcopyright} 2010 Cambridge University Press.},
author = {Hochbruck, Marlis and Ostermann, Alexander},
booktitle = {Acta Numerica},
doi = {10.1017/S0962492910000048},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Hochbruck.pdf:pdf},
isbn = {0962492910000},
issn = {09624929},
pages = {209--286},
title = {{Exponential integrators}},
volume = {19},
year = {2010}
}
@article{Ballard2010,
abstract = {Algorithms have two costs: arithmetic and communication. The latter represents the cost of moving data, either between levels of a memory hierarchy, or between processors over a network. Communication often dominates arithmetic and represents a rapidly increasing proportion of the total cost, so we seek algorithms that minimize communication. In $\backslash$cite{\{}BDHS10{\}} lower bounds were presented on the amount of communication required for essentially all {\$}O(n{\^{}}3){\$}-like algorithms for linear algebra, including eigenvalue problems and the SVD. Conventional algorithms, including those currently implemented in (Sca)LAPACK, perform asymptotically more communication than these lower bounds require. In this paper we present parallel and sequential eigenvalue algorithms (for pencils, nonsymmetric matrices, and symmetric matrices) and SVD algorithms that do attain these lower bounds, and analyze their convergence and communication costs.},
archivePrefix = {arXiv},
arxivId = {1011.3077},
author = {Ballard, Grey and Demmel, James and Dumitriu, Ioana},
eprint = {1011.3077},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Ballard2010arXiv.pdf:pdf;:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Ballard, Demmel, Dumitriu - 2010 - Minimizing Communication for Eigenproblems and the Singular Value Decomposition.pdf:pdf},
pages = {1--43},
title = {{Minimizing Communication for Eigenproblems and the Singular Value Decomposition}},
url = {http://arxiv.org/abs/1011.3077},
year = {2010}
}
@article{Tensors2008a,
author = {Tensors, Structured},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Tensors - 2008 - Copyright {\textcopyright} by SIAM . Unauthorized reproduction of this article is prohibited .pdf:pdf},
journal = {Society},
keywords = {060655936,1,10,1137,15a18,15a21,15a69,5,ams subject classifications,decomposition is now mature,doi,fast algorithms,introduction,multilinear svd,structured and unstructured tensors,the subject of multilinear},
number = {3},
pages = {1008--1021},
title = {{Copyright {\textcopyright} by SIAM . Unauthorized reproduction of this article is prohibited .}},
volume = {30},
year = {2008}
}
@article{Kennedy2001,
abstract = {Additive Runge-Kutta (ARK) methods are investigated for application to the spatially discretized one-dimensional convection-diffusion-reaction (CDR.) equations. First, accuracy, stability, conservation, and dense-output are considered for the general case when N different Runge-Kutta methods are grouped into a single composite method. Then, implicit-explicit, N = 2, additive Runge-Kutta (ARK2) methods from third- to fifth-order are presented that allow for integration of stiff terms by an L-stable, stiffly-accurate explicit, singly diagonally implicit Runge-Kutta (ESDIRK) method while the nonstiff terms are integrated with a traditional explicit Runge-Kutta method (ERK). Coupling error terms are of equal order to those of the elemental methods. Derived ARK2 methods have vanishing stability functions for very large values of the stiff scaled eigenvalue, z[I] → -∞, and retain high stability efficiency in the absence of stiffness, z[I] → 0. Extrapolation-type stage-value predictors are provided based on dense-output formulae. Optimized methods minimize both leading order ARK2 error terms and Butcher coefficient magnitudes as well as maximize conservation properties. Numerical tests of the new schemes on a CDR problem show negligible stiffness leakage and near classical order convergence rates. However, tests on three simple singular-perturbation problems reveal generally predictable order reduction. Error control is best managed with a PID-controller. While results for the fifth-order method are disappointing, both the new third- and fourth-order methods are at least as efficient as existing ARK2 methods while offering error control and stage-value predictors.},
author = {Kennedy, Christopher A. and Carpenter, Mark H.},
file = {:Users/luciaminahyang/Documents/school/research/Readings/KennedyCarpenter.pdf:pdf},
issn = {04999320},
journal = {NASA Technical Memorandum},
number = {211038},
pages = {1--42},
title = {{Additive Runge-Kutta schemes for Convection-Diffusion-Reaction equations}},
volume = {44},
year = {2001}
}
@book{Leveque2007,
author = {Leveque, Randall J},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Leveque{\_}FD.pdf:pdf},
isbn = {9780898716290},
pages = {1--341},
title = {{for Ordinary and Partial for Ordinary and Partial Steady-State and Time-Dependent Problems}},
year = {2007}
}
@book{Higham2002b,
author = {Higham, Nicholas J.},
booktitle = {Accuracy and Stability of Numerical Algorithms},
doi = {10.1137/1.9780898718027},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Higham - 2002 - Accuracy and Stability of Numerical Methods(2).pdf:pdf},
isbn = {0898715210},
pages = {1--656},
title = {{Accuracy and stability of numerical algorithms}},
year = {2002}
}
@article{Leyendecker2013,
abstract = {The simulation of systems with dynamics on strongly varying time scales is quite challenging and demanding with regard to possible numerical methods. A rather naive approach is to use the smallest necessary time step to guarantee a stable integration of the fast frequencies. However, this typically leads to unacceptable computational loads. Alternatively, multirate methods integrate the slow part of the system with a relatively large step size while the fast part is integrated with a small time step. In this work, a multirate integrator for constrained dynamical systems is derived in closed form via a discrete variational principle on a time grid consisting of macro and micro time nodes. Being based on a discrete version of Hamilton's principle, the resulting variational multirate integrator is a symplectic and momentum preserving integration scheme and also exhibits good energy behaviour. Depending on the discrete approximations for the Lagrangian function, one obtains different integrators, e.g. purely implicit or purely explicit schemes, or methods that treat the fast and slow parts in different ways. The performance of the multirate integrator is demonstrated by means of several examples.},
author = {Leyendecker, Sigrid and Ober-Bl{\"{o}}baum, Sina},
doi = {10.1007/978-94-007-5404-1_5},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Shu{\_}Osher.pdf:pdf},
isbn = {9789400754034},
issn = {18713033},
journal = {Computational Methods in Applied Sciences},
pages = {97--121},
title = {{A variational approach to multirate integration for constrained systems}},
volume = {28},
year = {2013}
}
@article{Prikopa2016,
abstract = {We present the novel parallel linear least squares solvers ARPLS-IR and ARPLS-MPIR for dense overdetermined linear systems. All internode communication of our ARPLS solvers arises in the context of all-reduce operations across the parallel system and therefore they benefit directly from efficient implementations of such operations. Our approach is based on the semi-normal equations, which are in general not backward stable. However, the method is stabilised by using iterative refinement. We show that performing iterative refinement in mixed precision also increases the parallel performance of the algorithm. We consider different variants of the ARPLS algorithm depending on the conditioning of the problem and in this context also evaluate the method of normal equations with iterative refinement. For ill-conditioned systems, we demonstrate that the semi-normal equations with standard iterative refinement achieve the best accuracy compared to other parallel solvers. We discuss the conceptual advantages of ARPLS-IR and ARPLS-MPIR over alternative parallel approaches based on QR factorisation or the normal equations. Moreover, we analytically compare the communication cost to an approach based on communication-avoiding QR factorisation. Numerical experiments on a high performance cluster illustrate speed-ups up to 3820 on 2048 cores for ill-conditioned tall and skinny matrices over state-of-the-art solvers from DPLASMA or ScaLAPACK.},
author = {Prikopa, Karl E. and Gansterer, Wilfried N. and Wimmer, Elias},
doi = {10.1016/j.parco.2016.05.014},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Prikopa, Gansterer, Wimmer - 2016 - Parallel iterative refinement linear least squares solvers based on all-reduce operations.pdf:pdf},
issn = {01678191},
journal = {Parallel Computing},
keywords = {All-reduce,Iterative refinement,Mixed precision,Normal equations,Parallel least squares solver,Semi-normal equations,Tall and skinny matrices},
number = {September},
pages = {167--184},
title = {{Parallel iterative refinement linear least squares solvers based on all-reduce operations}},
volume = {57},
year = {2016}
}
@article{Halko2011,
abstract = {Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed-either explicitly or implicitly-to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, robustness, and/or speed. These claims are supported by extensive numerical experiments and a detailed error analysis. The specific benefits of randomized techniques depend on the computational environment. Consider the model problem of finding the k dominant components of the singular value decomposition of an m × n matrix. (i) For a dense input matrix, randomized algorithms require O(mn log(k)) floating-point operations (flops) in contrast to O(mnk) for classical algorithms. (ii) For a sparse input matrix, the flop count matches classical Krylov subspace methods, but the randomized approach is more robust and can easily be reorganized to exploit multiprocessor architectures. (iii) For a matrix that is too large to fit in fast memory, the randomized techniques require only a constant number of passes over the data, as opposed to O(k) passes for classical algorithms. In fact, it is sometimes possible to perform matrix approximation with a single pass over the data. {\textcopyright} 2011 Society for Industrial and Applied Mathematics.},
archivePrefix = {arXiv},
arxivId = {0909.4061},
author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
doi = {10.1137/090771806},
eprint = {0909.4061},
file = {:Users/luciaminahyang/Documents/school/research/Readings/HalkoMartinsson2011.pdf:pdf},
issn = {00361445},
journal = {SIAM Review},
keywords = {Dimension reduction,Eigenvalue decomposition,Interpolative decomposition,Johnson-Lindenstrauss lemma,Matrix approximation,Parallel algorithm,Pass-efficient algorithm,Principal component analysis,Random matrix,Randomized algorithm},
number = {2},
pages = {217--288},
title = {{Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions}},
volume = {53},
year = {2011}
}
@article{Martinsson2020,
abstract = {This survey describes probabilistic algorithms for linear algebra computations, such as factorizing matrices and solving linear systems. It focuses on techniques that have a proven track record for real-world problem instances. The paper treats both the theoretical foundations of the subject and the practical computational issues. Topics covered include norm estimation; matrix approximation by sampling; structured and unstructured random embeddings; linear regression problems; low-rank approximation; subspace iteration and Krylov methods; error estimation and adaptivity; interpolatory and CUR factorizations; Nystr$\backslash$"om approximation of positive-semidefinite matrices; single view ("streaming") algorithms; full rank-revealing factorizations; solvers for linear systems; and approximation of kernel matrices that arise in machine learning and in scientific computing.},
archivePrefix = {arXiv},
arxivId = {2002.01387},
author = {Martinsson, Per-Gunnar and Tropp, Joel},
eprint = {2002.01387},
file = {:Users/luciaminahyang/Documents/school/research/Readings/MartinssonTropp2020.pdf:pdf},
title = {{Randomized Numerical Linear Algebra: Foundations {\&} Algorithms}},
url = {http://arxiv.org/abs/2002.01387},
year = {2020}
}
@article{Higham2004,
abstract = {The Lagrange representation of the interpolating polynomial can be rewritten in two more computationally attractive forms: a modified Lagrange form and a barycentric form. We give an error analysis of the evaluation of the interpolating polynomial using these two forms. The modified Lagrange formula is shown to be backward stable. The barycentric formula has a less favourable error analysis, but is forward stable for any set of interpolating points with a small Lebesgue constant. Therefore the barycentric formula can be significantly less accurate than the modified Lagrange formula only for a poor choice of interpolating points. This analysis provides further weight to the argument of Berrut and Trefethen that barycentric Lagrange interpolation should be the polynomial interpolation method of choice.},
author = {Higham, Nicholas J.},
doi = {10.1093/imanum/24.4.547},
issn = {02724979},
journal = {IMA Journal of Numerical Analysis},
keywords = {Backward error,Barycentric formula,Forward error,Lagrange interpolation,Lebesgue constant,Polynomial interpolation,Rounding error analysis},
number = {4},
pages = {547--556},
title = {{The numerical stability of barycentric Lagrange interpolation}},
volume = {24},
year = {2004}
}
@article{Ipsen2019,
abstract = {Probabilistic models are proposed for bounding the forward error in the numerically computed inner product (dot product, scalar product) between of two real {\$}n{\$}-vectors. We derive probabilistic perturbation bounds, as well as probabilistic roundoff error bounds for the sequential accumulation of the inner product. These bounds are non-asymptotic, explicit, and make minimal assumptions on perturbations and roundoffs. The perturbations are represented as independent, bounded, zero-mean random variables, and the probabilistic perturbation bound is based on Azuma's inequality. The roundoffs are also represented as bounded, zero-mean random variables. The first probabilistic bound assumes that the roundoffs are independent, while the second one does not. For the latter, we construct a Martingale that mirrors the sequential order of computations. Numerical experiments confirm that our bounds are more informative, often by several orders of magnitude, than traditional deterministic bounds -- even for small vector dimensions{\~{}}{\$}n{\$} and very stringent success probabilities. In particular the probabilistic roundoff error bounds are functions of {\$}\backslashsqrt{\{}n{\}}{\$} rather than{\~{}}{\$}n{\$}, thus giving a quantitative confirmation of Wilkinson's intuition. The paper concludes with a critical assessment of the probabilistic approach.},
archivePrefix = {arXiv},
arxivId = {1906.10465},
author = {Ipsen, Ilse C. F. and Zhou, Hua},
eprint = {1906.10465},
file = {:Users/luciaminahyang/Documents/school/research/Readings/IpsenZhou.pdf:pdf},
keywords = {perturbation bounds,random variables,roundoff errors,sums of random variables},
title = {{Probabilistic Error Analysis for Inner Products}},
url = {http://arxiv.org/abs/1906.10465},
year = {2019}
}
@article{Martinsson2015,
abstract = {A fundamental problem when adding column pivoting to the Householder QR factorization is that only about half of the computation can be cast in terms of high performing matrix-matrix multiplications, which greatly limits the benefits that can be derived from so-called blocking of algorithms. This paper describes a technique for selecting groups of pivot vectors by means of randomized projections. It is demonstrated that the asymptotic flop count for the proposed method is {\$}2mn{\^{}}2 - (2/3)n{\^{}}3{\$} for an {\$}m\backslashtimes n{\$} matrix, identical to that of the best classical unblocked Householder QR factorization algorithm (with or without pivoting). Experiments demonstrate acceleration in speed of close to an order of magnitude relative to the {\{}$\backslash$sc geqp3{\}} function in LAPACK, when executed on a modern CPU with multiple cores. Further, experiments demonstrate that the quality of the randomized pivot selection strategy is roughly the same as that of classical column pivoting. The described algorithm is made available under Open Source license and can be used with LAPACK or libflame.},
archivePrefix = {arXiv},
arxivId = {1512.02671},
author = {Martinsson, Per-Gunnar and Quintana-Orti, Gregorio and Heavner, Nathan and van de Geijn, Robert},
eprint = {1512.02671},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Martinsson et al. - 2015 - Householder QR Factorization with Randomization for Column Pivoting (HQRRP). FLAME Working Note {\#}78.pdf:pdf},
pages = {1--20},
title = {{Householder QR Factorization with Randomization for Column Pivoting (HQRRP). FLAME Working Note {\#}78}},
url = {http://arxiv.org/abs/1512.02671},
year = {2015}
}
@book{Saad2000,
author = {Saad, Yousef},
doi = {10.1137/1.9780898718003},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Saad - 2003 - Iterative Methods for Sparse Linear Systems.pdf:pdf;:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Saad - 2003 - Iterative Methods for Sparse Linear Systems(2).pdf:pdf;:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Saad - 2003 - Iterative Methods for Sparse Linear Systems(3).pdf:pdf;:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Saad - 2003 - Iterative Methods for Sparse Linear Systems(4).pdf:pdf;:Users/luciaminahyang/Documents/school/research/Readings/spectral graph/Saad.pdf:pdf},
isbn = {978-0-89871-534-7},
month = {jan},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Iterative Methods for Sparse Linear Systems}},
url = {http://epubs.siam.org/doi/book/10.1137/1.9780898718003},
year = {2003}
}
@article{Page1998,
abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.},
archivePrefix = {arXiv},
arxivId = {1111.4503v1},
author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
doi = {10.1.1.31.1768},
eprint = {1111.4503v1},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Page1999.pdf:pdf;:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Page et al. - 1998 - The PageRank Citation Ranking Bringing Order to the Web.pdf:pdf},
isbn = {9781424433803},
issn = {1752-0509},
journal = {World Wide Web Internet And Web Information Systems},
number = {1999-66},
pages = {1--17},
pmid = {20840727},
title = {{The PageRank Citation Ranking: Bringing Order to the Web}},
url = {http://ilpubs.stanford.edu:8090/422},
volume = {54},
year = {1998}
}
@article{Yamamoto2015,
abstract = {We consider the QR decomposition of an m × n matrix X with full column rank, where m × n. Among the many algorithms available, the Cholesky QR algorithm is ideal from the viewpoint of high performance computing since it consists entirely of standard level 3 BLAS operations with large matrix sizes, and requires only one reduce and broadcast in parallel environments. Unfortunately, it is well-known that the algorithm is not numerically stable and the deviation from orthogonality of the computed Q factor is of order O(($\kappa${\textless}inf{\textgreater}2{\textless}/inf{\textgreater}(X)){\textless}sup{\textgreater}2{\textless}/sup{\textgreater}u), where $\kappa${\textless}inf{\textgreater}2{\textless}/inf{\textgreater}(X) is the 2-norm condition number of X and u is the unit roundoff. In this paper, we show that if the condition number of X is not too large, we can greatly improve the stability by iterating the Cholesky QR algorithm twice. More specifically, if $\kappa${\textless}inf{\textgreater}2{\textless}/inf{\textgreater}(X) is at most O(u{\textless}sup{\textgreater}-1/2{\textless}/sup{\textgreater} ), both the residual and deviation from orthogonality are shown to be of order O(u). Numerical results support our theoretical analysis.},
author = {Yamamoto, Yusaku and Nakatsukasa, Yuji and Yanagisawa, Yuka and Fukaya, Takeshi},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Yamamoto et al. - 2015 - Roundoff error analysis of the Cholesky QR2 algorithm.pdf:pdf},
issn = {10689613},
journal = {Electronic Transactions on Numerical Analysis},
keywords = {Cholesky QR,Communication-avoiding algorithms,QR decomposition,Roundoff error analysis},
pages = {306--326},
title = {{Roundoff error analysis of the Cholesky QR2 algorithm}},
volume = {44},
year = {2015}
}
@article{Huang2009b,
abstract = {Spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis, image processing and data mining. However, the computational and/or communication resources required by the method in processing large-scale data are often prohibitively high, and practitioners are often required to perturb the original data in various ways (quantization, downsampling, etc) before invoking a spectral algorithm. In this paper, we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering. We show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the Laplacian matrix. From this result we derive approximate upper bounds on the clustering error. We show that this bound is tight empirically across a wide range of problems, suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a specification of permitted loss in clustering performance.},
author = {Huang, Ling and Yan, Donghui and Jordan, Michael I. and Taft, Nina},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Missclassification.pdf:pdf},
isbn = {9781605609492},
journal = {Advances in Neural Information Processing Systems 21 - Proceedings of the 2008 Conference},
number = {December},
pages = {705--712},
title = {{Spectral clustering with perturbed data}},
year = {2009}
}
@article{Robinson2018,
abstract = {This article shows that increasing the observation variance at small scales can reduce the ensemble size required to avoid collapse in particle filtering of spatially extended dynamics and improve the resulting uncertainty quantification at large scales. Particle filter weights depend on how well ensemble members agree with observations, and collapse occurs when a few ensemble members receive most of the weight. Collapse causes catastrophic variance underestimation. Increasing small-scale variance in the observation error model reduces the incidence of collapse by de-emphasizing small-scale differences between the ensemble members and the observations. Doing so smooths the posterior mean, though it does not smooth the individual ensemble members. Two options for implementing the proposed observation error model are described. Taking a discretized elliptic differential operator as an observation error covariance matrix provides the desired property of a spectrum that grows in the approach to small scales. This choice also introduces structure exploitable by scalable computation techniques, including multigrid solvers and multiresolution approximations to the corresponding integral operator. Alternatively the observations can be smoothed and then assimilated under the assumption of independent errors, which is equivalent to assuming large errors at small scales. The method is demonstrated on a linear stochastic partial differential equation, where it significantly reduces the occurrence of particle filter collapse while maintaining accuracy. It also improves continuous ranked probability scores by as much as 25{\%}, indicating that the weighted ensemble more accurately represents the true distribution. The method is compatible with other techniques for improving the performance of particle filters.},
archivePrefix = {arXiv},
arxivId = {1711.06758},
author = {Robinson, Gregor and Grooms, Ian and Kleiber, William},
doi = {10.1175/MWR-D-17-0349.1},
eprint = {1711.06758},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Robinson2018.pdf:pdf},
issn = {15200493},
journal = {Monthly Weather Review},
keywords = {Data assimilation,Filtering techniques,Operational forecasting,Probability forecasts/models/distribution,Statistical forecasting},
number = {8},
pages = {2433--2446},
title = {{Improving particle filter performance by smoothing observations}},
volume = {146},
year = {2018}
}
@article{AilonChazelle,
author = {Ailon, Nir and Chazelle, Bernard},
doi = {10.1137/060673096},
file = {:Users/luciaminahyang/Documents/school/research/Readings/AilonChazelle2006.pdf:pdf},
issn = {0097-5397},
journal = {SIAM Journal on Computing},
keywords = {050647931,10,1137,60f17,65c30,65c40,91a15,91a23,93e25,ams subject classifications,chain approximations,doi,markov,nonzero-sum games,numerical methods,stochastic differential games},
month = {jan},
number = {1},
pages = {302--322},
title = {{The Fast Johnson–Lindenstrauss Transform and Approximate Nearest Neighbors}},
url = {http://epubs.siam.org/doi/10.1137/060673096},
volume = {39},
year = {2009}
}
@article{Coriolis1963,
author = {Coriolis, The and Montgomery, The},
file = {:Users/luciaminahyang/Documents/school/research/Readings/equator.pdf:pdf},
title = {{Chapter 12 Equatorial Dynamics}},
year = {1963}
}
@article{Morrison2005a,
abstract = {Background: Interpretation of simple microarray experiments is usually based on the fold-change of gene expression between a reference and a "treated" sample where the treatment can be of many types from drug exposure to genetic variation. Interpretation of the results usually combines lists of differentially expressed genes with previous knowledge about their biological function. Here we evaluate a method - based on the PageRank algorithm employed by the popular search engine Google - that tries to automate some of this procedure to generate prioritized gene lists by exploiting biological background information. Results: GeneRank is an intuitive modification of PageRank that maintains many of its mathematical properties. It combines gene expression information with a network structure derived from gene annotations (gene ontologies) or expression profile correlations. Using both simulated and real data we find that the algorithm offers an improved ranking of genes compared to pure expression change rankings. Conclusion: Our modification of the PageRank algorithm provides an alternative method of evaluating microarray experimental results which combines prior knowledge about the underlying network. GeneRank offers an improvement compared to assessing the importance of a gene based on its experimentally observed fold-change alone and may be used as a basis for further analytical developments. {\textcopyright} 2005 Morrison et al; licensee BioMed Central Ltd.},
author = {Morrison, Julie L. and Breitling, Rainer and Higham, Desmond J. and Gilbert, David R.},
doi = {10.1186/1471-2105-6-233},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Morrison et al. - 2005 - GeneRank Using search engine technology for the analysis of microarray experiments.pdf:pdf},
issn = {14712105},
journal = {BMC Bioinformatics},
pages = {1--14},
pmid = {16176585},
title = {{GeneRank: Using search engine technology for the analysis of microarray experiments}},
volume = {6},
year = {2005}
}
@article{Higham2019a,
author = {Higham, Nicholas J. and Mary, Theo},
doi = {10.1137/18M1226312},
file = {:Users/luciaminahyang/Documents/school/research/Readings/ProbablisticMaryHigham.pdf:pdf},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
keywords = {65f05,65g50,65y04,ams subject classifications,bfloat16,bit flips,directed rounding,floating-point arithmetic,fp16,half precision,ieee arithmetic,low precision,matlab,mixed precision,round to nearest,rounding error analysis,simulation,stochastic rounding,subnormal numbers},
month = {jan},
number = {5},
pages = {A2815--A2835},
title = {{A New Approach to Probabilistic Rounding Error Analysis}},
url = {https://epubs.siam.org/doi/10.1137/18M1226312},
volume = {41},
year = {2019}
}
@article{Krogstad2005,
abstract = {The integrating factor (IF) method for numerical integration of stiff nonlinear PDEs has the disadvantage of producing large error coefficients when the linear term has large norm. We propose a generalization of the IF method, and in particular construct multistep-type methods with several orders of magnitude improved accuracy. We also consider exponential time differencing (ETD) methods, and point out connections with a particular application of the commutator-free Lie group methods. We present a new fourth order ETDRK method with improved accuracy. The methods considered are compared in several numerical examples. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
author = {Krogstad, Stein},
doi = {10.1016/j.jcp.2004.08.006},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Krogstad.pdf:pdf},
issn = {00219991},
journal = {Journal of Computational Physics},
keywords = {Exponential time differencing,Integrating factor methods,Lie group methods,Stiff systems},
number = {1},
pages = {72--88},
title = {{Generalized integrating factor methods for stiff PDEs}},
volume = {203},
year = {2005}
}
@book{Watkins2002,
abstract = {In this second edition), one finds, following the preface, the following modifications or additions. The last chapter seven devoted to iterative methods for solving large sparse linear systems of equations (with a main section on the conjugate-gradient method for positive symmetric definite systems and other sections on classical iteration, preconditioners, Krylov subspace methods) has been added. Other added topics are about large, sparse eigenvalue problems, error analysis in Gaussian QR decomposition, symmetric eigenvalue problems. The singular value decomposition is introduced earlier. One finds more application examples combined with a greater emphasis on the MATLAB software tools.},
author = {Watkins, David S},
file = {:Users/luciaminahyang/Documents/school/1617/numerics/david{\_}s-{\_}watkins{\_}fundamentals{\_}of{\_}matrix{\_}computat.pdf:pdf},
isbn = {0471213942},
pages = {618},
title = {{Fundamentals of Matrix Computations, 2nd Ed.}},
year = {2002}
}
@article{Yu2017,
abstract = {Principal component analysis (PCA) is a fundamental dimension reduction tool in statistics and machine learning. For large and high-dimensional data, computing the PCA (i.e., the top singular vectors of the data matrix) becomes a challenging task. In this work, a single-pass randomized algorithm is proposed to compute PCA with only one pass over the data. It is suitable for processing extremely large and high-dimensional data stored in slow memory (hard disk) or the data generated in a streaming fashion. Experiments with synthetic and real data validate the algorithm's accuracy, which has orders of magnitude smaller error than an existing single-pass algorithm. For a set of high-dimensional data stored as a 150 GB file, the algorithm is able to compute the first 50 principal components in just 24 minutes on a typical 24-core computer, with less than 1 GB memory cost.},
archivePrefix = {arXiv},
arxivId = {1704.07669},
author = {Yu, Wenjian and Gu, Yu and Li, Jian and Liu, Shenghua and Li, Yaohang},
doi = {10.24963/ijcai.2017/468},
eprint = {1704.07669},
file = {:Users/luciaminahyang/Documents/school/research/Readings/YuGu.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {domized algorithm,high-dimensional data,pca,principal component analysis,ran-,single-pass algorithm,truncated singular value decomposi-},
pages = {3350--3356},
title = {{Single-pass PCA of large high-dimensional data}},
year = {2017}
}
@article{Lurie1999,
abstract = {This monograph is an intertwined tale of eigenvalues and their use in unlocking a thousand secrets about graphs. The stories will be told how the spectrum reveals fundamental properties of a graph, how spectral graph theory links the discrete universe to the continuous one through geometric, analytic and algebraic techniques, and how, through eigenvalues, theory and applications in communications and computer science come together in symbiotic harmony....},
author = {Lurie, Jacob},
doi = {10.1145/568547.568553},
file = {:Users/luciaminahyang/Documents/school/research/Readings/spectral graph/FChung01.pdf:pdf},
issn = {0163-5700},
journal = {ACM SIGACT News},
number = {2},
pages = {14--16},
title = {{ Review of Spectral Graph Theory }},
volume = {30},
year = {1999}
}
@article{Nathan2017,
abstract = {Graphs and networks are prevalent in modeling relational datasets from many fields of research. By using iterative solvers to approximate graph measures (specifically Katz Centrality), we can obtain a ranking vector consisting of a number for each vertex in the graph identifying its relative importance. We use the residual to accurately estimate how much of the ranking from an approximate solution matches the ranking given by the exact solution. Using probabilistic matrix norms and applying numerical analysis to the computation of Katz Centrality, we obtain bounds on the accuracy of the approximation compared to the exact solution with respect to the highly ranked nodes. This relates the numerical accuracy of the linear solver to the data analysis accuracy of finding the correct ranking. In particular, we answer the question of which pairwise rankings are reliable given an approximate solution to the linear system. Experiments on many real-world networks up to several million vertices and several hundred million edges validate our theory and show that we are able to accurately estimate large portions of the approximation. By analyzing convergence error, we develop confidence in the ranking schemes of data mining.},
author = {Nathan, Eisha and Sanders, Geoffrey and Fairbanks, James and Henson, Van Emden and Bader, David A.},
doi = {10.1016/j.procs.2017.05.021},
file = {:Users/luciaminahyang/Documents/school/research/Readings/NathanSanders.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {data analysis,graphs,katz centrality,numerical accuracy,ranking},
pages = {68--78},
title = {{Graph Ranking Guarantees for Numerical Approximations to Katz Centrality}},
volume = {108},
year = {2017}
}
@article{Bathe2013,
abstract = {The objective in this paper is to present some recent developments regarding the subspace iteration method for the solution of frequencies and mode shapes. The developments pertain to speeding up the basic subspace iteration method by choosing an effective number of iteration vectors and by the use of parallel processing. The subspace iteration method lends itself particularly well to shared and distributed memory processing. We present the algorithms used and illustrative sample solutions. The present paper may be regarded as an addendum to the publications presented in the early 1970s, see Refs. [1,2], taking into account the changes in computers that have taken place. {\textcopyright} 2012 Elsevier Ltd.},
author = {Bathe, Klaus J{\"{u}}rgen},
doi = {10.1016/j.compstruc.2012.06.002},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Bathe - 2013 - The subspace iteration method - Revisited.pdf:pdf},
issn = {00457949},
journal = {Computers and Structures},
keywords = {Distributed memory,Frequencies,Mode shapes,Parallel processing,Shared memory,Subspace iteration method},
number = {1},
pages = {177--183},
title = {{The subspace iteration method - Revisited}},
volume = {126},
year = {2013}
}
@article{Tropp2017,
abstract = {This paper describes a suite of algorithms for constructing low-rank approximations of an input matrix from a random linear image, or sketch, of the matrix. These methods can preserve structural properties of the input matrix, such as positive-semidefiniteness, and they can produce approximations with a user-specied rank. The algorithms are simple, accurate, numerically stable, and provably correct. Moreover, each method is accompanied by an informative error bound that allows users to select parameters a priori to achieve a given approximation quality. These claims are supported by numerical experiments with real and synthetic data.},
archivePrefix = {arXiv},
arxivId = {1609.00048},
author = {Tropp, Joel A. and Yurtsever, Alp and Udell, Madeleine and Cevher, Volkan},
doi = {10.1137/17M1111590},
eprint = {1609.00048},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Tropp2018.pdf:pdf},
issn = {10957162},
journal = {SIAM Journal on Matrix Analysis and Applications},
keywords = {Dimension reduction,Matrix approximation,Numerical linear algebra,Randomized algorithm,Single-pass algorithm,Sketching,Streaming algorithm,Subspace embedding},
number = {4},
pages = {1454--1485},
title = {{Practical sketching algorithms for low-rank matrix approximation}},
volume = {38},
year = {2017}
}
@article{Mooney2012,
abstract = {A new method for analyzing molecular dynamics simulation data is employed to study the solvent shell structure and exchange processes of mono-, di-, and trivalent metal cations in water. The instantaneous coordination environment is characterized in terms of the coordinating waters' H-bonding network, orientations, mean residence times, and the polyhedral configuration. The graph-theory-based algorithm provides a rapid frame-by-frame identification of polyhedra and reveals fluctuations in the solvation shell shape-previously unexplored dynamic behavior that in many cases can be associated with the exchange reactions of water between the first and second solvation shells. Extended solvation structure is also analyzed graphically, revealing details of the hydrogen bonding network that have practical implications for connecting molecular dynamics data to ab initio cluster calculations. Although the individual analyses of water orientation, residence time, etc., are commonplace in the literature, their combination with graphical algorithms is new and provides added chemical insight. {\textcopyright} 2012 American Chemical Society.},
author = {Mooney, Barbara Logan and Corrales, L. Rene and Clark, Aurora E.},
doi = {10.1021/jp300193j},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Mooney, Corrales, Clark - 2012 - Novel analysis of cation solvation using a graph theoretic approach.pdf:pdf},
issn = {15205207},
journal = {Journal of Physical Chemistry B},
number = {14},
pages = {4263--4275},
title = {{Novel analysis of cation solvation using a graph theoretic approach}},
volume = {116},
year = {2012}
}
@article{Minchev2005,
author = {Minchev, Borislav V and Wright, Will M},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Minchev{\_}Wright{\_}TechReport{\_}05.pdf:pdf;:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Minchev, Wright - 2005 - UNIVERSITET A review of exponential integrators for first order semi-linear problems by PREPRINT NUMERICS NO ..pdf:pdf},
journal = {Science And Technology},
title = {{UNIVERSITET A review of exponential integrators for first order semi-linear problems by PREPRINT NUMERICS NO . 2 / 2005 NORWEGIAN UNIVERSITY OF SCIENCE AND TECHNOLOGY}},
year = {2005}
}
@book{Mayers2005,
author = {Mayers, K. W. Morton and DAVID},
file = {:Users/luciaminahyang/Documents/school/research/Readings/MortonMayers.pdf:pdf},
isbn = {9780521607933},
title = {{Morton{\_}Numerical Solution of PDE}},
year = {2005}
}
@article{Charikar2004,
abstract = {We present a 1-pass algorithm for estimating the most frequent items in a data stream using limited storage space. Our method relies on a data structure called a COUNT SKETCH, which allows us to reliably estimate the frequencies of frequent items in the stream. Our algorithm achieves better space bounds than the previously known best algorithms for this problem for several natural distributions on the item frequencies. In addition, our algorithm leads directly to a 2-pass algorithm for the problem of estimating the items with the largest (absolute) change in frequency between two data streams. To our knowledge, this latter problem has not been previously studied in the literature. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
author = {Charikar, Moses and Chen, Kevin and Farach-Colton, Martin},
doi = {10.1016/S0304-3975(03)00400-6},
file = {:Users/luciaminahyang/Documents/school/research/Readings/CharikarChenColton.pdf:pdf;:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Charikar, Chen, Farach-Colton - 2004 - Finding frequent items in data streams.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {Approximation,Frequent items,Streaming algorithm},
number = {1},
pages = {3--15},
title = {{Finding frequent items in data streams}},
volume = {312},
year = {2004}
}
@article{Haidar2019,
abstract = {Low-precision floating-point arithmetic is a powerful tool for accelerating scientific computing applications, especially those in artificial intelligence. Here, we present an investigation showing that other high-performance computing (HPC) applications can also harness this power. Specifically, we use the general HPC problem, Ax b, where A is a large dense matrix, and a double precision (FP64) solution is needed for accuracy. Our approach is based on mixed-precision (FP16-FP64) iterative refinement, and we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly tuned implementations. These new methods show how using half-precision Tensor Cores (FP16-TC) for the arithmetic can provide up to 4× speedup. This is due to the performance boost that the FP16-TC provide as well as to the improved accuracy over the classical FP16 arithmetic that is obtained because the GEMM accumulation occurs in FP32 arithmetic.},
author = {Haidar, Azzam and Tomov, Stanimire and Dongarra, Jack and Higham, Nicholas J.},
doi = {10.1109/SC.2018.00050},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Haidar et al. - 2019 - Harnessing GPU Tensor cores for fast FP16 arithmetic to speed up mixed-precision iterative refinement solvers.pdf:pdf},
isbn = {9781538683842},
journal = {Proceedings - International Conference for High Performance Computing, Networking, Storage, and Analysis, SC 2018},
keywords = {FP16 Arithmetic,GPU Computing,Half Precision,Iterative Refinement Computation,Linear Algebra,Mixed Precision Solvers},
pages = {603--613},
title = {{Harnessing GPU Tensor cores for fast FP16 arithmetic to speed up mixed-precision iterative refinement solvers}},
year = {2019}
}
@misc{Vemulapati2008,
author = {Vemulapati, Udaya Bhaskar},
booktitle = {Encyclopedia of Optimization},
doi = {10.1007/978-0-387-74759-0_533},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Vemulapati - 2008 - QR Factorization.htm:htm},
pages = {3116--3119},
title = {{QR Factorization}},
year = {2008}
}
@article{Demmel2007a,
abstract = {In Demmel et al. (Numer. Math. 106(2), 199-224, 2007) we showed that a large class of fast recursive matrix multiplication algorithms is stable in a normwise sense, and that in fact if multiplication of n-by-n matrices can be done by any algorithm in O(n $\omega$+$\eta$ ) operations for any $\eta$ {\textgreater} 0, then it can be done stably in O(n $\omega$+$\eta$ ) operations for any $\eta$ {\textgreater} 0. Here we extend this result to show that essentially all standard linear algebra operations, including LU decomposition, QR decomposition, linear equation solving, matrix inversion, solving least squares problems, (generalized) eigenvalue problems and the singular value decomposition can also be done stably (in a normwise sense) in O(n $\omega$+$\eta$ ) operations. {\textcopyright} 2007 Springer-Verlag.},
archivePrefix = {arXiv},
arxivId = {math/0612264},
author = {Demmel, James and Dumitriu, Ioana and Holtz, Olga},
doi = {10.1007/s00211-007-0114-x},
eprint = {0612264},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Demmel, Dumitriu, Holtz - 2007 - Fast linear algebra is stable.pdf:pdf},
issn = {0029599X},
journal = {Numerische Mathematik},
number = {1},
pages = {59--91},
primaryClass = {math},
title = {{Fast linear algebra is stable}},
volume = {108},
year = {2007}
}
@article{Clarkson2013a,
abstract = {We design a new distribution over poly(r$\epsilon$-1) × n matrices S so that for any fixed n×d matrix A of rank r, with probability at least 9/10, SAx 2 = (1± $\epsilon$) Ax 2 simultaneously for all x ∈ Rd. Such a matrix S is called a subspace embedding. Furthermore, SA can be computed in O(nnz(A))time, where nnz(A) is the number of non-zero entries of A. This improves over all previous subspace embeddings, which required at least $\Omega$(nd log d) time to achieve this property. We call our matrices S sparse embedding matrices. Using our sparse embedding matrices, we obtain the fastest known algorithms for overconstrained least-squares regression, low-rank approximation, approximating all leverage scores, and p-regression: • to output an xfor which Ax - b 2 ≤ (1 + $\epsilon$)min x Ax - b 2 for an n × d matrix A and an n × 1 column vector b, we obtain an algorithm running in O(nnz(A)) + ̃O (d3$\epsilon$-2) time, and another in O(nnz(A) log(1/$\epsilon$)) + ̃O (d3 log(1/$\epsilon$)) time. (Here ̃O(f) = f {\textperiodcentered} logO(1)(f).) • to obtain a decomposition of an n×n matrix A into a product of an n×k matrix L, a k ×k diagonal matrix D, and a n × k matrix W, for which A - LDW F ≤ (1 + $\epsilon$) A - Ak F , where Ak is the best rank-k approximation, our algorithm runs in O(nnz(A)) +̃O(nk2$\epsilon$-4 log n+k3$\epsilon$-5 log 2 n) time. • to output an approximation to all leverage scores of an n×d input matrix A simultaneously, with constant relative error, our algorithms run in O(nnz(A) log n)+ ̃O (r3) time. • to output an x for which Ax - b p ≤ (1 + $\epsilon$)min x Ax - b p for an n × d matrix A and an n × 1 column vector b, we obtain an algorithm running in O(nnz(A) log n) + poly(r$\epsilon$-1) time, for any constant 1 ≤ p {\textless} ∞. We optimize the polynomial factors in the above stated running times, and show various tradeoffs. Finally, we provide preliminary experimental results which suggest that our algorithms are of interest in practice. Copyright 2013 ACM.},
archivePrefix = {arXiv},
arxivId = {1207.6365},
author = {Clarkson, Kenneth L. and Woodruff, David P.},
doi = {10.1145/2488608.2488620},
eprint = {1207.6365},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Clarkson, Woodruff - 2013 - Low rank approximation and regression in input sparsity time.pdf:pdf},
isbn = {9781450320290},
issn = {07378017},
journal = {Proceedings of the Annual ACM Symposium on Theory of Computing},
keywords = {Algorithms,Theory},
pages = {81--90},
title = {{Low rank approximation and regression in input sparsity time}},
year = {2013}
}
@article{Paige1995,
abstract = {Approximations to the solution of a large sparse symmetric system of equations are considered. The conjugate gradient and minimum residual approximations are studied without reference to their computation. Several different bases for the associated Krylov subspace are used, including the usual Lanczos basis. The zeros of the iteration polynomial for the minimum residual approximation (harmonic Ritz values) are characterized in several ways and, in addition, attractive convergence properties are established. The connection of these harmonic Ritz values to Lehmann's optimal intervals for eigenvalues of the original matrix appears to be new. Copyright {\textcopyright} 1995 John Wiley {\&} Sons, Ltd},
author = {Paige, Chris C. and Parlett, Beresford N. and van der Vorst, Henk A.},
doi = {10.1002/nla.1680020205},
file = {:Users/luciaminahyang/Documents/school/research/Readings/PaigeParlettVV.pdf:pdf},
issn = {10991506},
journal = {Numerical Linear Algebra with Applications},
keywords = {Krylov subspace,Lanczos process,Lehmann intervals,conjugate gradients,minimum residual,symmetric matrix},
number = {2},
pages = {115--133},
title = {{Approximate solutions and eigenvalue bounds from Krylov subspaces}},
volume = {2},
year = {1995}
}
@article{Mori2012,
abstract = {The AllReduce algorithm is a promising new algorithm for parallelizing the Householder QR decomposition A = QR of a tall and skinny matrix. It divides the input matrix A vertically in a recursive manner, computes the QR decompositions of each submatrix independently, and merges the results to obtain the QR decomposition of A. While this algorithm has been shown to achieve excellent speedup in various parallel environments, its rounding error properties have not been elucidated yet. In this paper, we present theoretical error analysis of the AllReduce algorithm. Specifically, we derive bounds for the backward error of A and deviation from orthogonality of the computed Q factor. Our analysis shows that both of these bounds are smaller than their counterparts for the conventional Householder QR algorithm. Moreover, the bounds decrease as the number of submatrices increases. These results are supported by numerical experiments. Thus we can conclude that the AllReduce algorithm can be used as a reliable method of orthogonalization in parallel environments. {\textcopyright} The JJIAM Publishing Committee and Springer 2011.},
author = {Mori, Daisuke and Yamamoto, Yusaku and Zhang, Shao Liang},
doi = {10.1007/s13160-011-0053-x},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Mori, Yamamoto, Zhang - 2012 - Backward error analysis of the AllReduce algorithm for householder QR decomposition.pdf:pdf},
issn = {09167005},
journal = {Japan Journal of Industrial and Applied Mathematics},
keywords = {AllReduce algorithm,Error analysis,Householder transformation,Parallel algorithm,QRdecomposition},
number = {1},
pages = {111--130},
title = {{Backward error analysis of the AllReduce algorithm for householder QR decomposition}},
volume = {29},
year = {2012}
}
@article{Mary2015,
abstract = {A low-rank approximation of a dense matrix plays an important role in many applications. To compute such an approximation, a common approach uses the QR factorization with column pivoting (QRCP). Though the reliability and efficiency of QRCP have been demonstrated, this deterministic approach requires costly communication at each step of the factorization. Since such communication is becoming increasingly expensive on modern computers, an alternative approach based on random sampling, which can be implemented using communication-optimal kernels, is becoming attractive. To study its potential, in this paper, we compare the performance of random sampling with that of QRCP on an NVIDIA Kepler GPU. Our performance results demonstrate that random sampling can be up to 12.8x faster than the deterministic approach for computing the approximation of the same accuracy. We also present the parallel scaling of the random sampling over multiple GPUs on a single compute node, showing a speedup of 3.8x over three Kepler GPUs. These results demonstrate the potential of the random sampling as an excellent computational tool for many applications, and its potential is likely to grow on the emerging computers with the increasing communication costs.},
author = {Mary, Th{\'{e}}o and Yamazaki, Ichitaro and Kurzak, Jakub and Luszczek, Piotr and Tomov, Stanimire and Dongarra, Jack},
doi = {10.1145/2807591.2807613},
file = {:Users/luciaminahyang/Documents/school/research/Readings/MaryYamazaki2015.pdf:pdf},
isbn = {9781450337236},
issn = {21674337},
journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
title = {{Performance of random sampling for computing low-rank approximations of a dense matrix on GPUs}},
volume = {15-20-Nove},
year = {2015}
}
@article{Boutsidis2015,
abstract = {Spectral clustering is one of the most important algorithms in data mining and machine intelligence; however, its computational complexity limits its application to truly large scale data analysis. The computational bottleneck in spectral clustering is computing a few of the top eigenvectors of the (normalized) Laplacian matrix corresponding to the graph representing the data to be clustered. One way to speed up the computation of these eigenvectors is to use the "power method" from the numerical linear algebra literature. Although the power method has been empirically used to speed up spectral clustering, the theory behind this approach, to the best of our knowledge, remains unexplored. This paper provides the first such rigorous theoretical justification, arguing that a small number of power iterations suffices to obtain near-optimal partitionings using the approximate eigenvectors. Specifically, we prove that solving the $\kappa$-means clustering problem on the approximate eigenvectors obtained via the power method gives an additive-error approximation to solving the k-means problem on the optimal eigenvectors.},
archivePrefix = {arXiv},
arxivId = {1311.2854},
author = {Boutsidis, Christos and Gittens, Alex and Kambadur, Prabhanjan},
eprint = {1311.2854},
file = {:Users/luciaminahyang/Documents/school/research/Readings/spectral graph/Boutsidis2015.pdf:pdf},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
pages = {40--48},
title = {{Spectral clustering via the power method - Provably}},
volume = {1},
year = {2015}
}
@article{Sobel2010,
abstract = {The authors argue for the hypothesis that interactive feedbacks involving surface enthalpy fluxes are important to the dynamics of tropical intraseasonal variability. These include cloud-radiative feedbacks as well as surface turbulent flux feedbacks; the former effectively act to transport enthalpy from the ocean to the atmosphere, as do the latter. Evidence in favor of this hypothesis includes the observed spatial distribution of intraseasonal variance in precipitation and outgoing longwave radiation, the observed relationship between intraseasonal latent heat flux and precipitation anomalies in regions where intraseasonal variability is strong, and sensitivity experiments performed with a small number of general circulation and idealized models. The authors argue that it would be useful to assess the importance of surface fluxes to intraseasonal variability in a larger number of comprehensive numerical models. Such an assessment could provide insight into the relevance of interactive surface fluxes to real intraseasonal variability, perhaps making it possible to rule out either theoretical explanations in which surface fluxes are crucial, or those in which they are not.},
author = {Sobel, Adam H. and Maloney, Eric D. and Bellon, Gilles and Frierson, Dargan M.},
doi = {10.3894/james.2010.2.2},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Sobel{\_}et{\_}al-2010-Journal{\_}of{\_}Advances{\_}in{\_}Modeling{\_}Earth{\_}Systems.pdf:pdf},
issn = {1942-2466},
journal = {Journal of Advances in Modeling Earth Systems},
keywords = {Atmospheric Dynamics,Climate Dynamics,Climate Modeling,General Circulation Modeling},
title = {{Surface fluxes and tropical intraseasonal variability: A reassessment}},
volume = {2},
year = {2010}
}
@article{Beylkin1998,
abstract = {We consider issues of stability of time-discretization schemes withexacttreatment of thelinear part(ELP schemes) for solving nonlinear PDEs. A distinctive feature of ELP schemes is the exact evaluation of the contribution of the linear term, that is if the nonlinear term of the equation is zero, then the scheme reduces to the evaluation of the exponential function of the operator representing the linear term. Computing and applying the exponential or other functions of operators with variable coefficients in the usual manner requires evaluating dense matrices and is highly inefficient. It turns out that computing the exponential of strictly elliptic operators in the wavelet system of coordinates yields sparse matrices (for a finite but arbitrary accuracy). This observation makes our approach practical in a number of applications. In particular, we consider applications of ELP schemes to advection-diffusion equations. We study the stability of these schemes and show that both explicit and implicit ELP schemes have distinctly different stability properties if compared with known implicit-explicit schemes. For example, we describe explicit schemes with stability regions similar to those of typical implicit schemes used for solving advection-diffusion equations. {\textcopyright} 1998 Academic Press.},
author = {Beylkin, Gregory and Keiser, James M. and Vozovoi, Lev},
doi = {10.1006/jcph.1998.6093},
file = {:Users/luciaminahyang/Documents/school/research/Readings/BeylkinKaiserVozovoi1998.pdf:pdf},
issn = {00219991},
journal = {Journal of Computational Physics},
number = {2},
pages = {362--387},
title = {{A new class of time discretization schemes for the solution of nonlinear PDEs}},
volume = {147},
year = {1998}
}
@article{Vallis2006,
abstract = {applicability for this approach.},
author = {Vallis, Geoffrey K.},
doi = {10.1017/cbo9780511790447},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Vallis{\_}AOFD{\_}NewMaterial.pdf:pdf},
journal = {Atmospheric and Oceanic Fluid Dynamics},
title = {{Atmospheric and Oceanic Fluid Dynamics}},
year = {2006}
}
@book{Wilkinson1981a,
abstract = {According to Parlett, ‘Vibrations are everywhere, and so too are the eigenvalues associated with them. As mathematical models invade more and more disciplines, we can anticipate a demand for eigenvalue calculations in an ever richer variety of contexts'. Anyone who performs these calculations will welcome the reprinting of Parlett's book (originally published in 1980). In this unabridged, amended version, Parlett covers aspects of the problem that are not easily found elsewhere. The chapter titles convey the scope of the material succinctly. The aim of the book is to present mathematical knowledge that is needed in order to understand the art of computing eigenvalues of real symmetric matrices, either all of them or only a few. The author explains why the selected information really matters and he is not shy about making judgments. The commentary is lively but the proofs are terse.},
author = {Wilkinson, James H. and Parlett, Beresford N.},
booktitle = {Mathematics of Computation},
doi = {10.2307/2007453},
file = {:Users/luciaminahyang/Library/Application Support/Mendeley Desktop/Downloaded/Wilkinson, Parlett - 1981 - The Symmetric Eigenvalue Problem.pdf:pdf},
isbn = {9780898714029},
issn = {00255718},
number = {156},
pages = {599},
title = {{The Symmetric Eigenvalue Problem.}},
volume = {37},
year = {1981}
}
@article{Mezzadri2006,
abstract = {We discuss how to generate random unitary matrices from the classical compact groups U(N), O(N) and USp(N) with probability distributions given by the respective invariant measures. The algorithm is straightforward to implement using standard linear algebra packages. This approach extends to the Dyson circular ensembles too. This article is based on a lecture given by the author at the summer school on Number Theory and Random Matrix Theory held at the University of Rochester in June 2006. The exposition is addressed to a general mathematical audience.},
archivePrefix = {arXiv},
arxivId = {math-ph/0609050},
author = {Mezzadri, Francesco},
eprint = {0609050},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Mezzadri2006.pdf:pdf},
issn = {1088-9477},
primaryClass = {math-ph},
title = {{How to generate random matrices from the classical compact groups}},
url = {http://arxiv.org/abs/math-ph/0609050},
year = {2006}
}
@article{Majda1997,
abstract = {A family of one-dimensional nonlinear dispersive wave equations is introduced as a model for assessing the validity of weak turbulence theory for random waves in an unambiguous and transparent fashion. These models have an explicitly solvable weak turbulence theory which is developed here, with Kolmogorov-type wave number spectra exhibiting interesting dependence on parameters in the equations. These predictions of weak turbulence theory are compared with numerical solutions with damping and driving that exhibit a statistical inertial scaling range over as much as two decades in wave number. It is established that the quasi-Gaussian random phase hypothesis of weak turbulence theory is an excellent approximation in the numerical statistical steady state. Nevertheless, the predictions of weak turbulence theory fail and yield as much flatter (|k|-1/3) spectrum compared with the steeper (|k|-3/4) spectrum observed in the numerical statistical steady state. The reasons for the failure of weak turbulence theory in this context are elucidated here. Finally, an inertial range closure and scaling theory is developed which successfully predicts the inertial range exponents observed in the numerical statistical steady states.},
author = {Majda, A. J. and McLaughlin, D. W. and Tabak, E. G.},
doi = {10.1007/BF02679124},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Majda1997{\_}Article{\_}AOne-dimensionalModelForDisper.pdf:pdf},
issn = {09388974},
journal = {Journal of Nonlinear Science},
keywords = {Cascades,Inertial range,Turbulence},
number = {1},
pages = {9--44},
title = {{A One-Dimensional Model for Dispersive Wave Turbulence}},
volume = {7},
year = {1997}
}
@article{Majda2009,
abstract = {The Madden-Julian oscillation (MJO) is the dominant mode of variability in the tropical atmosphere on intraseasonal timescales and planetary spatial scales. Despite the primary importance of the MJO and the decades of research progress since its original discovery, a generally accepted theory for its essential mechanisms has remained elusive. Here, we present a minimal dynamical model for the MJO that recovers robustly its fundamental features (i.e., its "skeleton") on intraseasonal/planetary scales: (i) the peculiar dispersion relation of d?/dk ≈ 0, (ii) the slow phase speed of ≈5 m/s, and (iii) the horizontal quadrupole vortex structure. This is accomplished here in a model that is neutrally stable on planetary scales; i.e., it is tacitly assumed that the primary instabilities occur on synoptic scales. The key premise of the model is that modulations of synoptic scale wave activity are induced by low-level moisture preconditioning on planetary scales, and they drive the "skeleton" of the MJO through modulated heating. The "muscle" of the MJO-including tilts, vertical structure, etc. - is contributed by other potential upscale transport effects from the synoptic scales.},
author = {Majda, Andrew J. and Stechmann, Samuel N.},
doi = {10.1073/pnas.0903367106},
file = {:Users/luciaminahyang/Documents/school/research/Readings/Majda{\_}Stechmann{\_}PNAS{\_}09.pdf:pdf},
isbn = {0903367106},
issn = {00278424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Atmospheric convection,Convectively coupled equatorial waves,Madden-Julian oscillation},
number = {21},
pages = {8417--8422},
title = {{The skeleton of tropical intraseasonal oscillations}},
volume = {106},
year = {2009}
}
