Let us first consider rounding errors incurred from carrying out HQR in high precision, then cast down at the very end.
This could be useful in applications that require economical storage but have enough memory to carry out HQR in higher precision, or in block algorithms as will be shown in \cref{sec:mp-3,sec:mp-2}.
Consider two floating point types $\F_{low}$ and $\F_{high}$ where $\F_{l}\subseteq \F_{h}$, and for all $x,y\in\F_{l}$, the exact product $xy$ can be represented in $\F_{h}$.
Some example pairs of $\{\F_{l}, \F_{h}\}$ include $\{\text{fp16}, \text{fp32}\}$, $\{\text{fp32}, \text{fp64}\}$, and $\{\text{fp16}, \text{fp64}\}$.
Suppose that the matrix to be factorized is stored with low precision numbers, $\bb{A}\in\F_{l}^{m\times n}$.
Casting up adds no rounding errors, so we can directly apply the analysis that culminated in \cref{thm:feHQR}, and we only consider the columnwise forward error in the $\bb{Q}$ factor.
Then, the $j^{th}$ column of $\hat{\bb{Q}}_{HQR} = \bb{Q} + \bb{\Delta Q}_{HQR}$ is bounded normwise via $\|\bb{\Delta Q}_{HQR}[:,j]\|_2 \leq n\tilde{\gamma}_{m}^{high},$ and incurs an extra rounding error when $\bb{Q}\in\F_{high}^{m\times n}$ is cast down to $F_{low}^{m\times n}$.
Consider the backward error of a cast down operation represented by a linear transformation, 

First, consider casting down a vector $\bb{x}^h\in\F_h^m$ to $\F_l^m$.
We see that
\begin{equation}
	\bb{x}^l := \text{\tt castdown}(\bb{x}^h) = \bb{I}_{l}\bb{x}^h = (\bb{I}+\bb{E})\bb{x}^h = \bb{x}^h+\bb{\Delta x},
\end{equation}
where $|\bb{\Delta x}| \leq u^l |\bb{x}^h|$ and  $\|\bb{\Delta x}\|_2 \leq u^l \|\bb{x}^h\|_2$.
Then, $\bb{E} = \bb{\Delta x x}^{\top}/\|\bb{x}\|_2^2$ and we can use the same argument as in \cref{eqn:outer} to form a backward matrix norm bound, 
\begin{equation}
	\|\bb{E}\|_F\leq u^l. \label{eqn:castdown}
\end{equation}
Using this in \cref{lem:3.7} to analyze the forward norm error for the $j^{th}$ column of the $\bb{Q}$ factor computed with \cref{algo:hhQR} yields
\begin{equation}
	\|\text{\tt castdown}(\hat{\bb{Q}}_{HQR}[:,j]) - \bb{Q}[:,j]\|_2 = \|\bb{I}_l\hat{\bb{P_1}}\cdots\hat{\bb{P_n}}\hat{e}_j\|_2 \leq u^l+n\tilde{\gamma}_m^h.\label{eqn:HQR-mp}
\end{equation}
To convert this bound to the lower precision, we define function $d$,
\begin{equation}
d(m,u^h, q,u^l) := \lceil (qu^l+mu^h)/u^l\rceil = \cO(q+mu^h/u^l),\label{eqn:d}
\end{equation} 
so that if $\|\hat{\bb{x}}-\bb{x}\|_2 \leq \gamma_m^h$, then $\|\text{\tt castdown}(\hat{\bb{x}})-\bb{x}\|_2 \leq \gamma_{d(m,u^h, q,u^l)}^l$.|
This is a looser bound but it allows us to easily compare the errors to the uniform, low precision implementation of forming $\hat{\bb{x}}$.
%A looser upper bound is given by $\tilde{\gamma}_d^l$ where $d = \lceil u^l_+mu^h/u^l\rceil$ and we benefit from being able to represent it in terms of the low precision . 
%Additionally, we can use this to identify a function that allows us to formulate the new error bound after applying a castdown operation to vectors whose error bounds were known in high precision,
%Let $d = \lceil nmu^h/u^l\rceil$, so that $nmu^h\leq du^l$, and there exists some $r\leq \lfloor u^h/u^l \rfloor$ so that $nmu^h = (d-1)u^l+ru^h$.
%This $d$ value allows us to convert the error in terms of the higher precision to the lower precision while also adding in the extra rounding error that may be incurred in the casting down step. 
%Then, the componentwise error is bounded by
%\begin{equation}
%	|\text{{\tt castdown}}(\bb{\Delta Q}_{HQR}[i,j])| \leq \frac{cnmu^{h}}{1-cmu^{h}} \leq \frac{cdu^{l}}{1-cmu^h} \leq \tilde{\gamma}_{d}^l,
%\end{equation}
%and the columnwise error is 
%\begin{equation}
%	\|\text{{\tt castdown}}(\bb{\Delta Q}_{HQR}[:,j])\|_2 \leq \left(\sum_{i=1}^m |\tilde{\tth}_d^l|^2 \right)^{1/2} \leq \sqrt{m}\gamma_d^l
%\end{equation}
%Since $\hat{\bb{Q}}_{HQR}$ should be almost orthogonal with respect to the higher precision, we can expect all components to be within the dynamic range of $\F_{low}$.
%In \ref{sec:mp-f}, we look at the rounding errors incurred from carrying out a QR factorization in a high precision, then cast down at the very end.
%Since this requires only one cast down operation, this is very similar to the results from the standard uniform precision analysis.

We can easily apply the operator $\bb{I}^l$ to cast down the QR factorizations computed via BQR and TSQR to find the forward matrix norm error on the $\bb{Q}$ factor as shown below,
\begin{align*}
	\|\hat{\bb{Q}}_{BQR}\|_F&\leq u^l+n\tilde{\gamma}_m^h \leq \tilde{\gamma}_{d(nm,u^h,u^l)}^l,\\
	\|\hat{\bb{Q}}_{TSQR}\|_F&\leq u^l+n(L\tilde{\gamma}_{2n}^h+\tilde{\gamma}_{m2^{-L}}^h) \leq \tilde{\gamma}_{d(n(L2n+m2^{-L}),u^h,1,u^l)}^l.
\end{align*}
In the next sections, we consider performing BQR and TSQR with FLOPs within a block and/or a level in high precision, but cast down to low precision in between blocks in \ref{sec:mp-3}.
Finally, we consider all 3 algorithms with an ad hoc mixed precision setting where inner products are performed in high precision and all other operations are computed in low precision in \ref{sec:mp-2}.
%\subsection{Round down at the end of the factorization}\label{sec:mp-f}
%Results in this section are quite straight forward, but 
%\subsubsection{HQR}
\subsection{Round down at block-level (BLAS-3)}\label{sec:mp-3}
We use \cref{eqn:HQR-mp} to study the errors for BQR and TSQR in the case that each block QR factorized by HQR then cast down.
\subsubsection{Round down at block level: BQR}\label{sec:mp-3b}
Let us consider a setting in which only $M$ blocks of width $r$ can be loaded onto memory.
Then, lines 2-6 of \cref{algo:blockHQR} can be modified via \cref{algo:mpBQR}.
\begin{algorithm2e}
	$q = N/M$\tcc*{Note that $n=Nr=qMr$.}
	\For{$q'=1:q$}{
		\If {$q'>2$} {
				Update $[\bb{C_{(q'-1)M+1}}\cdots\bb{C_{qM}}]$ with WY updates from blocks $1:(q'-1)M$.
		}
		\For{$k=1:M$}{
		Apply HQR to $\bb{C_{(q'-1)M+k}}$\;
		Form WY update for $\bb{C_{(q'-1)M+k}}$\;
		WY update blocks to the right, $[\bb{C_{(q'-1)M+k+1}}\cdots \bb{C_{q'M}}]$.
	}
}
\caption{\label{algo:mpBQR} A portion of a mixed precision BQR: modifying first for-loop in \cref{algo:blockHQR}.}
\end{algorithm2e}
We now impose a mixed-precision setting where the inner for-loop in \cref{algo:mpBQR} is performed in high precision, but the WY updates for the outer loop is stored in low precision and only $M$ blocks is updated at a time due to the memory constraint.
These low precision WY updates would be used to build the $\bb{Q}$ factor serially in groups of $M$.
Then, the $j^{th}$ column of the $\bb{Q}$ factor computed in this mixed-precision BQR algorithm is computed via $$\hat{\bb{Q}}_{mpBQR}[:,j] = \left(\prod_{q'=1}^q(\hat{\bb{X}}_{\bb {(q'-1)M+1}}\cdots\hat{\bb{X}}_{\bb {q'M}}\bb{I}^l)\right) \hat{e}_j.$$
Applying \cref{lem:3.7}, we result in bounds
\begin{align}
	\|\hat{\bb{Q}}_{mpBQR}[:,j] - \bb{Q}[:,j]\|_2 &\leq q(u^l+Mr\tilde{\gamma}_m^h)\label{eqn:mpBQR1}\\%\leq \tilde{\gamma}_{d(mn, u^h, q, u^l)}^l
	\|\hat{\bb{Q}}_{mpBQR} - \bb{Q}\|_F &\leq n^{1/2}q(u^l+Mr\tilde{\gamma}_m^h)\leq \tilde{\gamma}_{d(mn^{3/2}, u^h, qn^{1/2}, u^l)}^l,\label{eqn:mpBQR2}
\end{align}
showing that $q$ number of cast downs add $\gamma_q^l$ order errors to columnwise bounds and the matrix norm bound is derived from that.
This mixed precision BQR variant still rich in level-3 BLAS operations can be implemented on TensorCore technology.
Furthermore, the bounds in \cref{eqn:mpBQR1,eqn:mpBQR2} show that the loss in precision that can occur from cast downs are linear to the number of cast downs.
\subsubsection{Level-Block round down: TSQR}\label{sec:mp-3t}

\subsection{Round down at inner-product level (BLAS-2)}\label{sec:mp-2}





























%Although Lemma~\ref{lem:gamma} only requires $ku<1$, we actually need $ku <\frac{1}{2}$, which implies $\gamma_{k} <1$, in order to maintain a meaningful relative error bound.
%While this assumption, $\gamma_{k} < 1$, is easily satisfied by fairly large $k$ in higher precision floating point numbers, it is a problem even for small $k$ in lower precision floating point numbers.
%Table \ref{table:ieeen} shows the maximum value of $k$ that still guarantees a relative error below $100\%$ ($\gamma_{k} < 1$). 
%\begin{table}[h]
%	\centering
%	\begin{tabular}{||c|c|c||} 
%		\hline
%		precision &$\tilde{k} = \mathrm{argmax}_{k}(\gamma_{k} \leq 1)$ \\ \hline
%		half & {\tt 512}\\
%		single & $\approx$ {\tt 4.194e06} \\ 
%		double &  $\approx$ {\tt 2.252e15}\\ \hline 
%	\end{tabular}
%	\caption{Upper limits of meaningful relative error bounds in the $\gamma_{k}$ notation.}
%	\label{table:ieeen}
%\end{table}
%Thus, accumulated rounding errors in lower precision types lead to instability with fewer operations in comparison to higher precision types.
%As $k$ represents the number of FLOPs, this constraint restricts low-precision floating point operations to smaller problem sizes and lower complexity algorithms.
%%That such a small number of operations, $k=512$, leads to $\gamma_{\text{IEEE 754 half}}_{k=512} = 1$ 
%
%%Thus, low-precision floats face problems within the rounding error analysis framework established in \cite{Higham2002} with smaller $k$-values than high-precision floats.
%
%%That small values of $k$ lead to $\gamma_{k} = 1$ can be interpreted as:
%%\begin{itemize}
%%	\item Accumulated rounding errors in lower precision types grow unstable very quickly and with fewer operations in comparison to higher precision types.
%%	\item The upper bound given by $\gamma_{k}$ is less pessimistic in low precision than in high precision. 
%%	Take the fact that 512 half precision operations and $2^{22}$ single precision operations both result in a 100\% relative error bound ($\gamma_{\text{half}}_{512} = \gamma_{\text{single}}_{2^{22}} = 1$).
%%	This error bound is at its tightest when every single one of the rounding errors admitted at each step were the largest-possible, worst-case scenarios.
%%	Arguably, $512$ successive instances of the largest possible errors is more probable than $2^{22}$ successive instances of the largest possible errors. 
%%	% This second point may not be necessary.
%%	%TODO: If we keep this point, right after this itemize might be a good place to give brief summary of probablistic error.
%%\end{itemize}
%
%%This reflects on two sources of difficulty: the larger round-off error 
%%1) Accumulated rounding errors in lower precision types grow unstable with fewer operations, and 2) the upper bound given by $\gamma_{k}$ becomes pessimistic faster.% in low precision.
%%First, rounding errors admitted at each operation are much larger in low precision floating point arithmetic.
%%For example, the unit round-off value for half precision floats is almost four decimal orders of magnitude larger than that of single precision floats. 
%%This implies that it would take almost $10^4$ as few operations in half precision than in single precision to reach the same order of magnitude in accumulated rounding error. 
%%Second, the convenient error bound presented in Lemma~\ref{lem:gamma} gives the worst-case upper bound, which is given by assuming that the worst-case rounding occurred at each of the $k$ successive floating point operations. 
%%As $k$ grows larger, the probability that the largest possible rounding occurs at every operation also grows smaller, and therefore, $\gamma_{k}$ upper bound becomes more pessimistic.
%\par
%
%To clearly illustrate how this situation restricts rounding error analysis in half precision, we now consider performing the dot product of two vectors.
%A forward error bound for dot products is
%\begin{equation}
%\frac{|\bb{x}^{\top}\bb{y} - \fl(\bb{x}^{\top}\bb{y})|}{|\bb{x}|^{\top}|\bb{y}|} \leq \gamma_{m}, \quad \bb{x},\bb{y}\in\R^{m},
%\label{eqn:DDerr}
%\end{equation}
%where details and proof for this statement can be found in Section 3.1 of \cite{Higham2002}.
%While this result does not guarantee a high relative accuracy when $|\bb{x}^{\top}\bb{y}| \ll |\bb{x}|^{\top}|\bb{y}|$, high relative accuracy is expected in some special cases.
%For example, let $\bb{x}=\bb{y}$.
%Then we have exactly $|\bb{x}^{\top}\bb{x}| = |\bb{x}|^{\top}|\bb{x}|=\|\bb{x}\|_2^2$, which leads to
%\begin{equation}
%\left|\frac{\|\bb{x}\|_2^2 - \fl(\|\bb{x}\|_2^2)}{\|\bb{x}\|_2^2}\right| \leq \gamma^{p}_{d+2}.
%\end{equation}
%Since vectors of length $m$ accumulate rounding errors that are bounded by $\gamma_{m}$, the worst-case relative error bound for a dot product of vectors of length $512$ is already at 100\% ($\gamma^{\text{half}}_{512}=1$). \par
%
%We present a simple numerical experiment that shows that the standard deterministic error bound is too pessimistic and cannot be practically used to approximate rounding error for half-precision arithmetic. 
%In this experiment, we generated 2 million random half-precision vectors of length $512$ from two random distributions: the standard normal distribution, $N(0,1)$, and the uniform distribution over $(0,1)$.
%Half precision arithmetic was simulated by calling Algorithm~\ref{algo:simulate} for every multiplication and summation step required in calculating the dot product, $\fl(\bb{x}^{\top}\bb{y})$.
%
%%Half precision arithmetic was simulated by: 1) casting all half precision floats up to single precision representation, 2) computing single precision operation, and 3) casting back down to half precision.
%
%\begin{algorithm2e}[H]
%	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
%	\KwIn{$\bb{x}_{\text{half}}, \bb{y}_{\text{half}}\in\F_{\text{half}}^m$, $f:\R^{m}\times \R^m \rightarrow \R^n$}
%	\KwOut{$\fl(f(\bb{x}_{\text{half}}, \bb{y}_{\text{half}}))\in\F_{\text{half}}^n$}
%	$\bb{x}_{\text{single}}, \bb{y}_{\text{single}} \gets$ {\tt castup}$([\bb{x}_{\text{half}},\bb{y}_{\text{half}}])$\\
%	$\bb{z}_{\text{single}} \gets \fl(f(\bb{x}_{\text{single}}, \bb{y}_{\text{single}}))$\\
%	$\bb{z}_{\text{half}} \gets$ {\tt castdown}$(\bb{z}_{\text{single}})$\\
%	\Return $\bb{z}_{\text{half}}$\\
%	\caption{$\bb{z}^{\text{half}} = {\tt simHalf}(f, \bb{x}^{\text{half}}, \bb{y}^{\text{half}})$ Simulate function $f\in$ OP$\cup \{{\tt dot product} \}$ in half precision arithmetic given input variables $\bb{x},\bb{y}$. Function {\tt castup} converts half precision floats to single precision floats, and {\tt castdown} converts single precision floats to half precision floats by rounding to the nearest half precision float.}
%	\label{algo:simulate}
%\end{algorithm2e}
%
%%These 3 steps were executed at every multiplication and addition operation in calculating the dot product, $\fl(\bb{x}^{\top}\bb{y})$.
%The casting up step is exact since all half precision numbers can be exactly represented in single precision, $\F_{\text{half}}\subset \F_{\text{single}}$; the second step incurs a rounding error from a single precision arithmetic operation; and the casting down step incurs a rounding error from casting down to half precision.
%Note that using Algorithm~\ref{algo:simulate} for any operation in OP results in simulating half precision arithmetic, whereas using it with the dot product results in simulating mixed precision arithmetic instead. 
%The relative error in this experiment is formulated as the left hand side of the inequality in Equation \ref{eqn:DDerr}, where all operations outside of calculating $\fl(\bb{x}^{\top}\bb{y})$ are executed by casting up to double precision format and using double precision arithmetic.
%Table \ref{table:HPdoterr} shows statistics from computing the relative error for simulated half precision dot products of $512$-length random vectors. 
%\begin{table}[h]
%	\centering
%	\begin{tabular}{||c|c|c|c||} 
%		\hline
%		Random Distribution & Average & \makecell{Standard\\deviation}& Maximum\\ \hline
%		Standard normal &{\tt 1.627e-04} & {\tt 1.640e-04 } & {\tt 2.838e-03}\\ \hline
%		Uniform $(0,1)$ & {\tt 2.599e-03}& {\tt 1.854e-03} & {\tt 1.399e-02}\\ \hline
%	\end{tabular}
%	\caption{Statistics from dot product backward relative error in for 512-length vectors stored in half-precision and computed in simulated half-precision from 2 million realizations.}
%	\label{table:HPdoterr}
%\end{table}
%%TODO: why is the standard dev for the first row larger than average? data should be nonneg.
%We see that the inner products of vectors sampled from the standard normal distribution have backward relative errors that do not deviate much from the unit round-off ({\tt 4.883e-4}), whereas the vectors sampled from the uniform distribution tend to accumulate larger errors. 
%Even so, the theoretical upper error bound of 100\% is too pessimistic, and it is difficult to predict the kind of results this experiment shows. 
%Recent work in developing probabilistic bounds on rounding errors of floating point operations have shown that the inner product relative backward error for the conditions used for this experiment is bounded by {\tt 5.466e-2} with probability 0.99. \par
%%While the probabilistic error bound does get the correct order of magnitude for a maximal error (with probability 99\%), it is not enough to describe the probability distribution of inner product errors. \par
%%TODO: state that probablistic bounds have been introduced and why they have been.
%%TODO: cite Theo Mary/N Higham paper and briefly mention probabilistic bound? https://personalpages.manchester.ac.uk/staff/theo.mary/doc/ProbErrAna.pdf
%
%Most importantly, no rounding error bounds (deterministic or probabilistic) allow flexibility in the precision types used for different operations. 
%This restriction is the biggest obstacle in gaining an understanding of rounding errors to expect from computations done on emerging hardware that support mixed precision such as GPUs that employ mixed precision arithmetic.
%In this paper, we extend the rounding error analysis framework established in \cite{Higham2002} to mixed precision arithmetic operations. 
%
%%Nevertheless, the error analysis within the framework given by Lemma \ref{lem:gamma} best allows us to keep the analysis simple.
%%We will use it to study mixed precision block QR factorization methods. \par 
%
%%TODO: include statement about assumption that floats are not subnormal numbers.
%
%
%% DONE: replace $w$ with $w$that does not contain $s$ or $p$?
%Lemma \ref{lem:up} shows rules from Lemma 3.3 in \cite{Higham2002} that summarize how to accumulate errors represented by $\tth$'s and $\gamma$'s.
%\begin{lemma}
%	\label{lem:up}
%	For any positive integer $k$, let $\tth_{k}$ denote a quantity bounded according to $|\tth_{k}|\leq \frac{k u }{1-ku} =:\gamma_{k}$. The following relations hold for positive integers $i$, $j$, and nonnegative integer $k$.
%	Arithmetic operations between $\tth_{k}$'s: 
%	%%%	\begin{align*}
%	\begin{equation}
%	(1+\tth_{k})(1+\tth_{j})%%%&
%	=(1+\tilde{\tth}_{k+j}) %%%\\
%	\qquad \mbox{and} \qquad
%	\frac{1+\tth_{k}}{1+\tth_{j}} %%%&
%	=
%	\begin{cases}
%	1+\tth_{k+j},& j \leq k\\
%	1+\tth_{k+2j},& j > k\\
%	\end{cases} 
%	%%%	\end{align*}
%	\end{equation}
%	Operations on $\gamma$'s: 
%	\begin{align*}
%	\gamma_{k}\gamma_{j} &\leq \gamma_{\rm{min}(k,j)}, \quad\text{for } \rm{max}_{(j,k)} u \leq \frac{1}{2}, \\
%	n\gamma_{k} &\leq \gamma_{nk}, \quad \mbox{for} \quad n \leq \frac{1}{uk},\\
%	\gamma_{k} + u &\leq \gamma_{k+1}, \\ 
%	\gamma_{k}+\gamma_{j}+\gamma_{k}\gamma_{j} & \leq \gamma_{k+j}.
%	\end{align*}
%\end{lemma}
%% TODO: make side by side to save space ?
%
%In Lemma~\ref{lem:mp}, we present modified versions of the rules in Lemma~\ref{lem:up}.
%This mixed precision error analysis relies on the framework given by Lemma~\ref{lem:gamma}, which best allows us to keep a simple analysis. 
%These relations allow us to easily accumulate errors in terms of $\tth$'s and $\gamma$'s and aid in writing clear and simpler error analyses.
%The modifications support multiple precision types, whereas Lemma \ref{lem:up} assumes that the same precision is used in all operations. 
%We distinguish between the different precision types using subscripts--- these types include products ($p$), sums ($s$), and storage formats ($w$).
%
%\begin{lemma}%[Mixed precision version of Lemma 3.3 from \cite{Higham2002}]
%	\label{lem:mp}
%	For any nonnegative integer $k$ and some precision $q$, let $\tth^{q}_{k}$ denote a quantity bounded according to $|\tth^{q}_{k}|\leq \frac{k u^{q} }{1-ku^{q}} =:\gamma^{q}_{k}$.
%	The following relations hold for two precisions $s$ and $p$, positive integers, $j_s$,$j_p$, non-negative integers $k_s$, and $k_p$, and $c>0$:
%	%Most of these result from commutativity. 
%	\begin{equation}
%	(1+\tth^{p}_{k_p})(1+\tth^{p}_{j_p})(1+\tth^{s}_{k_s})(1+\tth^{s}_{j_s})=(1+\tth^{p}_{k_p+j_p})(1+\tth^{s}_{k_s+j_s}), \\
%	\end{equation}
%	\begin{align}
%	\frac{(1+\tth^{p}_{k_p})(1+\tth^{s}_{k_s})}{(1+\tth^{p}_{j_p})(1+\tth^{s}_{j_s})} &=\left\{\begin{alignedat}{2}
%	(1+\tth^{s}_{k_s+j_s})(1+\tth^{p}_{k_p+j_p})&,\quad& j_s \leq k_s, j_p \leq k_p,\\
%	(1+\tth^{s}_{k_s+2j_s})(1+\tth^{p}_{k_p+j_p})&,\quad& j_s \leq k_s, j_p > k_p,\\
%	(1+\tth^{s}_{k_s+j_s})(1+\tth^{p}_{k_p+2j_p})&,\quad& j_s > k_s, j_p \leq k_p,\\
%	(1+\tth^{s}_{k_s+2j_s})(1+\tth^{p}_{k_p+2j_p})&,\quad& j_s > k_s, j_p > k_p.
%	\end{alignedat}\right.
%	\end{align}
%	Without loss of generality, let $1 \gg u^{p} \gg u^{s}>0$.
%	Let $d$, a nonnegative integer, and $r\in[0, \lfloor\frac{u^{p}}{u^{s}}\rfloor]$ be numbers that satisfy $k_su^{s} = d u^{p} + r u^{s}$. 
%	Alternatively, $d$ can be defined by $d := \lfloor\frac{k_su^{s}}{u^{p}}\rfloor$.
%	Then
%	\begin{align}
%	\gamma^{s}_{k_s}\gamma^{p}_{k_p} &\leq \gamma^{p}_{k_p}, \quad\text{for } k_p u^{p} \leq \frac{1}{2}  \\
%	\gamma^{s}_{k_s}+u^{p} &\leq \gamma^{p}_{d+2} \\
%	\gamma^{p}_{k_p} + u^{s} &\leq \gamma^{p}_{k_p+1} \\ %\quad{\color{blue}\text{(A loose bound)}}
%	\gamma^{p}_{k_p}+\gamma^{s}_{k_s}+\gamma^{p}_{k_p}\gamma^{s}_{k_s} & < \gamma^{p}_{k_p+ d+ 1}. \label{lem:mp1}
%	\end{align} 
%\end{lemma}
%A proof for Equation \ref{lem:mp1} is provided in Appendix \ref{appendix:A}.
%We use these principles to establish a mixed precision rounding error analysis for computing the dot product, which is crucial in many linear algebra routines such as the QR factorization.
%% TODO: make side by side to save space ?
%% TODO GEOFF and MINAH.   Meet and work out exact wording to fully formalize.   Should the wording be: there exists a \theta bounded by a \gamma (which is a specific value)?
%\subsection{Inner product mixed precision error}
%\label{ssec:IP}
%We will see in Section~\ref{sec:HQRf} that the inner product is a building block of the HQR factorization (HQR) algorithm, which was introduced in \cite{Householder1958}.
%More generally, it is used widely in most linear algebra tools such as matrix-vector multiply and projections.
%Thus, we will generalize classic round-off error analysis of inner products to algorithms that may employ different precision types to different operations. 
%Specifically, we consider performing an inner product with the storage precision, $u_w$, being lower than the summation precision, $u^{s}$.
%This choice was made to provide a more accurate rounding error analysis of mixed precision floating point operations present in recent GPU technologies such as NVIDIA's TensorCore. 
%Currently, TensorCore computes the inner product of vectors stored in half-precision by employing full precision multiplications and a single-precision accumulator. 
%%TODO: citation?
%As the majority of rounding errors from computing inner products occur during summation (see Section 3.1, \cite{Higham2002}), the single precision accumulator immensely reduces the error in comparison to using only half-precision operations.
%This increase in accuracy combined with its speedy performance motivates 1)to study how to best utilize mixed precision arithmetic in algorithms and 2) to develop more accurate error analyses appropriate for mixed precision algorithms.
%%TODO: was precision actually defined before? -Aly ( I think this was take care off)
%
%
%Lemma \ref{lem:ip_a} and Corollary \ref{lem:ip_b} present two mixed precision forward error bounds for inner products, which show a tighter bound than the existing error bounds. 
%In both cases, we assume storage in the lowest precision with round-off value, $u_w$, and summation performed with a higher precision with round-off value, $u^{s}$, and let $d \approx m u^{s} / u_w$,
%% be the ratio between $m\times u^{s}$ and $u_w$
%where $m$ is the length of the vectors. 
%Although there are additional differing assumptions in these two lemmas, results from both show a strong dependence on $d$.
%%Both lemmas show a dependence on $d$
%\begin{lemma}
%	\label{lem:ip_a}
%	Let $w$, $p$, and $s$ each represent floating point precisions for storage, product, and summation, where the varying precisions are defined by their unit round-off values denoted by $u_w$, $u^{p}$, and $u^{s}$.
%	Let $\bb{x},\bb{y}\in \F_w^{m}$ be two arbitrary vectors stored in $w$ precision.
%	If an inner product performs multiplications in precision $p$ and addition of the products using precision $s$, then
%	\begin{equation}
%	\fl(\bb{x}^{\top}\bb{y}) = (\bb{x}+\bb{\Delta x}) \bb{y} = \bb{x}(\bb{y}+\bb{\Delta y}),
%	\end{equation}
%	where $|\bb{\Delta x}|\leq \gamma^{p,s}_{1,m-1}|\bb{x}|$, $|\bb{\Delta y}|\leq \gamma^{p,s}_{1,m-1}|\bb{y}|$ componentwise, and $$\gamma^{p,s}_{1,m-1} := (1+u^{p})(1+\gamma^{s}_{m-1})-1.$$
%	This result is then stored in precision $w$, and, if we further assume that $u_w=u^{p}>u^{s}$, then $|\bb{\Delta x}|\leq \gamma_w^{d+2}|\bb{x}|$ and $|\bb{\Delta y}|\leq \gamma_w^{d+2}|\bb{y}|$, where $d:=\lfloor\frac{(m-1)u^{s}}{u_w}\rfloor$.
%\end{lemma}
%
%% TODO $\|\|_2$ for norms, What if x and or why have some zeros, or very small values?
%% TODO is it really componentwise or in the infinity norm?   
%Corollary \ref{lem:ip_b} presents another mixed precision forward error bound for mixed precision inner products with additional constraints.
%Here, we assume that the vectors are being stored in a lower precision than the precision types being used for multiplications and additions.
%This scenario is similar to how TensorCore technology works in GPUs.
%
%\begin{corollary}
%	\label{lem:ip_b}
%	In addition to the assumptions in Lemma~\ref{lem:ip_a}, assume $1\gg u_w \gg u^{s}>0$, and thus for any two numbers $x,y$ in $\F_w$, their product $xy$ is in $\F^{s}$.
%	Let $\bb{x},\bb{y}\in \F_w^m$ be two arbitrary vectors stored in $w$ precision.
%	If an inner product performs multiplications in full precision and addition of the products using precision $s$, then
%	\begin{equation}
%	\fl(\bb{x}^{\top}\bb{y}) = (\bb{x}+\bb{\Delta x}) \bb{y} = \bb{x}(\bb{y}+\bb{\Delta y}),
%	\end{equation}
%	where $|\Delta x|\leq \gamma_w^{d+1}|x|$, $|\Delta y|\leq \gamma_w^{d+1}|y|$ componentwise and $d:=\lfloor\frac{(m-1)u^{s}}{u_w}\rfloor$.
%\end{corollary}
%
%Proofs for Lemma \ref{lem:ip_a} and Corollary \ref{lem:ip_b} are shown in Appendix \ref{appendix:A}.
%The analyses for these two differ only in the type of mixed precision arithmetic performed within the inner product subroutine, and the difference is revealed to result in either $\gamma_w^{d+1}$ or $\gamma_w^{d+2}$.
%For the rest of this paper, we will refer to the forward error bound for the inner product as $\gamma_w^{d+z}$ for $z=1,2$ to generalize the analysis for varying assumptions.
%This simplification allows us to use the same analysis for the remaining steps of the HQR algorithm presented in the following sections.